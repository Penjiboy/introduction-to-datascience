# Classification II: Evaluation & tuning {#classification_continued}

## Overview 
This chapter continues the introduction to predictive modelling through classification. While the previous chapter
covered training and data preprocessing, this chapter focuses on how to split data, how to evaluate prediction accuracy, 
and how to choose model parameters to maximize performance.

## Chapter learning objectives 
By the end of the chapter, students will be able to:

- Describe what training, validation, and test data sets are and how they are used in classification
- Split data into training, validation, and test data sets
- Evaluate classification accuracy in R using a validation data set and appropriate metrics
- Execute cross-validation in R to choose the number of neighbours in a K-nearest neighbour classifier
- Describe advantages and disadvantages of the K-nearest neighbour classification algorithm

## Evaluating accuracy

Sometimes our classifier might make the wrong prediction. A classifier does not need to be right 100\% of 
the time to be useful, though we don't want the classifier to make too many wrong predictions. How do we measure 
how "good" our classifier is? Let's revisit the Wisconsin breast cancer example and think about how our classifier
will be used in practice. A biopsy will be performed on a *new* patient's tumour, the resulting image will be analyzed, and the classifier
will be asked to decide whether the tumour is benign or malignant. The key word here is *new*: our classifier is "good"
if it provides accurate predictions on data *not seen during training*. But then how can we evaluate our classifier
without having to visit the hospital to collect more tumour images? 

The trick is to split up the data set into a **training set** and **test set**, and only show the classifier 
the **training set** when building the classifier. Then to evaluate the accuracy of the classifier, we can use
it to predict the labels (which we know) in the **test set**. If our predictions match the true 
labels for the observations in the **test set** very well, then we have some confidence that our 
classifier might also do a good job of predicting the class labels for new observations that we do not have the 
class labels for.

> Note: if there were a golden rule of machine learning, it might be this: *you cannot use the test data to build the model!* 
> If you do, the model gets to "see" the test data in advance, making it look more accurate than it really is. Imagine
> how bad it would be to overestimate your classifier's accuracy when predicting whether a patient's tumour is malignant or benign!

<center><img src="img/training_validation.jpeg" width="600" /></center>
(TODO CHANGE VALIDATION TO TEST IN IMG)

How exactly can we assess how well our predictions match the true labels for the observations in the test set? One way 
we can do this is to calculate the **prediction accuracy**. This is the fraction of examples for which the classifier made
the correct prediction. To calculate this we divide the number of correct predictions by the number of predictions made. Other 
measures for how well our classifier performed include *precision* and *recall*; these will not be discussed here, but 
you will encounter them in other more advanced courses on this topic. This process is illustrated below:

<center><img src="img/ML-paradigm.png" width="800" /></center>
(TODO CHANGE VALIDATION TO TEST IN IMG)


In R, we can use the `caret` package not only to perform K-nearest neighbour classification, but also to assess how well our 
classification worked. Let's start by loading the necessary libraries, reading in the breast cancer data from the previous
chapter, and making a quick scatter plot visualization of tumour cell concavity versus smoothness coloured by diagnosis.

```{r 06-precode, message = FALSE, fig.height = 4, fig.width = 5}
# load libraries
library(tidyverse)
library(caret)

#load data
cancer <- read_csv("data/clean-wdbc.data.csv") %>% 
  mutate(Class = as.factor(Class)) # because we will be doing statistical analysis on a categorical variable

# colour palette
cbPalette <- c("#56B4E9", "#E69F00","#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#999999") 

# create scatter plot of tumour cell concavity versus smoothness, 
# labelling the points be diagnosis class
perim_concav <- cancer %>%  
  ggplot(aes(x = Smoothness, y = Concavity, color = Class)) + 
    geom_point(alpha = 0.5) +
    labs(color = "Diagnosis") + 
    scale_color_manual(labels = c("Benign", "Malignant"), values = cbPalette)
perim_concav
```

**1. Create the train / test split**

Once we have decided on a predictive question to answer and done some preliminary exploration, the 
very next thing to do is to split the data
into the training and test sets. Typically, the training set is between 50 - 100% of the data, while the test
set is the remaining 0 - 50%; the intuition is that you want to trade off between training an accurate model (by using
a larger training data set) and getting an accurate evaluation of its performance (by using a larger test data set). 
Here, we will use 75% of the data for training, and 25% for testing. 
To do this we will use the `createDataPartition` function from the `caret` package, specifying values for 3 arguments: 

1. `y`: the class labels. These must be a vector.
2. `p`: the proportion (between 0 and 1) of the data you would like in the training data set.
3. `list = FALSE`: this states that we want the training and test sets in the form of a matrix, not a list.

The `createDataPartition` function returns the row numbers for the training set. 

```{r 06-get-indices-for-training-set}
set.seed(1234) # makes the random selection of rows reproducible
set_rows <- cancer %>% 
  select(Class) %>% 
  unlist() %>% # converts Class from a tibble to a vector
  createDataPartition(p = 0.75, list = FALSE)
head(set_rows)
```

> Note: You will see in the code above that we use the `set.seed` function again, as discussed in the previous chapter. In this case it is because
> `createDataPartition` uses random sampling to choose which rows will be in the training set. Since we want our code to be reproducible
> and generate the same train/test split each time it is run, we use `set.seed`. 

Now that we have the row numbers for the training set, we can use the `slice` function to get the rows from the original data set (here `cancer`) to create the training and test data sets.

```{r 06-create-training-and-test-sets}
training_set <- cancer %>% slice(set_rows)
test_set <- cancer %>% slice(-set_rows)
glimpse(training_set)
```

```{r 06-test-set}
glimpse(test_set)
```

We can see from `glimpse` in the code above that the training set contains 427 observations, while the test set contains 142 observations. This 
corresponds to a train / test split of 75% / 25%, as desired.

**2. Train the classifier**

Now that we have split our original data set into training and test sets, we can create our K-nearest neighbour classifier with only the training set
using the technique we learned in the previous chapter. For now, we will just choose the number $K$ of neighbours to be 3, and use concavity and
smoothness as the predictors.

```{r 06-create-K-nearest neighbour-classifier}
X_train <- training_set %>% 
  select(Concavity, Smoothness) %>% 
  data.frame()
Y_train <- training_set %>% 
  select(Class) %>% 
  unlist()
k = data.frame(k = 3)

set.seed(1234)
model_knn <- train(x = X_train, y = Y_train, method = "knn", tuneGrid = k)
model_knn
```

> Note: Here again you see the `set.seed` function. In the K-nearest neighbour implementation in `caret`, when
> there is a tie for the majority neighbour class, the winner is randomly selected. Although there is no chance
> of a tie when $K$ is odd (here $K=3$), it is possible that the code may be changed in the future to have an even value of $K$. 
> Thus, to prevent potential issues with reproducibility, we have set the seed. Note that in your own code,
> you only have to set the seed once at the beginning of your analysis. 

**3. Predict the labels in the test set**

Now that we have a K-nearest neighbour classifier object, we can use it to predict the class labels for our test set:

```{r 06-predict-test}
X_test <- test_set %>% 
  select(Concavity, Smoothness) %>% 
  data.frame()
Y_test_predicted <- predict(object = model_knn, X_test)
head(Y_test_predicted)
```

**4. Compute the accuracy**

Finally we can assess our classifier's accuracy. To do this we need to create a vector containing the class labels for the test 
set. Next we use the function `confusionMatrix` to get the statistics about the quality of our model, this includes the 
statistic we are interested: accuracy.  `confusionMatrix` takes two arguments:

1. `data` (the predicted class labels for the test set), and 
2. `reference` (the original/measured class labels for the test set).

```{r 06-accuracy}
Y_test <- test_set %>% 
  select(Class) %>% 
  unlist()

model_quality <- confusionMatrix(data = Y_test_predicted, reference = Y_test)
model_quality
```

A lot of information is output from `confusionMatrix`, but what we are interested in at this point is accuracy (found 
on the 6th line of printed output). That single value can be obtained from the `confusionMatrix` object using base/built-in R subsetting:

```{r 06-model-quality}
model_quality$overall[1]
```

From a value of accuracy of 0.7957746, we can say that our K-nearest neighbour classifier predicted the correct class 
label on roughly 80% of the examples. 

## Tuning the classifier

The vast majority of predictive models in statistics and machine learning have *parameters* that you have to pick. For example,
in the K-nearest neighbour classification algorithm we have been using in the past two chapters, we have had to pick the
number of neighbours $K$ for the class vote. Is it possible to make this selection, i.e., *tune* the model, in a principled way?
Ideally what we want is to somehow maximize the performance of our classifier on data *it hasn't seen yet*. So we will play
the same trick we did before when evaluating our classifier: we'll split our **overall training data set** further into two subsets, called
the **training set** and **validation set**. We will use the newly-named **training set** for building the classifier,
and the **validation set** for evaluating it! Then we will try different values of the parameter $K$
and pick the one that yields the highest accuracy.

> **Remember:** *don't touch the test set during the tuning process. Tuning is a part of model training!*

### Cross-validation

There is an important detail to mention about the process of tuning: we can, if we want to, split our overall training data up 
in multiple different ways, train and evaluate a classifier for each split, and then choose the parameter based on *all* of the 
different results. If we just split our overall training data *once*, our best parameter choice will depend entirely on whatever 
data was lucky enough to end up in the validation set. Perhaps using multiple different train / validation splits, we'll 
get a better estimate of accuracy, which will lead to a better choice of the number of neighbours $K$ for the overall
set of training data. 

> **Note:** you might be wondering why we can't we use the multiple splits to test our final classifier after tuning is done. This is simply
> because at the end of the day, we will produce a single classifier using our overall training data. If we do multiple train / test splits, we will
> end up with multiple classifiers, each with their own accuracy evaluated on different test data!

###
Specifically, we will split our **training data set** into $C$ evenly-sized chunks (usually in practice researchers have chosen $C$ to be 5 or 10),

Is that the best estimate of accuracy that we can get? What would happen if we again shuffled the observations in our training and validation sets, would we get the same accuracy? Let's do and experiment and see. By changing the `set.seed` value, we can get a different shuffle of the data when we create our training and validation data sets. 

Using `set.seed(4321)`
###


Let's investigate this idea in R! In particular, we will generate three different train / validation splits of our overall training data,
train three different K-nearest neighbour models, and evaluate them using different seed values in the `set.seed` function. First we'll
rename our overall training data.

```{r 06-rename-train}
X_train_total <- X_train
Y_train_total <- Y_train
```

The first seed value we'll use is 1:

```{r 06-set-seed-1}
set.seed(1) # makes the random selection of rows reproducible
# create the 75 / 25 train/validation split
set_rows <- Y_train_total %>% 
  createDataPartition(p = 0.75, list = FALSE)

#split the X and Y data into train/validation
X_train <- X_train_total %>% slice(set_rows)
Y_train <- Y_train_total[set_rows]
X_validation <- X_train_total %>% slice(-set_rows)
Y_validation <- Y_train_total[-set_rows]

#train the KNN model with K=3, and predict the validation labels
k = data.frame(k = 3)
model_knn <- train(x = X_train, y = Y_train, method = "knn", tuneGrid = k)
Y_validation_predicted <- predict(object = model_knn, X_validation)

#compute the accuracy
model_quality <- confusionMatrix(data = Y_validation_predicted, reference = Y_validation)
model_quality$overall[1]
```

The second we'll use is 2:

```{r 06-set-seed-2}
set.seed(2) # makes the random selection of rows reproducible
# create the 75 / 25 train/validation split
set_rows <- Y_train_total %>% 
  createDataPartition(p = 0.75, list = FALSE)

#split the X and Y data into train/validation
X_train <- X_train_total %>% slice(set_rows)
Y_train <- Y_train_total[set_rows]
X_validation <- X_train_total %>% slice(-set_rows)
Y_validation <- Y_train_total[-set_rows]

#train the KNN model with K=3, and predict the validation labels
k = data.frame(k = 3)
model_knn <- train(x = X_train, y = Y_train, method = "knn", tuneGrid = k)
Y_validation_predicted <- predict(object = model_knn, X_validation)

#compute the accuracy
model_quality <- confusionMatrix(data = Y_validation_predicted, reference = Y_validation)
model_quality$overall[1]
```

And finally, for the third we'll use 3:

```{r 06-set-seed-3}
set.seed(3) # makes the random selection of rows reproducible
# create the 75 / 25 train/validation split
set_rows <- Y_train_total %>% 
  createDataPartition(p = 0.75, list = FALSE)

#split the X and Y data into train/validation
X_train <- X_train_total %>% slice(set_rows)
Y_train <- Y_train_total[set_rows]
X_validation <- X_train_total %>% slice(-set_rows)
Y_validation <- Y_train_total[-set_rows]

#train the KNN model with K=3, and predict the validation labels
k = data.frame(k = 3)
model_knn <- train(x = X_train, y = Y_train, method = "knn", tuneGrid = k)
Y_validation_predicted <- predict(object = model_knn, X_validation)

#compute the accuracy
model_quality <- confusionMatrix(data = Y_validation_predicted, reference = Y_validation)
model_quality$overall[1]
```

With three different shuffles of the data, we get three different values for accuracy: 0.80, 0.92, and 0.81! Which 
one is correct? Sadly, there is no easy answer to that question. The best we can do is to do this many times and take 
the average of the accuracies. Typically this is done is a more structured way so that each observation in the data 
set is used in a validation set only a single time. The name for this strategy is called cross-validation and we illustrate it below:

<img src="img/cv.png" width="800" />

In the picture above, 5 different folds/partitions of the data set are shown, and consequently we call this 5-fold cross-validation. To do 5-fold cross-validation in R with `caret`, we use another function called `trainControl`. This function passes additional information to the `train` function we use to create our classifier. The arguments we pass `trainControl` are:

1. `method` (method used for assessing classifier quality, here we specify `"cv"` for cross-validation)
2. `number` (how many folds/partitions of the data set we want to use for cross validation)

```{r 06-train-control}
train_control <- trainControl(method="cv", number = 5)
```

Then when we create our classifier we add an additional argument to `train`, called `trControl` where we give it the name of the object we created with the `trainControl` function. Additionally, we do not need to specify a training and a validation set because we are telling `train` that we are doing cross validation (it will take care creating the folds, calculating the accuracy for each fold and averaging the accuracies for us).

```{r 06-5-fold}
X_cancer <- cancer %>% 
  select(Concavity, Smoothness) %>% 
  data.frame()
Y_cancer <- cancer %>% 
  select(Class) %>% 
  unlist()
k = data.frame(k = 3)

set.seed(1234)
knn_model_cv_5fold <- train(x = X_cancer, y = Y_cancer, method = "knn", tuneGrid = k, trControl = train_control)
knn_model_cv_5fold
```

*This time we set the seed when we call `train` not only because of the potential for ties, but also because we are doing cross-validation. Cross-validation uses a random process to select which observations are included in which folds.*

We can choose any number of folds, typically the more the better. However we are limited by computational power. The more folds we choose, the more computation it takes, and hence the more time it takes to run the analysis. So for each time you do cross-validation you need to consider the size of the data, and the speed of the algorithm (here K-nearest neighbour) and the speed of your computer. In practice this is a trial and error process. Here we show what happens when we do 10 folds:

```{r 06-10-fold}
train_control <- trainControl(method="cv", number = 10)

set.seed(1234)
knn_model_cv_10fold <- train(x = X_cancer, y = Y_cancer, method = "knn", tuneGrid = k, trControl = train_control)
knn_model_cv_10fold
```







### Cross-validation

<center>
<figure class="image">
<img src="img/testing.png" width="600"/>
<figcaption> 
A typical 10-fold cross-validation data set split. <br>Source: https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6
</figcaption>
</figure>
</center>


### Choosing the number of neighbours

### Choosing the number of neighbours for K-nearest neighbour classification

From 5- and 10-fold cross-validate we estimate that the prediction accuracy of our classifier to be ~ 83%. This could be not too bad of an accuracy, however what accuracy you aim for always depends on the downstream application of your analysis. Here, we are trying to predict a very important outcome, tumour cell diagnosis class. And the class label we assign to a real patient may have life or death consequences. Hence, we'd like to do better for this application than 83%. To do this we can use cross-validation in an even bigger way, we can choose a range of possible $k$'s and perform cross-validation to calculate the accuracy for each $k$, and then choose the smallest $k$ which gives us the best cross-validation accuracy. To do this, we will create a vector of values for $k$ instead of providing just 1.

```{r 06-range-cross-val}
train_control <- trainControl(method="cv", number = 10)
k = data.frame(k = c(1, 3, 5, 7, 9, 11, 13, 15, 17))

set.seed(1234)
knn_model_cv_10fold <- train(x = X_cancer, y = Y_cancer, method = "knn", tuneGrid = k, trControl = train_control)
knn_model_cv_10fold
```

Then to help us choose $k$ it is very useful to visualize the accuracies as we increase $k$. This will help us choose the smallest $k$ with the biggest accuracy. We can access the results from the cross-validation by accessing the`results` attribute of the `train` object (our classifier).

```{r 06-accuracies}
accuracies <- knn_model_cv_10fold$results
accuracies 
```

Now we can plot accuracy versus k:

```{r 06-find-k, fig.height = 4, fig.width = 5}
accuracy_vs_k <- ggplot(accuracies, aes(x = k, y = Accuracy)) +
  geom_point() +
  geom_line()
accuracy_vs_k
```


Based off of the visualization above we typically would choose $k$ to be ~ 11, given that at this value of $k$ our accuracy is a high as it can be with much larger values of $k$. As you can see there is no exact or perfect answer here, what we are looking for is a value for $k$ where we get a roughly optimal increase of accuracy but at the same time we want to keep $k$ small.

Why do we want to keep $k$ small? Well this is because if we keep increasing $k$ our accuracy actually starts to decrease! Take a look as the plot below as we vary $k$ from 1 to almost the number of observations in the data set:

```{r 06-lots-of-ks, fig.height = 4, fig.width = 5}
train_control <- trainControl(method="cv", number = 10)
k_lots = data.frame(k = seq(from = 1, to = 499, by = 10))
set.seed(1234)
knn_model_cv_10fold_lots <- train(x = X_cancer, y = Y_cancer, method = "knn", tuneGrid = k_lots, trControl = train_control)
accuracies_lots <- knn_model_cv_10fold_lots$results
accuracy_vs_k_lots <- ggplot(accuracies_lots, aes(x = k, y = Accuracy)) +
  geom_point() +
  geom_line()
accuracy_vs_k_lots
```


## Other ways to increase accuracy

By using cross-validation to choose $k$ we were able to slightly increase our accuracy, but can we still do better? Perhaps. We can start to explore this by taking a look at what is called the training accuracy. Training accuracy is our accuracy if we asked our classifier to make predictions on the training data and then we assessed how well the predictions matched up to the true labels we have for our training data. If they don't match up really well (training accuracy is low), our classification model might be too simple and adding more information (e.g., additional predictors/explanatory variables) could potentially help. The situation where the training accuracy is low is often called underfitting, or high bias.

The training error can be obtained from using the classifier object returned from `train` (when you don't perform cross-validation) to predict on the training data. Then passing the predictions on the training data and the true observed labels into `confusionMatrix`.

```{r 06-other-increase-acc}
k = data.frame(k = 11)
set.seed(1234)
knn_model <- train(x = X_cancer, y = Y_cancer, method = "knn", tuneGrid = k)
training_pred <- predict(knn_model, X_cancer)
results <- confusionMatrix(training_pred, Y_cancer)
results
```

From the complex output, we can see the training accuracy, here 0.8752. Again we can use base/built-in subsetting syntax to directly get the value:

```{r 06-other-increase-acc-2}
results$overall[1]
```

Here we see that our training accuracy is high, 0.8752197, but there is still room for improvement! (If it were 1.0 there wouldn't be and we would have a different problem). So let's see if adding additional information (predictors/explanatory variables) might help our model training and consequently (and more importantly) validation accuracy.

*Note - when adding more information to the model, $k$ = 11 may no longer be the "best" $k$. So we will want to also choose $k$ again.*

```{r 06-other-increase-acc-3}
# set-up training data
X_cancer_all <- cancer %>% 
  select(-Class, -ID) %>% 
  data.frame()
Y_cancer_all <- cancer %>% 
  select(Class) %>% 
  unlist()

# set-up classifier specifications
train_control <- trainControl(method="cv", number = 10)
k = data.frame(k = seq(from = 1, to = 29, by = 2))

# create classifier
set.seed(1234)
knn_model_all <- train(x = X_cancer_all, y = Y_cancer_all, method = "knn", tuneGrid = k, trControl = train_control)

# assess training accuracy
training_pred_all <- predict(knn_model_all, X_cancer_all)
results_all <- confusionMatrix(training_pred_all, Y_cancer_all)
results_all$overall[1]
```

We can see that by including more information (making our classifier more complex by adding additional predictors/explanatory variables) we increased the training accuracy. What about the cross-validation accuracy? And what $k$ should we choose?

```{r 06-more-info, fig.height = 4, fig.width = 5}
accuracies_all <- knn_model_all
accuracy_vs_k_all <- ggplot(accuracies_all, aes(x = k, y = Accuracy)) +
  geom_point() +
  geom_line()
accuracy_vs_k_all
```

From the plot above, it seems as though now with more information in our model, we should choose a $k$ of 7. Additionally, with this extra information our validation accuracy has also increased with $k = 7$.

## Test data set

In addition to a training and validation sets, in practice we really split our data set into 3 different sets:

1. training 
2. validation
3. testing

What is the testing set and what purpose does this third set serve? Typically create the testing set at the very beginning of our analysis, leave it a the locked box so that it plays no role while we a fiddling with things (usually through cross validation) like choosing $k$, or the number of predictors/explanatory variables to put in the classifier. After we have settled on all the settings (e.g., $k$ and number of predictors) for our classifier and we have no plans to EVER change them again we re-train the classifier on the entire training set (i.e., don't do cross validation) with those settings and then predict on testing set observations. We then take those predictions and compare them to the true labels of the test set and come up with a test accuracy measure. This typically looks something like this:

<img src="img/testing.png" width="400"/>

source: https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6

Why do we have this super special test set? This is so we do not violate the golden rule of statistical/machine learning: YOU CANNOT USE THE TEST DATA TO BUILD THE MODEL!!! If you do, this is analagous to a student cheating on a midterm.

## Splitting data

### Shuffling

> **Note:** when we split the data, we make the assumption that there is no order to our originally 
> collected data set. However, if we think that there might be some order to the original data set, then we can randomly 
> shuffle the data before splitting it.  

### Stratification

[TODO]

## Summary

- KNN is a way of doing X

- write down the overall workflow here

**Strengths:**

1. Simple and easy to understand
2. No assumptions about what the data must look like 
3. Works easily for binary (two-class) and multi-class (> 2 classes) classification problems

**Weaknesses:**

1. As data gets bigger and bigger, K-nearest neighbour gets slower and slower, quite quickly
2. Does not perform well with a large number of predictors
3. Does not perform well when classes are imbalanced (when many more observations are in one of the classes compared to the others)
