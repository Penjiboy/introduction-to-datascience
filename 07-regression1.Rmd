# Introduction to regression through K-nearest neighbours {#regression1}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview 
Introduction to regression using K-nearest neighbours (k-nn). We will focus on prediction in cases where there is a response variable of interest and a single explanatory variable.

## Learning objectives 
By the end of the chapter, students will be able to:

* Recognize situations where a simple regression analysis would be appropriate for making predictions.
* Explain the k-nearest neighbour (k-nn) regression algorithm and describe how it differs from k-nn classification.
* Interpret the output of a k-nn regression.
* In a dataset with two variables, perform k-nearest neighbour regression in R using `caret::knnregTrain()` to predict the values for a test dataset.
* Using R, execute cross-validation in R to choose the number of neighbours.
* Using R, evaluate k-nn regression prediction accuracy using  a test data set and an appropriate metric (*e.g.*, means square prediction error).
* Describe advantages and disadvantages of the k-nearest neighbour regression approach.

## Regression 
We can use regression as a method to answer a very similar question to classification (e.g., use past information to predict future values) but in the case of regression the what we want to predict are not class labels, but instead numerical values. An example regression prediction question would be can we use hours spent on exercise each week to predict marathon race time? Another example regression prediction question, that we'll explore in the text book, is can we use house square footage to predict house sale price? For this question we will use a data set from Sacremento that is part of the `caret` package.

