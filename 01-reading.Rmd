# Reading in data locally and from the web {#reading}

## Overview 

Learn to read in various cases of tabular data sets locally and from the web. Once read in, these data sets will be used to walk through a real world Data Science application that includes wrangling the data into a useable format and creating an effective data visualization.

## Chapter learning objectives
By the end of the chapter, students will be able to:

* define the following:
    - absolute file path
    - relative file path
    - url
* match the following `tidyverse` `read_*` function arguments to their descriptions:
    - `file`
    - `delim`
    - `col_names`
    - `skip`
* choose the appropriate `tidyverse` `read_*` function and function arguments to load a given tabular data set into R
* use the `rvest` `html_nodes`, `html_text` and `html_attr` functions to scrape data from a `.html` file on the web
* compare downloading tabular data from a plain text file (e.g., `.csv`) from the web versus scraping data from a `.html` file

## Absolute and relative file paths

When you load in a data set a plain text file  (e.g., `.csv`), you need to tell R where that files lives on the computer you are using. We call this the "path" to the file. There are two kinds of paths, relative paths and absolute paths. A relative path is where the file is in respect to where you currently are on the computer (e.g., where the Jupyter notebook file you are working in is). Whereas an absolute path is where the file is in respect to the base or root folder of the computer's filesystem.

If our computer's filesystem looked like the picture below, and we were working in the Jupyter notebook titled "homework_02.ipynb" and we wanted to read in the `.csv` file named `avocado_prices.csv` into our Jupyter notebook using R, we could do this using either a relative or an absolute path. We show what both would be below.

```
|-- Users
    |-- guest
        |-- Documents
    |-- dawkins
        |-- Documents
            |-- dsci-100
                |-- homework_01
                |-- homework_02
                    |-- homework_02.ipynb
                    |-- data
                        |-- avocado_prices.csv
                |-- homework_03
        |-- Desktop
```

##### Loading `avocado_prices.csv` using a relative path:

```
avocado_data <- read_csv("data/avocado_prices.csv")
```

##### Loading `avocado_prices.csv` using an absolute path:

```
avocado_data <- read_csv("/Users/dawkins/Documents/dsci-100/homework_02/data/avocado_prices.csv")
```

So which one should you use? Well to ensure your code can be run across different machines, you should choose to use the relative path (and it's also less typing!). 

See this video for another explanation: 

<iframe width="840" height="473" src="https://www.youtube.com/embed/ephId3mYu9o" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Reading tabular data into R

Now we will learn more about reading tabular data into R, as well as how to write tabular data to a file. Last chapter we learned about using the `tidyverse` `read_csv` when the file we read it matches that functions expected defaults (column names are present, `,` is the delimiter/separator and there are no row names in the dataset ). We will now learn how to read files where that is not the case. 

Before we jump into the cases where the `tidyverse` `read_csv` functions expected defaults are not the case, let's revisit how we use this with one that does and thus the only argument we need to give to the function is the path to the file, here "historical_vote.csv".

Here is how the file would look in plain text editor:

```
election_num,election_year,winner,winner_party,elec_coll_votes_count,elec_coll_votes_perc,pop_votes_perc,pop_votes_perc_marg,pop_votes_count,pop_votes_count_marg,runner-up,runner-up_party,turnout
10,1824,John Quincy Adams,D.-R.,84/261,32.18%,30.92%,−10.44%,"113,142","−38,221",Andrew Jackson,D.-R.,26.9%
23,1876,Rutherford Hayes,Rep.,185/369,50.14%,47.92%,−3.00%,"4,034,142","−252,666",Samuel Tilden,Dem.,82.6%
```

Using `read_csv` to load in R:

```
library(tidyverse)
read_csv("historical_vote.csv")
```

```
# A tibble: 49 x 13
   election_num election_year winner            winne… elec_… elec_… pop_v… pop_v… pop_vo… pop_… `run… `run… turn…
          <int>         <int> <chr>             <chr>  <chr>  <chr>  <chr>  <chr>    <dbl> <chr> <chr> <chr> <chr>
 1           10          1824 John Quincy Adams D.-R.  84/261 32.18% 30.92% −10.4…  1.13e⁵ −38,… Andr… D.-R. 26.9%
 2           23          1876 Rutherford Hayes  Rep.   185/3… 50.14% 47.92% −3.00%  4.03e⁶ −252… Samu… Dem.  82.6%
 3           58          2016 Donald Trump      Rep.   304/5… 56.50% 45.98% −2.10%  6.30e⁷ −2,8… Hill… Dem.  60.2%
 4           26          1888 Benjamin Harrison Rep.   233/4… 58.10% 47.80% −0.83%  5.44e⁶ −94,… Grov… Dem.  80.5%
 5           54          2000 George W. Bush    Rep.   271/5… 50.37% 47.87% −0.51%  5.05e⁷ −543… Al G… Dem.  54.2%
 6           24          1880 James Garfield    Rep.   214/3… 57.99% 48.31% 0.09%   4.45e⁶ 1,898 Winf… Dem.  80.5%
 7           44          1960 John Kennedy      Dem.   303/5… 56.42% 49.72% 0.17%   3.42e⁷ 112,… Rich… Rep.  63.8%
 8           25          1884 Grover Cleveland  Dem.   219/4… 54.61% 48.85% 0.57%   4.91e⁶ 57,5… Jame… Rep.  78.2%
 9           46          1968 Richard Nixon     Rep.   301/5… 55.95% 43.42% 0.70%   3.18e⁷ 511,… Hube… Dem.  62.5%
10           15          1844 James Polk        Dem.   170/2… 61.82% 49.54% 1.45%   1.34e⁶ 39,4… Henr… Whig  79.2%
# ... with 39 more rows
```


### `read_delim` as a more flexible method to get tabular data into R

When our tabular data comes in a different format, we can use the `read_delim function()` instead. For example, a different version of this historical votes dataset has no column names and uses tabs as the delimiter instead of commas. 

Here is how the file would look in plain text editor:
```
10	1824	John Quincy Adams	D.-R.	84/261	32.18%	30.92%	−10.44%	113,142	−38,221	Andrew Jackson	D.-R.	26.9%
23	1876	Rutherford Hayes	Rep.	185/369	50.14%	47.92%	−3.00%	4,034,142	−252,666	Samuel Tilden	Dem.	82.6%
58	2016	Donald Trump	Rep.	304/538	56.50%	45.98%	−2.10%	62,979,636	−2,864,974	Hillary Rodham Clinton	Dem.	60.2%
```

To get this into R using the `read_delim()` function, we specify the first argument as the path to the file (as done with read_csv), and then provide values to the delim argument (here a tab) and the col_names argument (here false). Both `read_csv()` and `read_delim()` have a `col_names` argument and the default is True. 


```
library(tidyverse)
read_delim("historical_vote_no_header.tsv",  delim = "\t", col_names = FALSE)
```

```
# A tibble: 49 x 13
      X1    X2 X3                X4    X5      X6     X7     X8            X9 X10        X11           X12   X13  
   <int> <int> <chr>             <chr> <chr>   <chr>  <chr>  <chr>      <dbl> <chr>      <chr>         <chr> <chr>
 1    10  1824 John Quincy Adams D.-R. 84/261  32.18% 30.92% −10.44%   113142 −38,221    Andrew Jacks… D.-R. 26.9%
 2    23  1876 Rutherford Hayes  Rep.  185/369 50.14% 47.92% −3.00%   4034142 −252,666   Samuel Tilden Dem.  82.6%
 3    58  2016 Donald Trump      Rep.  304/538 56.50% 45.98% −2.10%  62979636 −2,864,974 Hillary Rodh… Dem.  60.2%
 4    26  1888 Benjamin Harrison Rep.  233/401 58.10% 47.80% −0.83%   5443633 −94,530    Grover Cleve… Dem.  80.5%
 5    54  2000 George W. Bush    Rep.  271/538 50.37% 47.87% −0.51%  50460110 −543,816   Al Gore       Dem.  54.2%
 6    24  1880 James Garfield    Rep.  214/369 57.99% 48.31% 0.09%    4453337 1,898      Winfield Sco… Dem.  80.5%
 7    44  1960 John Kennedy      Dem.  303/537 56.42% 49.72% 0.17%   34220984 112,827    Richard Nixon Rep.  63.8%
 8    25  1884 Grover Cleveland  Dem.  219/401 54.61% 48.85% 0.57%    4914482 57,579     James Blaine  Rep.  78.2%
 9    46  1968 Richard Nixon     Rep.  301/538 55.95% 43.42% 0.70%   31783783 511,944    Hubert Humph… Dem.  62.5%
10    15  1844 James Polk        Dem.  170/275 61.82% 49.54% 1.45%    1339570 39,413     Henry Clay    Whig  79.2%
# ... with 39 more rows
```

### Reading tabular data directly from a URL
We can also use `read_csv()` or `read_delim()` (and related functions) to read in tabular data directly from a url that contains tabular data. In this case, we provide the url as a string to `read_csv()` as the path to the file instead of a path to a local file on our computer. All other arguments that we use are the same as when using these functions with a local file on our computer.

```
library(tidyverse)
read_csv("https://github.com/swcarpentry/r-novice-gapminder/raw/gh-pages/data/gapminder_wide.csv")
```

```
# A tibble: 142 x 38
   conti… count… gdpPe… gdpPe… gdpPe… gdpP… gdpP… gdpP… gdpP… gdpP… gdpP… gdpP… gdpP… gdpP… life… life…
   <chr>  <chr>   <dbl>  <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
 1 Africa Alger…   2449   3014   2551  3247  4183  4910  5745  5681  5023  4797  5288  6223  43.1  45.7
 2 Africa Angola   3521   3828   4269  5523  5473  3009  2757  2430  2628  2277  2773  4797  30.0  32.0
 3 Africa Benin    1063    960    949  1036  1086  1029  1278  1226  1191  1233  1373  1441  38.2  40.4
 4 Africa Botsw…    851    918    984  1215  2264  3215  4551  6206  7954  8647 11004 12570  47.6  49.6
 5 Africa Burki…    543    617    723   795   855   743   807   912   932   946  1038  1217  32.0  34.9
 6 Africa Burun…    339    380    355   413   464   556   560   622   632   463   446   430  39.0  40.5
 7 Africa Camer…   1173   1313   1400  1508  1684  1783  2368  2603  1793  1694  1934  2042  38.5  40.4
 8 Africa Centr…   1071   1191   1193  1136  1070  1109   957   845   748   741   739   706  35.5  37.5
 9 Africa Chad     1179   1308   1390  1197  1104  1134   798   952  1058  1005  1156  1704  38.1  39.9
10 Africa Comor…   1103   1211   1407  1876  1938  1173  1267  1316  1247  1174  1076   986  40.7  42.5
# ... with 132 more rows, and 22 more variables: lifeExp_1962 <dbl>, lifeExp_1967 <dbl>, lifeExp_1972
#   <dbl>, lifeExp_1977 <dbl>, lifeExp_1982 <dbl>, lifeExp_1987 <dbl>, lifeExp_1992 <dbl>, lifeExp_1997
#   <dbl>, lifeExp_2002 <dbl>, lifeExp_2007 <dbl>, pop_1952 <dbl>, pop_1957 <dbl>, pop_1962 <dbl>,
#   pop_1967 <dbl>, pop_1972 <dbl>, pop_1977 <dbl>, pop_1982 <dbl>, pop_1987 <dbl>, pop_1992 <dbl>,
#   pop_1997 <dbl>, pop_2002 <int>, pop_2007 <int>
```

### Previewing a data file before reading it into R

In all the examples above, we gave you previews of the data file before we read it into R. This is essential so you can see whether or not there are column names, what the delimiters are, and if there are lines you need to skip. You should do this yourself when trying to read in data files. In Jupyter, you can do this by using the Jupyter home menu to navigate to the file and clicking on it to preview it as a plain text file. We demonstrate this in the video below:

<iframe width="840" height="473" src="https://www.youtube.com/embed/6orO4YMAyeQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


## Scraping data off the web using R

In the first part of this chapter we learned how to read in data from plain text files that are usually "rectangular" in shape using the tidyverse `read_*` functions. Sadly, not all data comes in this simple format, but happily there are many other tools we can use to read in more messy/wild data formats. One common place people often want/need to read in data from is websites. Such data exists in an a non-rectangular format. One quick and easy solution to get this data is to copy and paste it, however this becomes painstakingly long and boring when there is a lot of data that needs gathering, and anytime you start doing a lot of copying and pasting it is very likely you will introduce errors. 

The formal name for gathering non-rectangular data from the web and transforming it into a more useful format for data analysis is **web scraping**. There are two different ways to do web scraping: 1) screen scraping (similar to copying and pasting from a website, but done in a programmatic way to minimize errors and maximize efficiency) and 2) web APIs (**a**pplication **p**rogramming **i**nterface) (a website that provides a programatic way of returning the data as JSON or XML files via http requests). In this course we will explore the first method, screen scraping using R's [`rvest` package](https://github.com/hadley/rvest).

### HTML and CSS selectors

Before we jump into scraping, let's set up some motivation and learn a little bit about what the "source code" of a website looks like. Say we are interested in knowing the average rental price (per square footage) of the most recently available 1 bedroom apartments in Vancouver from https://vancouver.craigslist.org. When we visit the Vancouver Craigslist website and search for 1 bedroom apartments, this is what we are shown:

![](img/craigslist_human.png)

From that page, it's pretty easy for our human eyes to find the apartment price and square footage. But how can we do this programmatically so we don't have to copy and paste all these numbers? Well, we have to deal with the webpage source code, which we show a snippet of below (and link to the [entire source code here](img/website_source.txt)):

```
        <span class="result-meta">
                <span class="result-price">$800</span>

                <span class="housing">
                    1br -
                </span>

                <span class="result-hood"> (13768 108th Avenue)</span>

                <span class="result-tags">
                    <span class="maptag" data-pid="6786042973">map</span>
                </span>

                <span class="banish icon icon-trash" role="button">
                    <span class="screen-reader-text">hide this posting</span>
                </span>

            <span class="unbanish icon icon-trash red" role="button" aria-hidden="true"></span>
            <a href="#" class="restore-link">
                <span class="restore-narrow-text">restore</span>
                <span class="restore-wide-text">restore this posting</span>
            </a>

        </span>
    </p>
</li>
         <li class="result-row" data-pid="6788463837">

        <a href="https://vancouver.craigslist.org/nvn/apa/d/north-vancouver-luxury-1-bedroom/6788463837.html" class="result-image gallery" data-ids="1:00U0U_lLWbuS4jBYN,1:00T0T_9JYt6togdOB,1:00r0r_hlMkwxKqoeq,1:00n0n_2U8StpqVRYX,1:00M0M_e93iEG4BRAu,1:00a0a_PaOxz3JIfI,1:00o0o_4VznEcB0NC5,1:00V0V_1xyllKkwa9A,1:00G0G_lufKMygCGj6,1:00202_lutoxKbVTcP,1:00R0R_cQFYHDzGrOK,1:00000_hTXSBn1SrQN,1:00r0r_2toXdps0bT1,1:01616_dbAnv07FaE7,1:00g0g_1yOIckt0O1h,1:00m0m_a9fAvCYmO9L,1:00C0C_8EO8Yl1ELUi,1:00I0I_iL6IqV8n5MB,1:00b0b_c5e1FbpbWUZ,1:01717_6lFcmuJ2glV">
                <span class="result-price">$2285</span>
        </a>

    <p class="result-info">
        <span class="icon icon-star" role="button">
            <span class="screen-reader-text">favorite this post</span>
        </span>

            <time class="result-date" datetime="2019-01-06 12:06" title="Sun 06 Jan 12:06:01 PM">Jan  6</time>


        <a href="https://vancouver.craigslist.org/nvn/apa/d/north-vancouver-luxury-1-bedroom/6788463837.html" data-id="6788463837" class="result-title hdrlnk">Luxury 1 Bedroom CentreView with View - Lonsdale</a>


```

This is not easy for our human eyeballs to read! However, it is easy for us to use programmatic tools to extract the data we need by specifying which HTML tags (things inside `<` and `>` in the code above). For example, if we look in the code above and search for lines with a price, we can also look at the tags that are near that price and see if there's a common "word" we can use that is near the price but doesn't exist on other lines that have information we are not interested in:

```
<span class="result-price">$800</span>
```

and 

```
<span class="result-price">$2285</span>
```

What we can see is there is a special "word" here, "result-price", which appears only on the lines with prices and not on the other lines (that have information we are not interested in). This special word and the context in which is is used (learned from the other words inside the HTML tag) can be combined to create something called a CSS selector. The CSS selector can then be used by R's `rvest` package to select the information we want (here price) from the website source code.

Now, many websites are quite large and complex, and so then is their website source code. And as you saw above, it is not easy to read and pick out the special words we want with our human eyeballs. So to make this easier, we will use the SelectorGadget tool. It is an open source tool that simplifies generating and finding CSS selectors. We recommend you use the Chrome web browser to use this tool, and install the [selector gadget tool from the Chrome Web Store](https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb). Here is a short video on how to install and use the SelectorGadget tool to get a CSS selector for use in web scraping:

<iframe width="840" height="473" src="https://www.youtube.com/embed/YdIWI6K64zo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

From installing and using the selectorgadget as shown in the video above, we get the two CSS selectors `.housing` and `.result-price` that we can use to scrape information about the square footage and the rental price, respectively. The selector gadget returns them to us as a comma separated list (here `.housing , .result-price`), which is exactly the format we need to provide to R if we are using more than one CSS selector.

### Are you allowed to scrape that website?

**BEFORE** scraping data from the web, you should always check whether or not you are **ALLOWED** to scrape it! There are two documents that are important for this: the robots.txt file and reading the website's Terms of Service document. The website's Terms of Service document is probably the more important of the two, and so you should look there first. What happens when we look at Craigslist's Terms of Service document? Well we read this:

*"You agree not to copy/collect CL content via robots, spiders, scripts, scrapers, crawlers, or any automated or manual equivalent (e.g., by hand)."*

source: https://www.craigslist.org/about/terms.of.use


> Want to learn more about the legalities of web scraping and crawling? Read this interesting blog post titled ["Web Scraping and Crawling Are Perfectly Legal, Right?" by Benoit Bernard](https://benbernardblog.com/web-scraping-and-crawling-are-perfectly-legal-right/) (this is optional, not required reading).

So what to do now? Well, we shouldn't scrape Craigslist! Let's instead scrape some data on the population of Canadian cities from Wikipedia (who's [Terms of Service document](https://foundation.wikimedia.org/wiki/Terms_of_Use/en) does not explicilty say do not scrape). In this video below we demonstrate using the selectorgadget tool to get CSS Selectors from [Wikipedia's Canada](https://en.wikipedia.org/wiki/Canada) page to scrape a table that contains city names and their populations from the 2016 Canadian Census: 

<iframe width="840" height="473" src="https://www.youtube.com/embed/O9HKbdhqYzk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


### Using `rvest`

Now that we have our CSS selectors we can use `rvest` R package to scrape our desired data from the website. First we start by loading the `rvest` package:

```{r load rvest}
library(rvest)
```

> **`library(rvest)` gives error...**
>
> If you get an error about R not being able to find the package (e.g., `Error in library(rvest) : there is no package called ‘rvest’`) this is likely because it was not installed. To install the `rvest` package, run the following command once inside R (and then delete that line of code): `install.packages("rvest")`.

Next, we tell R what page we want to scrape by providing the webpage's URL in quotations to the function `read_html`:

```{r specify page}
page <- read_html("https://en.wikipedia.org/wiki/Canada")
```


Then we send the page object to the `html_nodes` function. We also provide that function with the CSS selectors we obtained from the selectorgadget tool. These should be surrounded by quotations. The `html_nodes` function select nodes from the HTML document using CSS selectors. nodes are the HTML tag pairs as well as the content between the tags. For our CSS selector `td:nth-child(5)` and example node that would be selected would be: `<td style="text-align:left;background:#f0f0f0;"><a href="/wiki/London,_Ontario" title="London, Ontario">London</a></td>`

```{r select nodes}
population_nodes <- html_nodes(page, "td:nth-child(5) , td:nth-child(7) , .infobox:nth-child(122) td:nth-child(1) , .infobox td:nth-child(3)")
head(population_nodes)
```

Next we extract the meaningful data from the HTML nodes using the `html_text` function. For our example, this functions only required argument is the an html_nodes object, which we named `rent_nodes`. In the case of this example node: `<td style="text-align:left;background:#f0f0f0;"><a href="/wiki/London,_Ontario" title="London, Ontario">London</a></td>`, the `html_text` function would return `London`.

```{r get text}
population_text <- html_text(population_nodes)
head(population_text)
```

Are we done? Not quite... If you look at the data closely you see that the data is not in an optimal format for data analysis. Both the city names and population are encoded as characters in a single vector instead of being in a data frame with one character column for city and one numeric column for population (think of how you would organize the data in a spreadsheet). Additionally, the populations contain commas (not useful for programmatically dealing with numbers), and some even contain a line break character at the end (`\n`). Next chapter we will learn more about data wrangling using R so that we can easily clean up this data with a few lines of code.
