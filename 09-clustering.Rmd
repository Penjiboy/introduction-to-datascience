# Clustering {#clustering}

## Overview 
As part of exploratory data analysis, it is often helpful to see if there are
meaningful subgroups (or *clusters*) in the data; this grouping can be used to
for many purposes, such as generating new questions or improving predictive analyses. 
This chapter provides an introduction to clustering using the *K-means* algorithm,
including techniques to choose the number of clusters as well as other practical
considerations (such as scaling).

## Chapter learning objectives 
By the end of the chapter, students will be able to:

* Describe a case where clustering is appropriate, and what insight it might extract from the data.
* Explain the K-means clustering algorithm.
* Interpret the output of a K-means analysis.
* Perform kmeans clustering in R using `kmeans`.
* Visualize the output of K-means clustering in R using pair-wise scatter plots.
* Identify when it is necessary to scale variables before clustering and do this using R.
* Use the elbow method to choose the number of clusters for K-means.
* Describe advantages, limitations and assumptions of the K-means clustering algorithm.

## Clustering
Clustering is a data analysis task involving separating a data set into
subgroups of related data. For example, we might use clustering to separate a
dataset of documents into groups that correspond to topics, a dataset of human
genetic information into groups that correspond to ancestral subpopulations, or
a dataset of online customers into groups that correspond to purchasing
behaviours.  Once the data are separated we can, for example, use the subgroups
to generate new questions about the data and follow up with a predictive
modelling exercise.

Note that clustering is a fundamentally different kind of task than
classification or regression. Most notably, both classification and regression
are *supervised tasks* where there is a *predictive target* (a class label or
value), and we have examples of past data with labels/values that help us
predict those of future data. By contrast, clustering is an *unsupervised
task*, as we are trying to understand and examine the structure of data without
any labels to help us. This approach has both advantages and disadvantages.
Clustering requires no additional annotation or input on the data; for example,
it would be nearly impossible to annotate all the articles on Wikipedia with
human-made topic labels, but we can still cluster the articles without this
information to automatically find groupings corresponding to topics. 

However, because there is no predictive target, it is not as easy to evaluate
the "quality" of a clustering.  With classification, we are able to use a test
data set to assess prediction performance. In clustering, there is not a single
good choice for evaluation. In this book, we will use visualization to ascertain
the quality of a clustering, and leave rigorous evaluation for more advanced
courses.  

> There are also so-called *semisupervised* tasks, where only some of the data
> come with labels / annotations, but the vast majority don't. The goal
> is to try to uncover underlying structure in the data that allows one to
> guess the missing labels. This sort of task is very useful, for example, when one 
> has an unlabelled data set that is too large to manually label, but one is willing to
> provide a few informative example labels as a "seed" to guess the labels for all the data.


**An illustrative example** 

Suppose we have customer data with two variables measuring
customer loyalty and satisfaction, and we want to learn whether there are distinct "types"
of customer. Understanding this might help us come up with better products or
promotions to improve our business in a data-driven way.

```{r 10-toy-example-data, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
data <- tibble(loyalty = c(7, 7.5, 8, 7, 3, 1, 8, 4, 2, 7, 6, 7, 6, 5, 9, 7, 9, 5, 2),
               csat = c(1, 1, 2, 2, 2, 3, 3, 4, 4, 6, 6, 7, 7, 7, 8, 8, 9, 9, 3),
               cluster = c("1",
                           "1",
                           "1",
                           "1",
                           "3",
                           "3",
                           "1",
                           "3",
                           "3",
                           "2",
                           "2",
                           "2",
                           "2",
                           "2",
                           "2",
                           "2",
                           "2",
                           "2", 
                           "3"))
marketing_data <- data[,1:2]
```

```{r 10-toy-example-plot, echo = FALSE, warning = FALSE, fig.height = 4, fig.width = 4.35, fig.cap = "Modified from http://www.segmentationstudyguide.com/using-cluster-analysis-for-market-segmentation/"}
base <- ggplot(data, aes(y = loyalty, x = csat)) +
  geom_point() +
  xlab("Customer satisfaction") +
  ylab("Loyalty") +
  xlim(c(0, 10)) +
  ylim(c(0, 10))

base
```

Based on this visualization, we might suspect there are a few subtypes of customer,
selected from combinations of high/low satisfaction and high/low loyalty. How
do we find this grouping automatically, and how do we pick the number of subtypes
to look for? The
way to rigorously separate the data into groups is to use a clustering algorithm.
In this chapter, we will focus on the *K-means* algorithm, a widely-used
and often very effective clustering method, combined with the *elbow method* for
selecting the number of clusters. This procedure will separate the data into 
the following groups denoted by colour:

```{r 10-toy-example-clustering, echo = FALSE, warning = FALSE, fig.height = 4, fig.width = 5}
ggplot(data, aes(y = loyalty, x = csat, color = cluster)) +
  geom_point() +
  xlab("Customer satisfaction") +
  ylab("Loyalty") +
  xlim(c(0, 10)) +
  ylim(c(0, 10))
```

What are the labels for these groups? Unfortunately, we don't have any. K-means,
like almost all clustering algorithms, just outputs meaningless "cluster labels"
that are typically whole numbers: 1, 2, 3, etc. But in a simple case like this,
where we can easily visualize the clusters on a scatter plot, we can give
human-made labels to the groups using their positions on
the plot:

- low loyalty and low satisfaction (<font color="#00BA38">green cluster</font>),
- high loyalty and low satisfaction (<font color="#F8766D">pink cluster</font>), 
- and high loyalty and high satisfaction (<font color="#619CFF">blue cluster</font>).

Once we have made these determinations, we can use them to inform our future business decisions,
or to ask further questions about our data. For example, here we might notice based on our clustering
that there aren't any customers with high satisfaction but low loyalty, and generate new analyses
or business strategies based on this information. 

## K-means 

### Measuring cluster quality

```{r 10-toy-example-clus1, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
clus1 <- tibble(loyalty = c(7, 7.5, 8, 7, 8),
               csat = c(1, 1, 2, 2, 3),
               )
#clus1_center <- clus1 %>% summarize(csat = mean(csat), loyalty = mean(loyalty))
```

The K-means algorithm is a procedure that groups data into K clusters.
It starts with an initial clustering of the data, and then iteratively
improves it by making adjustments to the assignment of data
to clusters until it cannot improve any further. But how do we measure
the "quality" of a clustering, and what does it mean to improve it? 
In K-means clustering, we measure the quality of a cluster by its
*within-cluster sum-of-squared-distances* (WSSD). Computing this involves two steps:
We first compute the mean of each variable we are using to cluster the data
over data points that are in the cluster (i.e., the *center* of the cluster). For example, suppose we have a 
cluster containing 3 observations, and we are using two variables, $x$ and $y$, to cluster the data.
Then we would compute the $x$ and $y$ variables, $\mu_x$ and $\mu_y$, of the cluster center via

$$\mu_x = \frac{1}{3}(x_1+x_2+x_3) \quad \mu_y = \frac{1}{3}(y_1+y_2+y_3).$$

In the first cluster from the customer satisfaction/loyalty example, there 
are 5 data points. These are shown with their cluster center 
(`csat = 1.8` and `loyalty = 7.5`) highlighted below.

```{r 10-toy-example-clus1-center, echo = FALSE, warning = FALSE, fig.height = 4, fig.width = 4.35, fig.cap = "Cluster 1 from the toy example, with center highlighted."}
base <- ggplot(clus1) +
  geom_point(aes(y = loyalty, x = csat)) +
  labs(x = "Customer satisfaction", y = "Loyalty") +
  xlim(c(0, 4)) +
  ylim(c(6, 9)) +
  geom_point(aes(y = mean(loyalty), x = mean(csat), colour="red", size=2.5)) +
  theme(legend.position = "none")
base
```

The second step is to add up the squared distance between each point in the cluster and the cluster center.
We use the usual distance formula that we learned about in the classification chapter.
In the 3-observation cluster example above, we would compute the WSSD $S^2$ via

$$S^2 = \left((x_1 - \mu_x)^2 + (y_1 - \mu_y)^2\right) + \left((x_2 - \mu_x)^2 + (y_2 - \mu_y)^2\right) +\left((x_3 - \mu_x)^2 + (y_3 - \mu_y)^2\right).$$

These distances are denoted by lines for the first cluster of the customer satisfaction/loyalty data example below. 

```{r 10-toy-example-clus1-dists, echo = FALSE, warning = FALSE, fig.height = 4, fig.width = 4.35, fig.cap = "Cluster 1 from the toy example, with distances to the center highlighted."}
base <- ggplot(clus1) +
  geom_point(aes(y = loyalty, x = csat)) +
  labs(x = "Customer satisfaction", y = "Loyalty") +
  xlim(c(0, 4)) +
  ylim(c(6, 9)) +
  geom_point(aes(y = mean(loyalty), x = mean(csat), colour="red", size=2.5)) +
  theme(legend.position = "none")

mn <- clus1 %>% summarize(csat = mean(csat), loyalty=mean(loyalty))
for (i in 1:5) {
  base <- base + geom_segment(x = unlist(mn[1, "csat"]), y = unlist(mn[1, "loyalty"]),
                           xend = unlist(clus1[i, "csat"]), yend = unlist(clus1[i, "loyalty"]))
}
base
```

The larger the value of $S^2$, the more spread-out the cluster is, since large $S^2$ means that points are far away from the cluster center.
Note, however, that "large" is relative to *both* the scale of the variables for clustering *and* the number of points in the cluster; a 
cluster where points are very close to the center might still have a large $S^2$ if there are many data points in the cluster. 

### The clustering algorithm

We begin by picking K, and randomly assigning data to K clusters. 
Then K-means consists of two major steps that attempt to minimize the
sum of WSSDs over all the clusters, i.e. the *total WSSD*:

1. Compute the center of each cluster, and
2. Reassign each data point to the cluster with the nearest center.

These two steps are repeated until the cluster assignments no longer change.
More details about each of the major algorithmic steps in K-means are provided in the following.

**Initialization**

This step is only performed once at the beginning.

> kmeans++??

**Center update**


**Label update**

Watch the video linked to below for an explanation of the K-means clustering algorithm:
- https://www.coursera.org/lecture/machine-learning-data-analysis/what-is-a-k-means-cluster-analysis-p94tY

*note - when the advertisement pops up to register for this course, you can
just click to ignore it (i.e., no need to sign up to watch the entire video)*

> will this run forever? no. each step reduces the total within cluster sos. since
> sos is a positive number, and it always decreases, and there are only a finite number
> of different clusterings, it must stop decreasing at some point.
> the algorithm stops.

### Choosing K

### Random restarts

K-means, unlike knn and linear regression, can get "stuck" in a bad solution.

## K-means in R

(combine clustering and choosing K in a single section)

Let's take a look at the data we plotted above:

```{r 10-clustering}
head(marketing_data)
```

To peform Kmeans clustering in R, we use the `kmeans` function. It takes at
least two arguments, the data frame containing the data you wish to cluster,
and K, the number of clusters (here we choose K = 3). Given that the K-means
algorithm uses a random start to begin the algorithm, to make this
reproducible, we need to set the seed. 

```{r 10-kmeans}
set.seed(1234)
marketing_clust <- kmeans(marketing_data, centers = 3)
marketing_clust
```

As you can see above, the clustering object returned has a lot of information
about our analysis that we need to explore. Let's take a look at it now. To do
this, we will call in help from the `broom` package so that we get the model
output back in a tidy data format. Let's first start by getting the cluster
identification for each point and plotting that on the scatter plot. To do that
we use the augment function. Augment takes in the model and the original data
frame, and returns a data frame with the data and the cluster assignments for
each point:

```{r 10-plot-clusters-1}
library(broom)

clustered_data <- augment(marketing_clust, marketing_data)
head(clustered_data)
```

Now that we have this data frame, we can easily plot the data (i.e., cluster assignments of each point):

```{r 10-plot-clusters-2, fig.height = 4, fig.width = 4.35}

cluster_plot <- ggplot(clustered_data, aes(x = csat, y = loyalty, colour = .cluster)) +
  geom_point()
cluster_plot
```

## Choosing K for K-means clustering

As mentioned above, we need to choose a K to perform K-means clustering. How
should we choose K? We have no data labels, and so cannot perform
cross-validation with some measure of model prediction error, so what can we
do? What we can do in this situation is to look at the total within-cluster sum
of squares for different K's and choose the K that gives the biggest decrease
in the total within-cluster sum of squares. Why total within-cluster sum of
squares? This statistic lets us know how close/tight-knit (or compact)
observations are within clusters. A larger number means that clusters are not
close/tight-knit, but are instead more spread out. A smaller number here means
that clusters are indeed close/tight-knit together. 

We can get at the total within-cluster sum of squares (`tot.withinss`) from our
clustering using `broom`'s `glance` function (it gives model-level statistics).
For example:

```{r 10-glance}
glance(marketing_clust)
```

Let's calculate the total within-cluster sum of squares for our data for a
variety of K's (say 1 - 9) and then plot them against K. To do this we will
create a data frame with a column named `k`, for each of the K's we want to try
our clustering with. Then we use `map` to apply the `kmeans` function to each
K. We also use `map` to then apply `glance` to each of the clustering models we
performed (one for each K). In the end we end up with a complex data frame with
3 columns, one for K, one for the models, and one for the model statistics
(output of `glance`, which is a data frame):

```{r 10-choose-k, fig.height = 4, fig.width = 4.35}
marketing_clust_ks <- tibble(k = 1:9) %>%
  mutate(marketing_clusts = map(k, ~kmeans(marketing_data, .x)),
         glanced = map(marketing_clusts, glance)) 
head(marketing_clust_ks)
```

What we need to do next, is get the value for the total within-cluster sum of
squares (`tot.withinss`) from the `glanced` column. Given that each item in
this column is a data frame, we will need to use the `unnest` function to
unpack the data frames in the `glanced` column. 

```{r 10-get-total-within-sumsqs}
clustering_statistics <- marketing_clust_ks %>%
  unnest(glanced)

head(clustering_statistics)
```

Now that we have `tot.withinss` and `k` as columns in a data frame, we can make a plot to choose K:

```{r 10-plot-choose-k, fig.height = 4, fig.width = 4.35}
elbow_plot <- ggplot(clustering_statistics, aes(x = k, y = tot.withinss)) +
  geom_point() +
  geom_line() +
  xlab("K") +
  ylab("Total within-cluster sum of squares")
elbow_plot
```

We call the plot above an "elbow plot" and we look for the "elbow" in total
within-cluster sum of squares, the point where afterwards increasing K doesn't
have as much impact reducing the total within-cluster sum of squares. Here we
would choose K = 3. 

## Additional readings:


- Pages 385-390 and 404-405 of [Introduction to Statistical Learning with Applications in R](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf) by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani and the companion video linked to below:

<iframe width="840" height="473" src="https://www.youtube.com/embed/aIybuNt9ps4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
