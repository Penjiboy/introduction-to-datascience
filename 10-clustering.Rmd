# Clustering {#clustering}

## Overview 
Introduction to clustering using K-means. Will discuss the K-means algorithm, how we choose K (the number of clusters) and other practical considerations (such as scaling).

## Learning objectives 
By the end of the chapter, students will be able to:

* Describe a case where clustering would be an appropriate tool, and what insight it would bring from the data.
* Explain the kmeans clustering algorithm.
* Interpret the output of a kmeans analysis.
* Perform kmeans clustering in R using `kmeans`.
* Visualize the output of kmeans clustering in R using pair-wise scatter plots.
* Identify when it is necessary to scale variables before clustering and do this using R.
* Use the elbow method to choose the number of clusters for k-means.
* Describe advantages, limitations and assumptions of the kmeans clustering algorithm.

## Clustering
While at first glance, clustering may seem very similar to classification, these two methods have some very important distinction. Most notably, classification is a supervised method (we use past information to predict the future values/labels for our target/response variable), whereas clustering is considered an unsupervised method (there is no target/response variable and we are looking to find sub-groups/clusters of observations based on how similar they are). So, where classification might be used to label future emails as spam or not spam, clustering might be instead be used to group emails into categories based on their similarity, however we would not have labels for these categories in the case of clustering. Another example problem we might try to solve with clustering is grouping Amazon customers into groups based upon their similar purchasing behaviours. Again here, we do not have, nor need, labels for customer groups.

Another way to think about it is, that classification is really about predicting something that you might have a scientific question about and/or hypothesis for, whereas, clustering is very often a hypothesis generating process (you identify things that are similar to each other that might be unexpected, and from those observations, you might generate a question and hypothesis that you might follow-up with classification).

Another major difference between clustering and classification is in how success is determined. With classification we are able to use a test data set to assess prediction performance, in clustering we must use variance metrics to determine how well our defined clusters fit the data. The two metrics used to determine success are between- and within- variation. Ideally we want clusters where the between-variance is large (so that the clusters are well separated) and the within- variation is small (so that the clusters are composed of close/tight-knit observations).

### A toy example

What if we had some customer data, and we wanted to learn more about the types of customers we had so that we could come up with better products and/or promotions to increase our business in a data-driven way. For example, let's consider this data below, where we have assessed customer loyalty and customer satisfaction:

```{r echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
data <- tibble(loyalty = c(7, 5, 8, 7, 3, 1, 8, 4, 2, 7, 6, 7, 6, 5, 9, 7, 9, 5, 2),
               csat = c(1, 1, 2, 2, 2, 3, 4, 4, 4, 6, 6, 7, 7, 7, 8, 8, 9, 9, 3),
               cluster = c("1",
                           "1",
                           "1",
                           "1",
                           "2",
                           "2",
                           "1",
                           "2",
                           "2",
                           "3",
                           "3",
                           "3",
                           "3",
                           "3",
                           "3",
                           "3",
                           "3",
                           "3", 
                           "2"))
```

```{r Example, echo = FALSE, warning = FALSE, fig.height = 4, fig.width = 4.35}
base <- ggplot(data, aes(y = loyalty, x = csat)) +
  geom_point() +
  xlab("Customer satisfaction") +
  ylab("Loyalty") +
  xlim(c(0, 10)) +
  ylim(c(0, 10))

base
```

data modified from: http://www.segmentationstudyguide.com/using-cluster-analysis-for-market-segmentation/

From this data we might ask whether there are sub-groups within our customers? For example do we have customers with high loyalty and high satisfaction? What about low satisfaction and high loyalty? One way to answer such a question is to apply K-means clustering analysis. When we do such an analysis on this data set we identify 3 customer subgroups within our data set:

```{r clusteringExample, echo = FALSE, warning = FALSE, fig.height = 4, fig.width = 5}
ggplot(data, aes(y = loyalty, x = csat, color = cluster)) +
  geom_point() +
  xlab("Customer satisfaction") +
  ylab("Loyalty") +
  xlim(c(0, 10)) +
  ylim(c(0, 10))
```

What are the labels for these groups? We don't really have any, only cluster numbers are output from the clustering algorithm. In a simple case like this, where we can easily visualize the clusters on a scatter plot, we can give labels to these groups after clustering using the positions of the groups on the plot:

- low loyalty and low satisfaction (<font color="#00BA38">green cluster</font>),
- high loyalty and low satisfaction (<font color="#F8766D">pink cluster</font>), 
- and high loyalty and high satisfaction (<font color="#619CFF">blue cluster</font>).

Once we have such data we can use it to inform our future business decisions, and/or ask questions like, why did we not observe customers who had high satisfaction but low loyalty?

## K-means clustering algorithm

How does the K-means clustering algorithm work? Let's use the toy example shown above to illustrate it. First, we start by choosing $K$, the number of clusters. For this example we will choose 3. How do we choose $K$? It's an important question that we will answer later in this chapter. After choosing the number of clusters, we randomly assign a cluster centre (called a centroid) for each of the K clusters. We illustrate that below:

```{r randomCentroid, echo = FALSE, warning = FALSE, fig.height = 4, fig.width = 4.35}
set.seed(1234)
cent1 <- data.frame(csat = runif(1, min(data$csat), max(data$csat)),
                    loyalty = runif(1, min(data$loyalty), max(data$loyalty)))

cent2 <- data.frame(csat = runif(1, min(data$csat), max(data$csat)),
                    loyalty = runif(1, min(data$loyalty), max(data$loyalty)))

cent3 <- data.frame(csat = runif(1, min(data$csat), max(data$csat)),
                    loyalty = runif(1, min(data$loyalty), max(data$loyalty)))
base +
  geom_point(aes(x = cent1$csat, y = cent1$loyalty), color = "#F8766D", size = 2.5, shape = 25) +
  geom_point(aes(x = cent2$csat, y = cent2$loyalty), color = "#00BA38", size = 2.5) +
  geom_point(aes(x = cent3$csat, y = cent3$loyalty), color = "#619CFF", size = 2.5) 
```


Next, we assign the points to the cluster with the closest centroid based on straight line distance:

```{r iter1, echo = FALSE, warning = FALSE, fig.height = 4, fig.width = 5}
data1 <- tibble(loyalty = c(7, 5, 8, 7, 3, 1, 8, 4, 2, 7, 6, 7, 6, 5, 9, 7, 9, 5, 2),
               csat = c(1, 1, 2, 2, 2, 3, 4, 4, 4, 6, 6, 7, 7, 7, 8, 8, 9, 9, 3),
               cluster = c("1",
                           "1",
                           "1",
                           "1",
                           "1",
                           "1",
                           "1",
                           "1",
                           "2",
                           "2",
                           "2",
                           "3",
                           "3",
                           "3",
                           "3",
                           "3",
                           "3",
                           "3", 
                           "1"))

it1 <- ggplot(data1, aes(y = loyalty, x = csat, colour = cluster)) +
  geom_point() +
  xlab("Customer satisfaction") +
  ylab("Loyalty") +
  xlim(c(0, 10)) +
  ylim(c(0, 10)) 

it1 +
  geom_point(aes(x = cent1$csat, y = cent1$loyalty), color = "#F8766D", size = 2.5) +
  geom_point(aes(x = cent2$csat, y = cent2$loyalty), color = "#00BA38", size = 2.5) +
  geom_point(aes(x = cent3$csat, y = cent3$loyalty), color = "#619CFF", size = 2.5) 
```

Next, we re-adjust the position of the centroids to be the centre of the clusters:

```{r adjustCentroid, echo = FALSE, warning = FALSE, fig.height = 4, fig.width = 5}
old_cent1 <- cent1
clust1 <- data1 %>% 
  filter(cluster == "1")
cent1 <- data.frame(csat = mean(clust1$csat),
                    loyalty = mean(clust1$loyalty))
old_cent2 <- cent2
clust2 <- data1 %>% 
  filter(cluster == "2")
cent2 <- data.frame(csat = mean(clust2$csat),
                    loyalty = mean(clust2$loyalty))
old_cent3 <- cent3
clust3 <- data1 %>% 
  filter(cluster == "3")
cent3 <- data.frame(csat = mean(clust3$csat),
                    loyalty = mean(clust3$loyalty))

it1 +
  geom_point(aes(x = cent1$csat, y = cent1$loyalty), color = "#F8766D", size = 2.5) +
  geom_point(aes(x = cent2$csat, y = cent2$loyalty), color = "#00BA38", size = 2.5) +
  geom_point(aes(x = cent3$csat, y = cent3$loyalty), color = "#619CFF", size = 2.5) +
  geom_segment(aes(x = old_cent1$csat, y = old_cent1$loyalty, xend = cent1$csat, yend = cent1$loyalty), color = "black") +
  geom_segment(aes(x = old_cent2$csat, y = old_cent2$loyalty, xend = cent2$csat, yend = cent2$loyalty), color = "black") +
  geom_segment(aes(x = old_cent3$csat, y = old_cent3$loyalty, xend = cent3$csat, yend = cent3$loyalty), color = "black") +
  geom_point(aes(x = old_cent1$csat, y = old_cent1$loyalty), color = "#F8766D", size = 2.5, alpha = 0.01) +
  geom_point(aes(x = old_cent2$csat, y = old_cent2$loyalty), color = "#00BA38", size = 2.5, alpha = 0.01) +
  geom_point(aes(x = old_cent3$csat, y = old_cent3$loyalty), color = "#619CFF", size = 2.5, alpha = 0.01) 
```

Then we update cluster assignments of the points to the nearest cluster:

```{r iter2, echo = FALSE, warning = FALSE, fig.height = 4, fig.width = 5}
data2 <- tibble(loyalty = c(7, 5, 8, 7, 3, 1, 8, 4, 2, 7, 6, 7, 6, 5, 9, 7, 9, 5, 2),
               csat = c(1, 1, 2, 2, 2, 3, 4, 4, 4, 6, 6, 7, 7, 7, 8, 8, 9, 9, 3),
               cluster = c("1",
                           "1",
                           "1",
                           "1",
                           "1",
                           "1",
                           "1",
                           "2",
                           "2",
                           "3",
                           "2",
                           "3",
                           "3",
                           "2",
                           "3",
                           "3",
                           "3",
                           "3", 
                           "1"))

it2 <- ggplot(data2, aes(y = loyalty, x = csat, colour = cluster)) +
  geom_point() +
  xlab("Customer satisfaction") +
  ylab("Loyalty") +
  xlim(c(0, 10)) +
  ylim(c(0, 10)) 

it2 +
  geom_point(aes(x = cent1$csat, y = cent1$loyalty), color = "#F8766D", size = 2.5) +
  geom_point(aes(x = cent2$csat, y = cent2$loyalty), color = "#00BA38", size = 2.5) +
  geom_point(aes(x = cent3$csat, y = cent3$loyalty), color = "#619CFF", size = 2.5) 
```

And again we update the centroid position to be the centre of the new clusters:

```{r adjustCentroid2, echo = FALSE, warning = FALSE, fig.height = 4, fig.width = 5}
old_cent1 <- cent1
clust1 <- data2 %>% 
  filter(cluster == "1")
cent1 <- data.frame(csat = mean(clust1$csat),
                    loyalty = mean(clust1$loyalty))
old_cent2 <- cent2
clust2 <- data2 %>% 
  filter(cluster == "2")
cent2 <- data.frame(csat = mean(clust2$csat),
                    loyalty = mean(clust2$loyalty))
old_cent3 <- cent3
clust3 <- data2 %>% 
  filter(cluster == "3")
cent3 <- data.frame(csat = mean(clust3$csat),
                    loyalty = mean(clust3$loyalty))

it2 +
  geom_point(aes(x = cent1$csat, y = cent1$loyalty), color = "#F8766D", size = 2.5) +
  geom_point(aes(x = cent2$csat, y = cent2$loyalty), color = "#00BA38", size = 2.5) +
  geom_point(aes(x = cent3$csat, y = cent3$loyalty), color = "#619CFF", size = 2.5)  +
  geom_segment(aes(x = old_cent1$csat, y = old_cent1$loyalty, xend = cent1$csat, yend = cent1$loyalty), color = "black") +
  geom_segment(aes(x = old_cent2$csat, y = old_cent2$loyalty, xend = cent2$csat, yend = cent2$loyalty), color = "black") +
  geom_segment(aes(x = old_cent3$csat, y = old_cent3$loyalty, xend = cent3$csat, yend = cent3$loyalty), color = "black") +
  geom_point(aes(x = old_cent1$csat, y = old_cent1$loyalty), color = "#F8766D", size = 2.5, alpha = 0.01) +
  geom_point(aes(x = old_cent2$csat, y = old_cent2$loyalty), color = "#00BA38", size = 2.5, alpha = 0.01) +
  geom_point(aes(x = old_cent3$csat, y = old_cent3$loyalty), color = "#619CFF", size = 2.5, alpha = 0.01) 
```

And then next, we update the points again based on which centroid they are closest too. We do this over and over and over again until we get to a point where the centroids no longer change very much (or don't change at all) between iterations of the algorithm.

Watch the video linked to below for an explanation of the K-means clustering algorithm:
- https://www.coursera.org/lecture/machine-learning-data-analysis/what-is-a-k-means-cluster-analysis-p94tY

*note - when the add pops up to register for this course, you can just click to ignore it (i.e., no need to sign up to watch the entire video)*

## Additional readings:


- Pages 385-390 and 404-405 of [Introduction to Statistical Learning with Applications in R](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf) by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani and the companion video linked to below:

<iframe width="840" height="473" src="https://www.youtube.com/embed/aIybuNt9ps4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
