# Clustering {#clustering}

## Overview 
Introduction to clustering using K-means. Will discuss the K-means algorithm, how we choose K (the number of clusters) and other practical considerations (such as scaling).

## Learning objectives 
By the end of the chapter, students will be able to:

* Describe a case where clustering would be an appropriate tool, and what insight it would bring from the data.
* Explain the kmeans clustering algorithm.
* Interpret the output of a kmeans analysis.
* Perform kmeans clustering in R using `kmeans`.
* Visualize the output of kmeans clustering in R using pair-wise scatter plots.
* Identify when it is necessary to scale variables before clustering and do this using R.
* Use the elbow method to choose the number of clusters for k-means.
* Describe advantages, limitations and assumptions of the kmeans clustering algorithm.

## Clustering
While at first glance, clustering may seem very similar to classification, these two methods have some very important distinction. Most notably, classification is a supervised method (we use past information to predict the future values/labels for our target/response variable), whereas clustering is considered an unsupervised method (there is no target/response variable and we are looking to find sub-groups/clusters of observations based on how similar they are). So, where classification might be used to label future emails as spam or not spam, clustering might be instead be used to group emails into categories based on their similarity, however we would not have labels for these categories in the case of clustering. Another example problem we might try to solve with clustering is grouping Amazon customers into groups based upon their similar purchasing behaviours. Again here, we do not have, nor need, labels for customer groups.

Another way to think about it is, that classification is really about predicting something that you might have a scientific question about and/or hypothesis for, whereas, clustering is very often a hypothesis generating process (you identify things that are similar to each other that might be unexpected, and from those observations, you might generate a question and hypothesis that you might follow-up with classification).

Another major difference between clustering and classification is in how success is determined. With classification we are able to use a test data set to assess prediction performance, in clustering we must use variance metrics to determine how well our defined clusters fit the data. The two metrics used to determine success are between- and within- variation. Ideally we want clusters where the between-variance is large (so that the clusters are well separated) and the within- variation is small (so that the clusters are composed of close/tight-knit observations).

Below we show an example of an output of a K-means clustering analysis from a data set of customers where we have two explanatory variables: customer loyalty and customer satisfaction. From this analysis we identified 3 customer subgroups within our data set (note - we did not have these sub-group/cluster labels before we did our cluster analysis):


```{r echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
data <- tibble(loyalty = c(7, 5, 8, 7, 3, 1, 8, 4, 2, 7, 6, 7, 6, 5, 9, 7, 9, 5, 2),
               csat = c(1, 1, 2, 2, 2, 3, 4, 4, 4, 6, 6, 7, 7, 7, 8, 8, 9, 9, 3),
               cluster = c("1",
                           "1",
                           "1",
                           "1",
                           "2",
                           "2",
                           "1",
                           "2",
                           "2",
                           "3",
                           "3",
                           "3",
                           "3",
                           "3",
                           "3",
                           "3",
                           "3",
                           "3", 
                           "2"))
# data %>% 
#   select(loyalty, csat) %>% 
#   kable(format = "html", align = 'c') %>% 
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive", full_width = F))
```



```{r clusteringExample, echo = FALSE, warning = FALSE, fig.height = 4, fig.width = 5}
ggplot(data, aes(y = loyalty, x = csat, color = cluster)) +
  geom_point() +
  xlab("Customer satisfaction") +
  ylab("Loyalty") +
  xlim(c(0, 10)) +
  ylim(c(0, 10))
```

source: http://www.segmentationstudyguide.com/using-cluster-analysis-for-market-segmentation/

## Additional readings:
- Pages 385-390 and 404-405 of [Introduction to Statistical Learning with Applications in R](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf) by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani