{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textbook Data Retrieval "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides code to create the data sets used in this textbook. It also contains a summary of the data, their original source location, the license for the data, the date the data were accessed to generate the committed repository version of the original/processed data, and any other relevant meta-information.\n",
    "\n",
    "Running all cells in this notebook will:\n",
    "\n",
    "1. obtain all data from their original sources\n",
    "- validate the original data:\n",
    "    - if the original does not exist in the repository already, a warning will be generated and the newly obtained data will be stored.\n",
    "    - if the original data already exists and does not match with the newly downloaded data, a warning will be generated and the newly obtained data will be discarded.\n",
    "- process the data into the format(s) required for generating the textbook\n",
    "    - if a processed version of the data does not exist in the repository, a warning will be generated prior to storing the processed data.\n",
    "    - if the processed data already exists and does not match with the newly processed data, a warning will be generated and the newly processed data will be discarded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When adding a new data source to the textbook, it should be appended to the end of this notebook with a consistent notebook cell formatting / arrangement. The best way to do this is to copy and paste a block of cells for one of the examples below and then replace the information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the following Python3 packages to obtain and process data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm              #for progress bars\n",
    "import numpy as np       #for manipulating arrays\n",
    "import pandas as pd      #for loading/writing/manipulating tabular data\n",
    "import requests, ftplib  #for downloading files\n",
    "import os                #for handling files\n",
    "import hashlib           #for validating files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_data(data):\n",
    "    data_bytes = data.encode()\n",
    "    sha1 = hashlib.sha1()\n",
    "    sha1.update(data_bytes)\n",
    "    return sha1.hexdigest()\n",
    "\n",
    "def validate_data(new_data, stored_data, compare_hash, stored_filename):\n",
    "    print('Validating new/stored data')\n",
    "    new_hash = hash_data(new_data)\n",
    "    stored_hash = hash_data(stored_data)\n",
    "    print('Comparison hash:  ' + compare_hash)\n",
    "    print('New data hash:    ' + new_hash)\n",
    "    print('Stored data hash: ' + stored_hash)\n",
    "    new_valid = (hash_data(new_data) == compare_hash)\n",
    "    stored_valid = (hash_data(stored_data) == compare_hash)\n",
    "    data = None\n",
    "    if not new_valid:\n",
    "        print('The newly obtained data hash is different from the stored hash')\n",
    "        new_filename = stored_filename+'.new'\n",
    "        f = open(new_filename, 'w')\n",
    "        f.write(raw_data)\n",
    "        f.close()\n",
    "        print('New data was saved in ' + new_filename + '')\n",
    "        print('If you want to use the new data, you must replace ' + stored_filename + ' with \\\n",
    "               the contents of ' + new_filename + ' and update the hash in this notebook\\\n",
    "               to ' + hash_data(new_data))\n",
    "        if stored_valid:\n",
    "            print('Stored data hash matches; continuing with stored data.')\n",
    "            data = stored_data\n",
    "        else:\n",
    "            print('Stored data hash is also different from stored hash.')\n",
    "            print('Please follow the above directions and update the hash in this notebook to '+ hash_data(raw_data))\n",
    "    else:\n",
    "        print('Newly obtained data hash matches.')\n",
    "        if not stored_valid:\n",
    "            print('Stored data hash does not match; replacing stored data with newly obtained data')\n",
    "            f = open(stored_filename, 'w')\n",
    "            f.write(new_data)\n",
    "            f.close()\n",
    "        else:\n",
    "            print('Stored data hash matches too.')\n",
    "        print('Continuing with new data')\n",
    "        data = new_data\n",
    "    return data\n",
    "    \n",
    "def load_file(filename):\n",
    "    print('Loading ' + str(filename))\n",
    "    stored_data = ''\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            stored_data = f.read()\n",
    "    except Exception as e:\n",
    "        print('Exception while loading '+filename)\n",
    "        print(e)\n",
    "\n",
    "    return stored_data\n",
    "\n",
    "def download_ftp(url, folder_path, filename):\n",
    "    print('Downloading ' + filename + ' from ' + url)\n",
    "    raw_data = ''\n",
    "    try:\n",
    "        with ftplib.FTP(url) as ftp:\n",
    "            ftp.login()\n",
    "            ftp.cwd(folder_path)\n",
    "            resp = []\n",
    "            ftp.retrlines('RETR '+filename, callback = lambda ln : resp.append(ln))\n",
    "            raw_data = '\\n'.join(resp)\n",
    "    except Exception as e:\n",
    "        print('Exception while downloading ' + filename + ' from ' + url)\n",
    "        print(e)\n",
    "    \n",
    "    return raw_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mauna Loa CO2 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Source:** [National Ocean and Atmospheric Administration (NOAA)](https://www.esrl.noaa.gov/gmd/ccgg/trends/data.html)\n",
    "- **Data URL:** ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_weekly_mlo.txt\n",
    "- **Date Accessed:** July 4, 2020\n",
    "- **Attribution:** Dr. Pieter Tans, [NOAA/GML](www.esrl.noaa.gov/gmd/ccgg/trends/) and Dr. Ralph Keeling, [Scripps Institution of Oceanography](https://scrippsco2.ucsd.edu/).\n",
    "- **License:** Custom:\n",
    "\n",
    "\n",
    "```\n",
    "# --------------------------------------------------------------------\n",
    "# USE OF NOAA ESRL DATA\n",
    "# \n",
    "# These data are made freely available to the public and the\n",
    "# scientific community in the belief that their wide dissemination\n",
    "# will lead to greater understanding and new scientific insights.\n",
    "# The availability of these data does not constitute publication\n",
    "# of the data.  NOAA relies on the ethics and integrity of the user to\n",
    "# ensure that ESRL receives fair credit for their work.  If the data \n",
    "# are obtained for potential use in a publication or presentation, \n",
    "# ESRL should be informed at the outset of the nature of this work.  \n",
    "# If the ESRL data are essential to the work, or if an important \n",
    "# result or conclusion depends on the ESRL data, co-authorship\n",
    "# may be appropriate.  This should be discussed at an early stage in\n",
    "# the work.  Manuscripts using the ESRL data should be sent to ESRL\n",
    "# for review before they are submitted for publication so we can\n",
    "# ensure that the quality and limitations of the data are accurately\n",
    "# represented.\n",
    "# \n",
    "# Contact:   Pieter Tans (303 497 6678; pieter.tans@noaa.gov)\n",
    "# \n",
    "# File Creation:  Sat Jul  4 05:00:25 2020\n",
    "# \n",
    "# RECIPROCITY\n",
    "# \n",
    "# Use of these data implies an agreement to reciprocate.\n",
    "# Laboratories making similar measurements agree to make their\n",
    "# own data available to the general public and to the scientific\n",
    "# community in an equally complete and easily accessible form.\n",
    "# Modelers are encouraged to make available to the community,\n",
    "# upon request, their own tools used in the interpretation\n",
    "# of the ESRL data, namely well documented model code, transport\n",
    "# fields, and additional information necessary for other\n",
    "# scientists to repeat the work and to run modified versions.\n",
    "# Model availability includes collaborative support for new\n",
    "# users of the models.\n",
    "# --------------------------------------------------------------------\n",
    "#  \n",
    "#  \n",
    "# See www.esrl.noaa.gov/gmd/ccgg/trends/ for additional details.\n",
    "#  \n",
    "# NOTE: DATA FOR THE LAST SEVERAL MONTHS ARE PRELIMINARY, ARE STILL SUBJECT\n",
    "# TO QUALITY CONTROL PROCEDURES.\n",
    "# NOTE: The week \"1 yr ago\" is exactly 365 days ago, and thus does not run from\n",
    "# Sunday through Saturday. 365 also ignores the possibility of a leap year.\n",
    "# The week \"10 yr ago\" is exactly 10*365 days +3 days (for leap years) ago.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain / validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mauna_loa_raw.txt\n"
     ]
    }
   ],
   "source": [
    "#download data from source, load it from storage\n",
    "new_data = ftp_download('aftp.cmdl.noaa.gov', 'products/trends/co2/', 'co2_weekly_mlo.txt')\n",
    "stored_filename = 'mauna_loa_raw.txt'\n",
    "stored_data = load_file(stored_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating new/stored data\n",
      "Comparison hash:  464aa24fcd8cdb051d92361da956be3d3a2818eb\n",
      "New data hash:    464aa24fcd8cdb051d92361da956be3d3a2818eb\n",
      "Stored data hash: 464aa24fcd8cdb051d92361da956be3d3a2818eb\n",
      "Newly obtained data hash matches.\n",
      "Stored data hash matches too.\n",
      "Continuing with new data\n"
     ]
    }
   ],
   "source": [
    "#validate both newly downloaded and stored data\n",
    "compare_hash = '464aa24fcd8cdb051d92361da956be3d3a2818eb'\n",
    "data = validate_data(new_data, stored_data, compare_hash, stored_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mauna_loa.csv\n"
     ]
    }
   ],
   "source": [
    "# remove the lines beginning with # (these are for meta information)\n",
    "no_meta_info = [s for s in data.split('\\n') if s[0] != '#']\n",
    "# replace all whitespace with a single space, strip from beginning and end, keep only first 5 cols\n",
    "standardized_whitespace = [', '.join([num for num in s.strip().split(' ') if len(num)>0][:5]) for s in no_meta_info]\n",
    "# stitch together into a string with col names at the head\n",
    "clean_data = 'year, month, day, date_decimal, ppm\\n'+'\\n'.join(standardized_whitespace)\n",
    "\n",
    "#load previously processed data\n",
    "stored_filename = 'mauna_loa.csv'\n",
    "stored_clean_data = load_file(stored_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating new/stored data\n",
      "Comparison hash:  033baea66bf56351ad858e10ed7996c6ac7b9aa7\n",
      "New data hash:    033baea66bf56351ad858e10ed7996c6ac7b9aa7\n",
      "Stored data hash: 033baea66bf56351ad858e10ed7996c6ac7b9aa7\n",
      "Newly obtained data hash matches.\n",
      "Stored data hash matches too.\n",
      "Continuing with new data\n"
     ]
    }
   ],
   "source": [
    "#validate the preprocessed data\n",
    "compare_hash = '033baea66bf56351ad858e10ed7996c6ac7b9aa7'\n",
    "data = validate_data(clean_data, stored_clean_data, compare_hash, stored_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Island Landmasses Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical Vote Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wisconsin Breast Cancer Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Property Vote Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Income and Housing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
