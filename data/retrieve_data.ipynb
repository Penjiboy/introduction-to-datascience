{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textbook Data Retrieval "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides code to create the data sets used in this textbook. It also contains a summary of the data, their original source location, the license for the data, the date the data were accessed to generate the committed repository version of the original/processed data, and any other relevant meta-information.\n",
    "\n",
    "Running all cells in this notebook will:\n",
    "\n",
    "1. obtain all data from their original sources\n",
    "- validate the original data:\n",
    "    - if the original does not exist in the repository already, a warning will be generated and the newly obtained data will be stored.\n",
    "    - if the original data already exists and does not match with the newly downloaded data, a warning will be generated and the newly obtained data will be discarded.\n",
    "- process the data into the format(s) required for generating the textbook\n",
    "    - if a processed version of the data does not exist in the repository, a warning will be generated prior to storing the processed data.\n",
    "    - if the processed data already exists and does not match with the newly processed data, a warning will be generated and the newly processed data will be discarded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Name** | **Chapters**| **R Dataset** | **Remote Database** |\n",
    "| -------- | ----------- | ----------- | ---------------- |\n",
    "| US 2016 Census/Vote Data | 1, 2, 3 | N | N |\n",
    "| Canadian Movies | 2 | N | Y |\n",
    "| Historical US Vote | 3 | N | N |\n",
    "| mtcars | 3 | Y| N |\n",
    "| Mauna Loa CO2 | 4 | N | N |\n",
    "| Islands | 4 | Y | N |\n",
    "| Old Faithful | 4 | Y | N |\n",
    "| Speed of Light | 4 | Y | N |\n",
    "| Wisconsin Breast Cancer | 6, 7 | N | N |\n",
    "| Sacramento Real Estate | 8, 9 | Y | N | \n",
    "| Marketing Data | 10 | N | N |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the following Python3 packages to obtain and process data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np       #for manipulating arrays\n",
    "import pandas as pd      #for loading/writing/manipulating tabular data\n",
    "import requests, ftplib  #for downloading files\n",
    "import os                #for handling files\n",
    "import hashlib           #for validating files\n",
    "import io                #for creating byte streams for xlsx files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "\n",
    "# This function takes in a string and outputs its SHA1 hash\n",
    "def hash_data(data):\n",
    "    data_bytes = data.encode()\n",
    "    sha1 = hashlib.sha1()\n",
    "    sha1.update(data_bytes)\n",
    "    return sha1.hexdigest()\n",
    "\n",
    "# This function takes in new/stored data strings, a hash to compare, and a filename to store data in\n",
    "# If the new/stored data have a matching hash with compare_hash, just return the data\n",
    "# If the stored one matches but new one doesn't, output a message with instructions on how to update the data and continue with old data\n",
    "# If the new one matches but the stored one doesn't, just overwrite the stored data\n",
    "def validate_data(new_data, stored_data, compare_hash, stored_filename):\n",
    "    print('Validating new/stored data')\n",
    "    new_hash = hash_data(new_data)\n",
    "    stored_hash = hash_data(stored_data)\n",
    "    print('Comparison hash:  ' + compare_hash)\n",
    "    print('New data hash:    ' + new_hash)\n",
    "    print('Stored data hash: ' + stored_hash)\n",
    "    new_valid = (hash_data(new_data) == compare_hash)\n",
    "    stored_valid = (hash_data(stored_data) == compare_hash)\n",
    "    \n",
    "    data = None\n",
    "    if not new_valid:\n",
    "        print('The newly obtained data hash does not match')\n",
    "        if len(new_data) == 0:\n",
    "            print('The new data is empty, please check for errors in downloading')\n",
    "        else:\n",
    "            new_filename = stored_filename+'.new'\n",
    "            f = open(new_filename, 'w')\n",
    "            f.write(new_data)\n",
    "            f.close()\n",
    "            print('New data was saved in ' + new_filename)\n",
    "            print('If you want to use the new data, you must replace ' + stored_filename + ' with the contents of ' + new_filename + ' and update the hash in this notebook to ' + new_hash)\n",
    "        \n",
    "        if stored_valid:\n",
    "            print('Stored data hash matches; continuing with stored data.')\n",
    "            data = stored_data\n",
    "        else:\n",
    "            print('Stored data hash also does not match.')\n",
    "            print('Please follow the above directions and update the hash in this notebook')\n",
    "    else:\n",
    "        print('Newly obtained data hash matches.')\n",
    "        if not stored_valid:\n",
    "            print('Stored data hash does not match')\n",
    "            if len(stored_data) == 0:\n",
    "                print('Stored data is empty')\n",
    "            else:\n",
    "                old_filename = stored_filename+'.old'\n",
    "                f = open(old_filename, 'w')\n",
    "                f.write(stored_data)\n",
    "                f.close()\n",
    "                print('Moved stored data to ' + old_filename)\n",
    "                print('If you want to use the old data, you must replace ' + stored_filename + ' with the contents of ' + new_filename + ' and update the hash in this notebook to ' + stored_hash)\n",
    "            f = open(stored_filename, 'w')\n",
    "            f.write(new_data)\n",
    "            f.close()\n",
    "            print('Saved new data to ' + stored_filename)\n",
    "        else:\n",
    "            print('Stored data hash matches too.')\n",
    "        print('Continuing with new data')\n",
    "        data = new_data\n",
    "    return data\n",
    "    \n",
    "def load_file(filename):\n",
    "    print('Loading ' + str(filename))\n",
    "    stored_data = ''\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            stored_data = f.read()\n",
    "    except Exception as e:\n",
    "        print('Exception while loading '+filename)\n",
    "        print(e)\n",
    "\n",
    "    return stored_data\n",
    "\n",
    "def download_ftp(url, folder_path, filename):\n",
    "    print('Downloading ' + filename + ' from ' + url)\n",
    "    raw_data = ''\n",
    "    try:\n",
    "        with ftplib.FTP(url) as ftp:\n",
    "            ftp.login()\n",
    "            ftp.cwd(folder_path)\n",
    "            resp = []\n",
    "            ftp.retrlines('RETR '+filename, callback = lambda ln : resp.append(ln))\n",
    "            raw_data = '\\n'.join(resp)\n",
    "    except Exception as e:\n",
    "        print('Exception while downloading ' + filename + ' from ' + url)\n",
    "        print(e)\n",
    "    \n",
    "    return raw_data\n",
    "\n",
    "def download_http(url):\n",
    "    return requests.get(url).content.decode('utf-8')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mauna Loa CO2 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Source:** [National Ocean and Atmospheric Administration (NOAA)](https://www.esrl.noaa.gov/gmd/ccgg/trends/data.html)\n",
    "- **Data URL:** ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_weekly_mlo.txt\n",
    "- **Date Accessed:** July 4, 2020\n",
    "- **Attribution:** Dr. Pieter Tans, [NOAA/GML](www.esrl.noaa.gov/gmd/ccgg/trends/) and Dr. Ralph Keeling, [Scripps Institution of Oceanography](https://scrippsco2.ucsd.edu/).\n",
    "- **Where Used:** Chapters X, Y, Z\n",
    "- **License:** Custom:\n",
    "\n",
    "\n",
    "```\n",
    "# --------------------------------------------------------------------\n",
    "# USE OF NOAA ESRL DATA\n",
    "# \n",
    "# These data are made freely available to the public and the\n",
    "# scientific community in the belief that their wide dissemination\n",
    "# will lead to greater understanding and new scientific insights.\n",
    "# The availability of these data does not constitute publication\n",
    "# of the data.  NOAA relies on the ethics and integrity of the user to\n",
    "# ensure that ESRL receives fair credit for their work.  If the data \n",
    "# are obtained for potential use in a publication or presentation, \n",
    "# ESRL should be informed at the outset of the nature of this work.  \n",
    "# If the ESRL data are essential to the work, or if an important \n",
    "# result or conclusion depends on the ESRL data, co-authorship\n",
    "# may be appropriate.  This should be discussed at an early stage in\n",
    "# the work.  Manuscripts using the ESRL data should be sent to ESRL\n",
    "# for review before they are submitted for publication so we can\n",
    "# ensure that the quality and limitations of the data are accurately\n",
    "# represented.\n",
    "# \n",
    "# Contact:   Pieter Tans (303 497 6678; pieter.tans@noaa.gov)\n",
    "# \n",
    "# File Creation:  Sat Jul  4 05:00:25 2020\n",
    "# \n",
    "# RECIPROCITY\n",
    "# \n",
    "# Use of these data implies an agreement to reciprocate.\n",
    "# Laboratories making similar measurements agree to make their\n",
    "# own data available to the general public and to the scientific\n",
    "# community in an equally complete and easily accessible form.\n",
    "# Modelers are encouraged to make available to the community,\n",
    "# upon request, their own tools used in the interpretation\n",
    "# of the ESRL data, namely well documented model code, transport\n",
    "# fields, and additional information necessary for other\n",
    "# scientists to repeat the work and to run modified versions.\n",
    "# Model availability includes collaborative support for new\n",
    "# users of the models.\n",
    "# --------------------------------------------------------------------\n",
    "#  \n",
    "#  \n",
    "# See www.esrl.noaa.gov/gmd/ccgg/trends/ for additional details.\n",
    "#  \n",
    "# NOTE: DATA FOR THE LAST SEVERAL MONTHS ARE PRELIMINARY, ARE STILL SUBJECT\n",
    "# TO QUALITY CONTROL PROCEDURES.\n",
    "# NOTE: The week \"1 yr ago\" is exactly 365 days ago, and thus does not run from\n",
    "# Sunday through Saturday. 365 also ignores the possibility of a leap year.\n",
    "# The week \"10 yr ago\" is exactly 10*365 days +3 days (for leap years) ago.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading co2_weekly_mlo.txt from aftp.cmdl.noaa.gov\n",
      "Loading mauna_loa_raw.txt\n"
     ]
    }
   ],
   "source": [
    "def retrieve_mauna_loa(data):\n",
    "    data = download_ftp('aftp.cmdl.noaa.gov', 'products/trends/co2/', 'co2_weekly_mlo.txt')\n",
    "    # remove the lines beginning with # (these are for meta information)\n",
    "    no_meta_info = [s for s in data.split('\\n') if s[0] != '#']\n",
    "    # replace all whitespace with a single space, strip from beginning and end, keep only first 5 cols\n",
    "    standardized_whitespace = [', '.join([num for num in s.strip().split(' ') if len(num)>0][:5]) for s in no_meta_info]\n",
    "    # stitch together into a string with col names at the head\n",
    "    clean_data = 'year, month, day, date_decimal, ppm\\n'+'\\n'.join(standardized_whitespace)\n",
    "    return clean_data\n",
    "\n",
    "datasets['mauna_loa'] = {}\n",
    "datasets['mauna_loa']['compare_hash'] = '033baea66bf56351ad858e10ed7996c6ac7b9aa7'\n",
    "datasets['mauna_loa']['retrieve_data'] = retrieve_mauna_loa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wisconsin Breast Cancer Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Source:** [The UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)) \n",
    "- **Date Accessed:** July 4, 2020\n",
    "- **Where Used:** Chapters X, Y, Z\n",
    "- **License:** Custom:\n",
    "\n",
    "- **Source:** [The UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))\n",
    "- **Data URL:**\n",
    "    - Names: https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names\n",
    "    - Data: https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\n",
    "- **Date Accessed:** July 5, 2020\n",
    "- **Attribution:** Dr. William H. Wolberg, W. Nick Street, and Olvi L. Mangasarian, University of Wisconsin.\n",
    "- **Where Used:** Chapters X, Y, Z\n",
    "- **License:** Custom:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_wdbc(data):\n",
    "    data = download_http('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data')\n",
    "    #create list of variable names\n",
    "    names = ['Class', 'Radius', 'Texture', 'Perimeter', 'Area', 'Smoothness', 'Compactness', 'Concavity', 'Concave Points', 'Symmetry', 'Fractal Dimension']\n",
    "    #remove all but the class label (B/M) and first 10 entries (means of each value)\n",
    "    data_lines = [line.split(',')[1:12] for line in data.split('\\n')]\n",
    "    clean_data = ','.join(names) + '\\n' + '\\n'.join([','.join(line) for line in data_lines])\n",
    "    return clean_data\n",
    "    \n",
    "\n",
    "datasets['wdbc'] = {}\n",
    "datasets['wdbc']['compare_hash'] = '033baea66bf56351ad858e10ed7996c6ac7b9aa7'\n",
    "datasets['wdbc']['retrieve_data'] = retrieve_wdbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Island Landmasses Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Source:** The World Almanac and Book of Facts, 1975, page 406. See https://stat.ethz.ch/R-manual/R-patched/library/datasets/html/islands.html \n",
    "- **Date Accessed:** July 4, 2020\n",
    "- **Where Used:** Chapters X, Y, Z\n",
    "- **License:** Custom:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_islands():\n",
    "    return load_file('islands_raw.txt')\n",
    "\n",
    "datasets['islands'] = {}\n",
    "datasets['islands']['compare_hash'] = '033baea66bf56351ad858e10ed7996c6ac7b9aa7'\n",
    "datasets['islands']['retrieve_data'] = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US 2016 Census / Vote Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_state_property_vote():\n",
    "    #add a user agent header so that the API doesn't return 403\n",
    "    useragent = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "    #create the query URL\n",
    "    state_data_url = \"http://datausa.io/api/data?drilldowns=State&measure=Average Commute Time,Property Value,Median Household Income,Population&year=2016\"\n",
    "    state_data = requests.get(state_data_url, headers=useragent).json()['data']\n",
    "    #convert to a list of lists, removing DC, Puerto Rico\n",
    "    #format numerical entries as either a 2-decimal float or integer\n",
    "    colnames = ['State', 'Population', 'Property Value', 'Median Household Income', 'Average Commute Time']\n",
    "    datalines = [[d[colname] if type(d[colname]) == str else format(d[colname], '.2f').rstrip('0').rstrip('.') for colname in colnames] for d in state_data if d['State'] != 'Puerto Rico' and d['State'] != 'District of Columbia']\n",
    "    #obtain general presidential election 2016 results data\n",
    "    elec_data_url = \"https://www.fec.gov/documents/1890/federalelections2016.xlsx\"\n",
    "    stream = io.BytesIO(requests.get(elec_data_url).content)\n",
    "    elec_data = pd.read_excel(stream,sheet_name=8)\n",
    "    #extract one row per state with winner\n",
    "    elec_data = elec_data[elec_data['WINNER INDICATOR'] == 'W']\n",
    "    elec_dict = {}\n",
    "    for i in range(elec_data.shape[0]):\n",
    "        party = elec_data.iloc[i]['PARTY']\n",
    "        #some rows assign the \"winner\" to combined parties; check the winner name for these rows\n",
    "        #otherwise convert DEM/REP to long form names\n",
    "        if party == 'Combined Parties:':\n",
    "            if elec_data.iloc[i]['LAST NAME'] == 'Trump':\n",
    "                party = 'Republican'\n",
    "            else:\n",
    "                party = 'Democratic'\n",
    "        elif party == 'REP':\n",
    "            party = 'Republican'\n",
    "        elif party == 'DEM':\n",
    "            party = 'Democratic'\n",
    "        elec_dict[elec_data.iloc[i]['STATE']] = party\n",
    "    #combine the two data\n",
    "    data = [','.join(colnames+['Party'])]\n",
    "    for line in datalines:\n",
    "        data.append(','.join(line)+',' + elec_dict[line[0]])\n",
    "    print('\\n'.join(data))\n",
    "    \n",
    "\n",
    "datasets['state_property_vote'] = {}\n",
    "datasets['state_property_vote']['compare_hash'] = '033baea66bf56351ad858e10ed7996c6ac7b9aa7'\n",
    "datasets['state_property_vote']['retrieve_data'] = retrieve_state_property_vote\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
