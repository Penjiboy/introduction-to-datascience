[
["regression3.html", "Chapter 10 Regression, continued some more… 10.1 Overview 10.2 Learning objectives 10.3 Multivariate k-nn regression", " Chapter 10 Regression, continued some more… 10.1 Overview This week we will work through some examples of multiple regression in the prediction context. We will emphasize the interpretation and relevance of the mix of negative/positive slopes in this context. We will then discuss another application of regression; modelling the relationship between two or more variables so that we can better understand or describe it. We will emphasize that this is a jumping off point for the study of statistical inference. 10.2 Learning objectives By the end of the chapter, students will be able to: In a dataset with &gt; 2 variables, perform k-nn regression in R using caret’s train with method = &quot;k-nn&quot; to predict the values for a test dataset. In a dataset with &gt; 2 variables, perform simple ordinary least squares regression in R using caret’s train with method = &quot;lm&quot; to predict the values for a test dataset. Explain in words or using drawings, how bootstrap samples can be generated from a single sample. From a single sample, generate multiple samples through bootstrapping using R. In R, overlay the ordinary least squares regression lines from multiple bootstrapped samples on a single plot. Discuss what the collection of these slopes represents. 10.3 Multivariate k-nn regression As in k-nn classification, in k-nn regression we can have multiple predictors. When we have multiple predictors in k-nn regression, we have the same concern regarding the scale of the predictors. This is because as in k-nn classification, in k-nn regression predictions are made by identifying the \\(k\\) observations that are nearest to the new point we want to predict, and any variables that are on a large scale will have a much larger effect than variables on a small scale. Thus, once we start performing multivariate k-nn regression we need to use the scale function in R on our predictors to ensure this doesn’t happen. We will now demonstrate a multi-variate k-nn regression analysis usin the caret package on the Sacramento real estate data (also used in the the last two chapters in this book). This time we will use house size (measured in square feet) as well as number of bathrooms as our predictors, and continue to use house sale price as our outcome/target variable that we are trying to predict. Let’s first load the libraries and the data: library(tidyverse) library(caret) library(GGally) data(&quot;Sacramento&quot;) head(Sacramento) ## city zip beds baths sqft type price latitude longitude ## 1 SACRAMENTO z95838 2 1 836 Residential 59222 38.63191 -121.4349 ## 2 SACRAMENTO z95823 3 1 1167 Residential 68212 38.47890 -121.4310 ## 3 SACRAMENTO z95815 2 1 796 Residential 68880 38.61830 -121.4438 ## 4 SACRAMENTO z95815 2 1 852 Residential 69307 38.61684 -121.4391 ## 5 SACRAMENTO z95824 2 1 797 Residential 81900 38.51947 -121.4358 ## 6 SACRAMENTO z95841 3 1 1122 Condo 89921 38.66260 -121.3278 It is always a good practice to do exploratory data analysis, such as visualizing the data, before we start modeling the data. Thus the first thing we will do is use ggpairs (from the GGally package) to plot all the variables we are interested in using in our analyses: plot_pairs &lt;- Sacramento %&gt;% select(price, sqft, baths) %&gt;% ggpairs() plot_pairs From this we can see that generally, as both house size and number of bathrooms increase, so does price. Does adding the number of baths to our model improve our ability to predict house price? To answer that question, we will have to come up with the test error for a k-nn regression model using house size and number of baths, and then we can compare it to the test error for the model we previously came up with that only used house size to see if it is smaller (decreased test error indicates increased prediction quality). Let’s do that now! Looking at the data above, we can see that sqft and beds (number of bedrooms) are on vastly different scales. Thus we need to apply the scale function to these columns before we start our analysis: scaled_Sacramento &lt;- Sacramento %&gt;% select(price, sqft, baths) %&gt;% mutate(sqft = scale(sqft, center = FALSE), baths = scale(baths, center = FALSE)) head(scaled_Sacramento) ## price sqft baths ## 1 59222 0.4564854 0.459221 ## 2 68212 0.6372231 0.459221 ## 3 68880 0.4346440 0.459221 ## 4 69307 0.4652220 0.459221 ## 5 81900 0.4351901 0.459221 ## 6 89921 0.6126515 0.459221 Now we can split our data into a trained and test set as we did before: set.seed(2019) # makes the random selection of rows reproducible training_rows &lt;- scaled_Sacramento %&gt;% select(price) %&gt;% unlist() %&gt;% # converts Class from a tibble to a vector createDataPartition(p = 0.6, list = FALSE) X_train &lt;- scaled_Sacramento %&gt;% select(sqft, baths) %&gt;% slice(training_rows) %&gt;% data.frame() Y_train &lt;- scaled_Sacramento %&gt;% select(price) %&gt;% slice(training_rows) %&gt;% unlist() X_test &lt;- scaled_Sacramento %&gt;% select(sqft, baths) %&gt;% slice(-training_rows) %&gt;% data.frame() Y_test &lt;- scaled_Sacramento %&gt;% select(price) %&gt;% slice(-training_rows) %&gt;% unlist() Next, we’ll use 10-fold cross-validation to choose \\(k\\): train_control &lt;- trainControl(method = &quot;cv&quot;, number = 10) # makes a column of k&#39;s, from 1 to 100 in increments of 10 k_lots = data.frame(k = seq(from = 1, to = 500, by = 5)) set.seed(1234) knn_reg_cv_10 &lt;- train(x = X_train, y = Y_train, method = &quot;knn&quot;, tuneGrid = k_lots, trControl = train_control) ggplot(knn_reg_cv_10$results, aes(x = k, y = RMSE)) + geom_point() + geom_line() knn_reg_cv_10$method ## [1] &quot;knn&quot; Here we see that the smallest \\(RMSE\\) is from the model where \\(k\\) = 31. Thus the best \\(k\\) for this model, with two predictors, is 31. Now that we have chosen \\(k\\), we need to re-train the model on the entire training data set with \\(k\\) = 31, and after that we can use that model to predict on the test data to get our test error. At that point we will also visualize the model predictions overlaid on top of the data. This time the predictions will be a plane in 3-D space, instead of a line in 2-D space, as we have 2 predictors instead of 3. k = data.frame(k = 31) set.seed(1234) knn_mult_reg_final &lt;- train(x = X_train, y = Y_train, method = &quot;knn&quot;, tuneGrid = k) test_pred &lt;- predict(knn_mult_reg_final, X_test) modelvalues &lt;- data.frame(obs = Y_test, pred = test_pred) knn_mult_test_results &lt;- defaultSummary(modelvalues) knn_mult_test_results[[1]] ## [1] 90108.49 This time when we performed k-nn regression on the same data set, but also included number of bathrooms as a predictor we obtained a RMSPE test error of 90108.49. This compares to a RMSPE test error of 91620.40 when we used only house size as the single predictor. What do the predictions from this model look like overlaid on the data? ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout # library(tidyverse) # library(plotly) # library(moderndive) # # # # Preprocess data --------------------------------------------------------- # set.seed(76) # data(house_prices) # # # Re-scale price and size (square footage) by log10 and take a subsample of points # house_prices &lt;- house_prices %&gt;% # mutate( # log10_price = log10(price), # log10_size = log10(sqft_living) # ) %&gt;% # sample_n(500) # # # # Define 3D scatterplot points -------------------------------------------- # # Get coordinates of points for 3D scatterplot # x_values &lt;- house_prices$log10_size %&gt;% # round(3) # y_values &lt;- house_prices$yr_built %&gt;% # round(3) # z_values &lt;- house_prices$log10_price %&gt;% # round(3) # # # # Define regression plane ------------------------------------------------- # # Construct x and y grid elements # x_grid &lt;- seq(from = min(x_values), to = max(x_values), length = 50) # y_grid &lt;- seq(from = min(y_values), to = max(y_values), length = 50) # # # Construct z grid by computing # # 1) fitted beta coefficients # # 2) fitted values of outer product of x_grid and y_grid # # 3) extracting z_grid (matrix needs to be of specific dimensions) # beta_hat &lt;- house_prices %&gt;% # lm(log10_price ~ log10_size + yr_built, data = .) %&gt;% # coef() # fitted_values &lt;- crossing(y_grid, x_grid) %&gt;% # mutate(z_grid = beta_hat[1] + beta_hat[2]*x_grid + beta_hat[3]*y_grid) # z_grid &lt;- fitted_values %&gt;% # pull(z_grid) %&gt;% # matrix(nrow = length(x_grid)) %&gt;% # t() # # # Define text element for each point in plane # text_grid &lt;- fitted_values %&gt;% # pull(z_grid) %&gt;% # round(3) %&gt;% # as.character() %&gt;% # paste(&quot;log10(price): &quot;, ., sep = &quot;&quot;) %&gt;% # matrix(nrow = length(x_grid)) %&gt;% # t() # # # # Plot using plotly ------------------------------------------------------- # plot_ly() %&gt;% # # 3D scatterplot: # add_markers( # x = x_values, # y = y_values, # z = z_values, # marker = list(size = 5)#, # #hoverinfo = &#39;text&#39;, # #text = ~paste( # # &quot;log10(price): &quot;, z_values, &quot;&lt;br&gt;&quot;, # # &quot;year: &quot;, y_values, &quot;&lt;br&gt;&quot;, # # &quot;log10(size): &quot;, x_values # #) # ) %&gt;% # # Regression plane: # add_surface( # x = x_grid, # y = y_grid, # z = z_grid#, # #hoverinfo = &#39;text&#39;, # #text = text_grid # ) #%&gt;% # # Axes labels and title: # # layout( # # title = &quot;3D scatterplot and regression plane&quot;, # # scene = list( # # zaxis = list(title = &quot;y: log10(price)&quot;), # # yaxis = list(title = &quot;x2: year&quot;), # # xaxis = list(title = &quot;x1: log10(size)&quot;) # # ) # # ) "]
]
