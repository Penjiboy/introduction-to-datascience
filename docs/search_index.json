[
["clustering.html", "Chapter 11 Clustering 11.1 Overview 11.2 Learning objectives 11.3 Clustering 11.4 K-means clustering algorithm 11.5 K-means clustering in R 11.6 Choosing K for K-means clustering 11.7 Additional readings:", " Chapter 11 Clustering 11.1 Overview Introduction to clustering using K-means. Will discuss the K-means algorithm, how we choose K (the number of clusters) and other practical considerations (such as scaling). 11.2 Learning objectives By the end of the chapter, students will be able to: Describe a case where clustering would be an appropriate tool, and what insight it would bring from the data. Explain the K-means clustering algorithm. Interpret the output of a K-means analysis. Perform kmeans clustering in R using kmeans. Visualize the output of K-means clustering in R using pair-wise scatter plots. Identify when it is necessary to scale variables before clustering and do this using R. Use the elbow method to choose the number of clusters for K-means. Describe advantages, limitations and assumptions of the K-means clustering algorithm. 11.3 Clustering While at first glance, clustering may seem very similar to classification, these two methods have some very important distinction. Most notably, classification is a supervised method (we use past information to predict the future values/labels for our target/response variable), whereas clustering is considered an unsupervised method (there is no target/response variable and we are looking to find sub-groups/clusters of observations based on how similar they are). So, where classification might be used to label future emails as spam or not spam, clustering might be instead be used to group emails into categories based on their similarity, however we would not have labels for these categories in the case of clustering. Another example problem we might try to solve with clustering is grouping Amazon customers into groups based upon their similar purchasing behaviours. Again here, we do not have, nor need, labels for customer groups. Another way to think about it is, that classification is really about predicting something that you might have a scientific question about and/or hypothesis for, whereas, clustering is very often a hypothesis generating process (you identify things that are similar to each other that might be unexpected, and from those observations, you might generate a question and hypothesis that you might follow-up with classification). Another major difference between clustering and classification is in how success is determined. With classification we are able to use a test data set to assess prediction performance, in clustering we must use variance metrics to determine how well our defined clusters fit the data. The two metrics used to determine success are between- and within- variation. Ideally we want clusters where the between-variance is large (so that the clusters are well separated) and the within- variation is small (so that the clusters are composed of close/tight-knit observations). 11.3.1 A toy example What if we had some customer data, and we wanted to learn more about the types of customers we had so that we could come up with better products and/or promotions to increase our business in a data-driven way. For example, let’s consider this data below, where we have assessed customer loyalty and customer satisfaction: data modified from: http://www.segmentationstudyguide.com/using-cluster-analysis-for-market-segmentation/ From this data we might ask whether there are sub-groups within our customers? For example do we have customers with high loyalty and high satisfaction? What about low satisfaction and high loyalty? One way to answer such a question is to apply K-means clustering analysis. When we do such an analysis on this data set we identify 3 customer subgroups within our data set: What are the labels for these groups? We don’t really have any, only cluster numbers are output from the clustering algorithm. In a simple case like this, where we can easily visualize the clusters on a scatter plot, we can give labels to these groups after clustering using the positions of the groups on the plot: low loyalty and low satisfaction (green cluster), high loyalty and low satisfaction (pink cluster), and high loyalty and high satisfaction (blue cluster). Once we have such data we can use it to inform our future business decisions, and/or ask questions like, why did we not observe customers who had high satisfaction but low loyalty? 11.4 K-means clustering algorithm Watch the video linked to below for an explanation of the K-means clustering algorithm: - https://www.coursera.org/lecture/machine-learning-data-analysis/what-is-a-k-means-cluster-analysis-p94tY note - when the add pops up to register for this course, you can just click to ignore it (i.e., no need to sign up to watch the entire video) 11.5 K-means clustering in R Let’s take a look at the data we plotted above: head(marketing_data) ## # A tibble: 6 x 2 ## loyalty csat ## &lt;dbl&gt; &lt;dbl&gt; ## 1 7 1 ## 2 7.5 1 ## 3 8 2 ## 4 7 2 ## 5 3 2 ## 6 1 3 To peform Kmeans clustering in R, we use the kmeans function. It takes at least two arguments, the data frame containing the data you wish to cluster, and K, the number of clusters (here we choose K = 3). Given that the K-means algorithm uses a random start to begin the algorithm, to make this reproducible, we need to set the seed. set.seed(1234) marketing_clust &lt;- kmeans(marketing_data, centers = 3) marketing_clust ## K-means clustering with 3 clusters of sizes 5, 9, 5 ## ## Cluster means: ## loyalty csat ## 1 7.500000 1.800000 ## 2 6.777778 7.444444 ## 3 2.400000 3.200000 ## ## Clustering vector: ## [1] 1 1 1 1 3 3 1 3 3 2 2 2 2 2 2 2 2 2 3 ## ## Within cluster sum of squares by cluster: ## [1] 3.80000 27.77778 8.00000 ## (between_SS / total_SS = 83.6 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; ## [5] &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; ## [9] &quot;ifault&quot; As you can see above, the clustering object returned has a lot of information about our analysis that we need to explore. Let’s take a look at it now. To do this, we will call in help from the broom package so that we get the model output back in a tidy data format. Let’s first start by getting the cluster identification for each point and plotting that on the scatter plot. To do that we use the augment function. Augment takes in the model and the original data frame, and returns a data frame with the data and the cluster assignments for each point: library(broom) clustered_data &lt;- augment(marketing_clust, marketing_data) head(clustered_data) ## loyalty csat .cluster ## 1 7.0 1 1 ## 2 7.5 1 1 ## 3 8.0 2 1 ## 4 7.0 2 1 ## 5 3.0 2 3 ## 6 1.0 3 3 Now that we have this data frame, we can easily plot the data (i.e., cluster assignments of each point): cluster_plot &lt;- ggplot(clustered_data, aes(x = csat, y = loyalty, colour = .cluster)) + geom_point() cluster_plot 11.6 Choosing K for K-means clustering As mentioned above, we need to choose a K to perform K-means clustering. How should we choose K? We have no data labels, and so cannot perform cross-validation with some measure of model prediction error, so what can we do? What we can do in this situation is to look at the total within-cluster sum of squares for differnt K’s and choose the K that gives the biggest decrease in the total within-cluster sum of squares. Why total within-cluster sum of squares? This statistic lets us know how close/tight-knit (or compact) observations are within clusters. A larger number means that clusters are not close/tight-knit, but are instead more spread out. A smaller number here means that clusters are indeed close/tight-knit together. We can get at the total within-cluster sum of squares (tot.withinss) from our clustering using broom’s glance function (it gives model-level statistics). For example: glance(marketing_clust) ## totss tot.withinss betweenss iter ## 1 241.1316 39.57778 201.5538 2 Let’s calculate the total within-cluster sum of squares for our data for a variety of K’s (say 1 - 9) and then plot them against K. To do this we will create a data frame with a column named k, for each of the K’s we want to try our clustering with. Then we use map to apply the kmeans function to each K. We also use map to then apply glance to each of the clustering models we performed (one for each K). In the end we end up with a complex data frame with 3 columns, one for K, one for the models, and one for the model statistics (output of glance, which is a data frame): marketing_clust_ks &lt;- tibble(k = 1:9) %&gt;% mutate(marketing_clusts = map(k, ~kmeans(marketing_data, .x)), glanced = map(marketing_clusts, glance)) head(marketing_clust_ks) ## # A tibble: 6 x 3 ## k marketing_clusts glanced ## &lt;int&gt; &lt;list&gt; &lt;list&gt; ## 1 1 &lt;S3: kmeans&gt; &lt;data.frame [1 × 4]&gt; ## 2 2 &lt;S3: kmeans&gt; &lt;data.frame [1 × 4]&gt; ## 3 3 &lt;S3: kmeans&gt; &lt;data.frame [1 × 4]&gt; ## 4 4 &lt;S3: kmeans&gt; &lt;data.frame [1 × 4]&gt; ## 5 5 &lt;S3: kmeans&gt; &lt;data.frame [1 × 4]&gt; ## 6 6 &lt;S3: kmeans&gt; &lt;data.frame [1 × 4]&gt; What we need to do next, is get the value for the total within-cluster sum of squares (tot.withinss) from the glanced column. Given that each item in this column is a data frame, we will need to use the unnest function to unpack the data frames in the glanced column. clustering_statistics &lt;- marketing_clust_ks %&gt;% unnest(glanced) head(clustering_statistics) ## # A tibble: 6 x 6 ## k marketing_clusts totss tot.withinss betweenss iter ## &lt;int&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 &lt;S3: kmeans&gt; 241. 241. 0 1 ## 2 2 &lt;S3: kmeans&gt; 241. 110. 132. 1 ## 3 3 &lt;S3: kmeans&gt; 241. 39.6 202. 1 ## 4 4 &lt;S3: kmeans&gt; 241. 24.0 217. 2 ## 5 5 &lt;S3: kmeans&gt; 241. 18.3 223. 2 ## 6 6 &lt;S3: kmeans&gt; 241. 32.2 209. 1 Now that we have tot.withinss and k as columns in a data frame, we can make a plot to choose K: elbow_plot &lt;- ggplot(clustering_statistics, aes(x = k, y = tot.withinss)) + geom_point() + geom_line() + xlab(&quot;K&quot;) + ylab(&quot;Total within-cluster sum of squares&quot;) elbow_plot We call the plot above an “elbow plot” and we look for the “elbow” in total within-cluster sum of squares, the point where afterwards increasing K doesn’t have as much impact reducing the total within-cluster sum of squares. Here we would choose K = 3. 11.7 Additional readings: Pages 385-390 and 404-405 of Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani and the companion video linked to below: "]
]
