[
["clustering.html", "Chapter 10 Clustering 10.1 Overview 10.2 Chapter learning objectives 10.3 Clustering 10.4 K-means 10.5 K-means in R 10.6 Choosing K for K-means clustering 10.7 Additional readings:", " Chapter 10 Clustering 10.1 Overview As part of exploratory data analysis, it is often helpful to see if there are meaningful subgroups (or clusters) in the data; this grouping can be used to for many purposes, such as generating new questions or improving predictive analyses. This chapter provides an introduction to clustering using the K-means algorithm, including techniques to choose the number of clusters as well as other practical considerations (such as scaling). 10.2 Chapter learning objectives By the end of the chapter, students will be able to: Describe a case where clustering is appropriate, and what insight it might extract from the data. Explain the K-means clustering algorithm. Interpret the output of a K-means analysis. Perform kmeans clustering in R using kmeans. Visualize the output of K-means clustering in R using pair-wise scatter plots. Identify when it is necessary to scale variables before clustering and do this using R. Use the elbow method to choose the number of clusters for K-means. Describe advantages, limitations and assumptions of the K-means clustering algorithm. 10.3 Clustering Clustering is a data analysis task involving separating a data set into subgroups of related data. For example, we might use clustering to separate a dataset of documents into groups that correspond to topics, a dataset of human genetic information into groups that correspond to ancestral subpopulations, or a dataset of online customers into groups that correspond to purchasing behaviours. Once the data are separated we can, for example, use the subgroups to generate new questions about the data and follow up with a predictive modelling exercise. Note that clustering is a fundamentally different kind of task than classification or regression. Most notably, both classification and regression are supervised tasks where there is a predictive target (a class label or value), and we have examples of past data with labels/values that help us predict those of future data. By contrast, clustering is an unsupervised task, as we are trying to understand and examine the structure of data without any labels to help us. This approach has both advantages and disadvantages. Clustering requires no additional annotation or input on the data; for example, it would be nearly impossible to annotate all the articles on Wikipedia with human-made topic labels, but we can still cluster the articles without this information to automatically find groupings corresponding to topics. However, because there is no predictive target, it is not as easy to evaluate the “quality” of a clustering. With classification, we are able to use a test data set to assess prediction performance. In clustering, there is not a single good choice for evaluation. In this book, we will use visualization to ascertain the quality of a clustering, and leave rigorous evaluation for more advanced courses. There are also so-called semisupervised tasks, where only some of the data come with labels / annotations, but the vast majority don’t. The goal is to try to uncover underlying structure in the data that allows one to guess the missing labels. This sort of task is very useful, for example, when one has an unlabelled data set that is too large to manually label, but one is willing to provide a few informative example labels as a “seed” to guess the labels for all the data. An illustrative example Suppose we have customer data with two variables measuring customer loyalty and satisfaction, and we want to learn whether there are distinct “types” of customer. Understanding this might help us come up with better products or promotions to improve our business in a data-driven way. Figure 10.1: Modified from http://www.segmentationstudyguide.com/using-cluster-analysis-for-market-segmentation/ Based on this visualization, we might suspect there are a few subtypes of customer, selected from combinations of high/low satisfaction and high/low loyalty. How do we find this grouping automatically, and how do we pick the number of subtypes to look for? The way to rigorously separate the data into groups is to use a clustering algorithm. In this chapter, we will focus on the K-means algorithm, a widely-used and often very effective clustering method, combined with the elbow method for selecting the number of clusters. This procedure will separate the data into the following groups denoted by colour: What are the labels for these groups? Unfortunately, we don’t have any. K-means, like almost all clustering algorithms, just outputs meaningless “cluster labels” that are typically whole numbers: 1, 2, 3, etc. But in a simple case like this, where we can easily visualize the clusters on a scatter plot, we can give human-made labels to the groups using their positions on the plot: low loyalty and low satisfaction (green cluster), high loyalty and low satisfaction (pink cluster), and high loyalty and high satisfaction (blue cluster). Once we have made these determinations, we can use them to inform our future business decisions, or to ask further questions about our data. For example, here we might notice based on our clustering that there aren’t any customers with high satisfaction but low loyalty, and generate new analyses or business strategies based on this information. 10.4 K-means 10.4.1 Measuring cluster quality The K-means algorithm is a procedure that groups data into K clusters. It starts with an initial clustering of the data, and then iteratively improves it by making adjustments to the assignment of data to clusters until it cannot improve any further. But how do we measure the “quality” of a clustering, and what does it mean to improve it? In K-means clustering, we measure the quality of a cluster by its within-cluster sum-of-squared-distances (WSSD). Computing this involves two steps: We first compute the mean of each variable we are using to cluster the data over data points that are in the cluster (i.e., the center of the cluster). For example, suppose we have a cluster containing 3 observations, and we are using two variables, \\(x\\) and \\(y\\), to cluster the data. Then we would compute the \\(x\\) and \\(y\\) variables, \\(\\mu_x\\) and \\(\\mu_y\\), of the cluster center via \\[\\mu_x = \\frac{1}{3}(x_1+x_2+x_3) \\quad \\mu_y = \\frac{1}{3}(y_1+y_2+y_3).\\] In the first cluster from the customer satisfaction/loyalty example, there are 5 data points. These are shown with their cluster center (csat = 1.8 and loyalty = 7.5) highlighted below. Figure 10.2: Cluster 1 from the toy example, with center highlighted. The second step is to add up the squared distance between each point in the cluster and the cluster center. We use the usual distance formula that we learned about in the classification chapter. In the 3-observation cluster example above, we would compute the WSSD \\(S^2\\) via \\[S^2 = \\left((x_1 - \\mu_x)^2 + (y_1 - \\mu_y)^2\\right) + \\left((x_2 - \\mu_x)^2 + (y_2 - \\mu_y)^2\\right) +\\left((x_3 - \\mu_x)^2 + (y_3 - \\mu_y)^2\\right).\\] These distances are denoted by lines for the first cluster of the customer satisfaction/loyalty data example below. Figure 10.3: Cluster 1 from the toy example, with distances to the center highlighted. The larger the value of \\(S^2\\), the more spread-out the cluster is, since large \\(S^2\\) means that points are far away from the cluster center. Note, however, that “large” is relative to both the scale of the variables for clustering and the number of points in the cluster; a cluster where points are very close to the center might still have a large \\(S^2\\) if there are many data points in the cluster. 10.4.2 The clustering algorithm The K-means algorithm is quite simple. We begin by picking K, and uniformly randomly assigning data to the K clusters. Then K-means consists of two major steps that attempt to minimize the sum of WSSDs over all the clusters, i.e. the total WSSD: Center update: Compute the center of each cluster. Label update: Reassign each data point to the cluster with the nearest center. These two steps are repeated until the cluster assignments no longer change. For example, in the customer data example from earlier, our initialization might look like this: Figure 10.4: Random initialization of labels. And the first three iterations of K-means would look like (each row corresponds to an iteration, where the left column depicts the center update, and the right column depicts the reassignment of data to clusters): Note that at this point we can terminate the algorithm, since none of the assignments changed in the third iteration; both the centers and labels will remain the same from this point onward. Is K-means guaranteed to stop at some point, or could it iterate forever? As it turns out, the answer is thankfully that K-means is guaranteed to stop after some number of iterations. For the interested reader, the logic for this has three steps: (1) both the label update and the center update decrease total WSSD in each iteration, (2) the total WSSD is always greater than or equal to 0, and (3) there are only a finite number of possible ways to assign the data to clusters. So at some point, the total WSSD must stop decreasing, which means none of the assignments are changing and the algorithm terminates. 10.4.3 Random restarts K-means, unlike the classification and regression models we studied in previous chapters, can get “stuck” in a bad solution. For example, if we were unlucky and initialized K-means with the following labels: Figure 10.5: Random initialization of labels. Then the iterations of K-means would look like: This looks like a relatively bad clustering of the data, but K-means cannot improve it. To solve this problem when clustering data using K-means, we should randomly reinitialize the labels a few times, run K-means for each initialization, and pick the clustering that has the lowest final total WSSD. 10.4.4 Choosing K horizontal plots 10.5 K-means in R (combine clustering and choosing K in a single section) Let’s take a look at the data we plotted above: head(marketing_data) ## # A tibble: 6 x 3 ## loyalty csat label ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 7 1 3 ## 2 7.5 1 3 ## 3 8 2 3 ## 4 7 2 3 ## 5 3 2 3 ## 6 1 3 3 To peform Kmeans clustering in R, we use the kmeans function. It takes at least two arguments, the data frame containing the data you wish to cluster, and K, the number of clusters (here we choose K = 3). Given that the K-means algorithm uses a random start to begin the algorithm, to make this reproducible, we need to set the seed. set.seed(1234) marketing_clust &lt;- kmeans(marketing_data, centers = 3) marketing_clust ## K-means clustering with 3 clusters of sizes 5, 9, 5 ## ## Cluster means: ## loyalty csat label ## 1 7.500000 1.800000 3.000000 ## 2 6.777778 7.444444 1.555556 ## 3 2.400000 3.200000 3.000000 ## ## Clustering vector: ## [1] 1 1 1 1 3 3 1 3 3 2 2 2 2 2 2 2 2 2 3 ## ## Within cluster sum of squares by cluster: ## [1] 3.8 30.0 8.0 ## (between_SS / total_SS = 83.5 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; ## [5] &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; ## [9] &quot;ifault&quot; As you can see above, the clustering object returned has a lot of information about our analysis that we need to explore. Let’s take a look at it now. To do this, we will call in help from the broom package so that we get the model output back in a tidy data format. Let’s first start by getting the cluster identification for each point and plotting that on the scatter plot. To do that we use the augment function. Augment takes in the model and the original data frame, and returns a data frame with the data and the cluster assignments for each point: library(broom) clustered_data &lt;- augment(marketing_clust, marketing_data) head(clustered_data) ## # A tibble: 6 x 4 ## loyalty csat label .cluster ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; ## 1 7 1 3 1 ## 2 7.5 1 3 1 ## 3 8 2 3 1 ## 4 7 2 3 1 ## 5 3 2 3 3 ## 6 1 3 3 3 Now that we have this data frame, we can easily plot the data (i.e., cluster assignments of each point): cluster_plot &lt;- ggplot(clustered_data, aes(x = csat, y = loyalty, colour = .cluster)) + geom_point() cluster_plot 10.6 Choosing K for K-means clustering As mentioned above, we need to choose a K to perform K-means clustering. How should we choose K? We have no data labels, and so cannot perform cross-validation with some measure of model prediction error, so what can we do? What we can do in this situation is to look at the total within-cluster sum of squares for different K’s and choose the K that gives the biggest decrease in the total within-cluster sum of squares. Why total within-cluster sum of squares? This statistic lets us know how close/tight-knit (or compact) observations are within clusters. A larger number means that clusters are not close/tight-knit, but are instead more spread out. A smaller number here means that clusters are indeed close/tight-knit together. We can get at the total within-cluster sum of squares (tot.withinss) from our clustering using broom’s glance function (it gives model-level statistics). For example: glance(marketing_clust) ## # A tibble: 1 x 4 ## totss tot.withinss betweenss iter ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 253. 41.8 211. 2 Let’s calculate the total within-cluster sum of squares for our data for a variety of K’s (say 1 - 9) and then plot them against K. To do this we will create a data frame with a column named k, for each of the K’s we want to try our clustering with. Then we use map to apply the kmeans function to each K. We also use map to then apply glance to each of the clustering models we performed (one for each K). In the end we end up with a complex data frame with 3 columns, one for K, one for the models, and one for the model statistics (output of glance, which is a data frame): marketing_clust_ks &lt;- tibble(k = 1:9) %&gt;% mutate(marketing_clusts = map(k, ~kmeans(marketing_data, .x)), glanced = map(marketing_clusts, glance)) head(marketing_clust_ks) ## # A tibble: 6 x 3 ## k marketing_clusts glanced ## &lt;int&gt; &lt;list&gt; &lt;list&gt; ## 1 1 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt; ## 2 2 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt; ## 3 3 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt; ## 4 4 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt; ## 5 5 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt; ## 6 6 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt; What we need to do next, is get the value for the total within-cluster sum of squares (tot.withinss) from the glanced column. Given that each item in this column is a data frame, we will need to use the unnest function to unpack the data frames in the glanced column. clustering_statistics &lt;- marketing_clust_ks %&gt;% unnest(glanced) head(clustering_statistics) ## # A tibble: 6 x 6 ## k marketing_clusts totss tot.withinss betweenss iter ## &lt;int&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 &lt;kmeans&gt; 253. 253. -1.14e-13 1 ## 2 2 &lt;kmeans&gt; 253. 112. 1.42e+ 2 1 ## 3 3 &lt;kmeans&gt; 253. 41.8 2.11e+ 2 1 ## 4 4 &lt;kmeans&gt; 253. 25.7 2.28e+ 2 2 ## 5 5 &lt;kmeans&gt; 253. 19.5 2.34e+ 2 2 ## 6 6 &lt;kmeans&gt; 253. 34.5 2.19e+ 2 1 Now that we have tot.withinss and k as columns in a data frame, we can make a plot to choose K: elbow_plot &lt;- ggplot(clustering_statistics, aes(x = k, y = tot.withinss)) + geom_point() + geom_line() + xlab(&quot;K&quot;) + ylab(&quot;Total within-cluster sum of squares&quot;) elbow_plot We call the plot above an “elbow plot” and we look for the “elbow” in total within-cluster sum of squares, the point where afterwards increasing K doesn’t have as much impact reducing the total within-cluster sum of squares. Here we would choose K = 3. 10.7 Additional readings: Watch the video linked to below for an explanation of the K-means clustering algorithm: - https://www.coursera.org/lecture/machine-learning-data-analysis/what-is-a-k-means-cluster-analysis-p94tY note - when the advertisement pops up to register for this course, you can just click to ignore it (i.e., no need to sign up to watch the entire video) Pages 385-390 and 404-405 of Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani and the companion video linked to below: "]
]
