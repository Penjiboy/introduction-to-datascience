[
["regression1.html", "Chapter 8 Introduction to regression through K-nearest neighbours 8.1 Overview 8.2 Learning objectives 8.3 Regression 8.4 Sacremento real estate example 8.5 K-nearest neighbours regression 8.6 Assessing a knn regression model 8.7 Assessing model goodness with the test set", " Chapter 8 Introduction to regression through K-nearest neighbours 8.1 Overview Introduction to regression using K-nearest neighbours (k-nn). We will focus on prediction in cases where there is a response variable of interest and a single explanatory variable. 8.2 Learning objectives By the end of the chapter, students will be able to: Recognize situations where a simple regression analysis would be appropriate for making predictions. Explain the k-nearest neighbour (k-nn) regression algorithm and describe how it differs from k-nn classification. Interpret the output of a k-nn regression. In a dataset with two variables, perform k-nearest neighbour regression in R using caret::knnregTrain() to predict the values for a test dataset. Using R, execute cross-validation in R to choose the number of neighbours. Using R, evaluate k-nn regression prediction accuracy using a test data set and an appropriate metric (e.g., means square prediction error). Describe advantages and disadvantages of the k-nearest neighbour regression approach. 8.3 Regression We can use regression as a method to answer a very similar question to classification (can we use past information to predict future observations?), but in the case of regression the goal is to predict numerical values instead of class labels. An example regression prediction question would be: can we use hours spent on exercise each week to predict marathon race time? And another example regression prediction question is: can we use house size (livable square feet) to predict house sale price? We will use this question to explore regression in this text book. For this we will use a real estate data set from Sacremento, California that is part of the caret package. 8.4 Sacremento real estate example Let’s start by loading the libraries we need and previewing the data set. The data set comes with the caret package, so as soon as we load the caret library and type data(Sacramento) we are able to access it as a data frame named Sacramento. library(tidyverse) ## Warning: package &#39;tibble&#39; was built under R version 3.5.2 library(scales) library(caret) data(Sacramento) head(Sacramento) ## city zip beds baths sqft type price latitude longitude ## 1 SACRAMENTO z95838 2 1 836 Residential 59222 38.63191 -121.4349 ## 2 SACRAMENTO z95823 3 1 1167 Residential 68212 38.47890 -121.4310 ## 3 SACRAMENTO z95815 2 1 796 Residential 68880 38.61830 -121.4438 ## 4 SACRAMENTO z95815 2 1 852 Residential 69307 38.61684 -121.4391 ## 5 SACRAMENTO z95824 2 1 797 Residential 81900 38.51947 -121.4358 ## 6 SACRAMENTO z95841 3 1 1122 Condo 89921 38.66260 -121.3278 We are interested in the columns sqft (which is the house size in livable square feet) and price, which is the house size in US dollars (USD). Let’s now visaulize the data as a scatter plot where we place the predictor/explanatory variable, house size, on the x-axis and the target/response variable, price, on the y-axis (this is what we would like to predict): eda &lt;- ggplot(Sacramento, aes(x = sqft, y = price)) + geom_point(alpha = 0.5) + xlab(&quot;House size (square footage)&quot;) + ylab(&quot;Price (USD)&quot;) + scale_y_continuous(labels=dollar_format()) eda From looking at the visualization above, we see that as house size (square footage) increases, so does house price. Thus, we can reason that house size might be a useful predictor of house price and perhaps we can use the size of the house to predict the price a house will be sold at (for a home that has not yet sold and thus consequently we do not know the house price). 8.5 K-nearest neighbours regression Let’s take a small sample of the data above and walk through how K-nearest neighbours (knn) regression works before we dive in to creating our model and assessing how well it predicts house price. To take a small random sample of size 30 , we’ll use the function sample_n. This function takes two arguments: tbl (a data frame-like object to sample from) size (the number of observations/rows to be randomly selected/sampled) set.seed(2019) small_sacramento &lt;- sample_n(Sacramento, size = 30) Next let’s say we come across a new house we are interested in purchasing, and it is 2000 square feet! Its advertised list price is $350,000 should we give them what they are asking? Or is that overpriced and we should offer less? Perhaps we cannot directly answer that, but we can get close by using the data we have to predict the sale price given the sale prices we have already observed. Given the data in the plot below, we have no observations of a house that has sold that is 2000 square feet, so how can we predict the price? small_plot &lt;- ggplot(small_sacramento, aes(x = sqft, y = price)) + geom_point() + xlab(&quot;House size (square footage)&quot;) + ylab(&quot;Price (USD)&quot;) + scale_y_continuous(labels=dollar_format()) + geom_vline(xintercept = 2000, linetype = &quot;dotted&quot;) small_plot What we can do is use the neighbouring points to suggest/predict what the price should be. For the example above, below we find and label the 5 nearest neighbours to our observation of a house that is 2000 square feet: small_sacramento %&gt;% mutate(diff = abs(2000 - sqft)) %&gt;% arrange(diff) %&gt;% head(5) ## city zip beds baths sqft type price latitude longitude ## 1 SACRAMENTO z95835 4 2 1910 Residential 300500 38.67745 -121.4948 ## 2 SACRAMENTO z95825 3 2 1720 Residential 290000 38.56786 -121.4107 ## 3 ANTELOPE z95843 3 2 1669 Residential 168750 38.71846 -121.3709 ## 4 LINCOLN z95648 3 2 1650 Residential 276500 38.84412 -121.2748 ## 5 SACRAMENTO z95827 5 3 2367 Residential 315537 38.55599 -121.3404 ## diff ## 1 90 ## 2 280 ## 3 331 ## 4 350 ## 5 367 small_plot + geom_segment(aes(x = 2000, y = 300500, xend = 1910, yend = 300500), col = &quot;orange&quot;) + geom_segment(aes(x = 2000, y = 290000, xend = 1720, yend = 290000), col = &quot;orange&quot;) + geom_segment(aes(x = 2000, y = 168750, xend = 1669, yend = 168750), col = &quot;orange&quot;) + geom_segment(aes(x = 2000, y = 276500, xend = 1650, yend = 276500), col = &quot;orange&quot;) + geom_segment(aes(x = 2000, y = 315537, xend = 2367, yend = 315537), col = &quot;orange&quot;) Now that we have the 5 nearest neighbours to our new observation that we would like to predict the price for, we can use their values to predict a selling price for the home we are interested in buying that is 2000 square feet. Specifically, we can take the mean (or average) of these 5 values as our predicted value. small_sacramento %&gt;% mutate(diff = abs(2000 - sqft)) %&gt;% arrange(diff) %&gt;% head(5) %&gt;% summarise(predicted = mean(price)) ## predicted ## 1 270257.4 Our predicted price is $270,257.40, which is much less than $350,000, and so perhaps we might want to offer less than the list price that the house is advertised at. Simple right? Not quite. We have all the same unanswered questions here with k-nn regression that we had with k-nn classification. Which \\(k\\) do we choose? And, is our model any good at making predictions? We’ll shortly address how to answer these questions in the context of k-nn regression. 8.6 Assessing a knn regression model As usual, we should start by putting some test data away in a lock box that we can come back to after we choose our final model, so let’s take care of that business now. For the remainder of the chapter we’ll work with the full data set. set.seed(2019) # makes the random selection of rows reproducible training_rows &lt;- Sacramento %&gt;% select(price) %&gt;% unlist() %&gt;% # converts Class from a tibble to a vector createDataPartition(p = 0.6, list = FALSE) X_train &lt;- Sacramento %&gt;% select(sqft) %&gt;% slice(training_rows) %&gt;% data.frame() Y_train &lt;- Sacramento %&gt;% select(price) %&gt;% slice(training_rows) %&gt;% unlist() X_test &lt;- Sacramento %&gt;% select(sqft) %&gt;% slice(-training_rows) %&gt;% data.frame() Y_test &lt;- Sacramento %&gt;% select(price) %&gt;% slice(-training_rows) %&gt;% unlist() Next, we’ll use cross-validation to choose \\(k\\). In k-nn classification, we used accuracy to see how well our predictions matched the true labels. Here in the context of k-nn regression we will use root mean square error (\\(RMSE\\)) instead. If the predictions are very close to the true values, then \\(RMSE\\) will be small. If, on the other-hand, the predictions are very different to the true values, then \\(RMSE\\) will be quite large. Thus, when we are doing cross validation to choose \\(k\\), we want to choose the \\(k\\) that gives us the smallest \\(RMSE\\). The mathematical formula for calculation \\(RMSE\\) is shown below: \\[RMSE = \\sqrt{\\frac{1}{n}\\sum\\limits_{i=1}^{\\infty}(y_i - \\hat{y_i})^2}\\] Where: \\(n\\) is the number of observations \\(y_i\\) is the observed value for the \\(ith\\) observation \\(\\hat{y_i}\\) is the forcasted/predicted value for the \\(ith\\) observation A key feature the formula for RMSE is the distance between the observed target/response variable value, \\(y\\), and the prediction target/response variable value, \\(\\hat{y_i}\\), for each observation (from 1 to \\(i\\)). Now that we know how we can assess how well our model predicts a numerical value, let’s use R to perform cross-validation and to choose the optimal \\(k\\). train_control &lt;- trainControl(method = &quot;cv&quot;, number = 10) # makes a column of k&#39;s, from 1 to 100 in increments of 10 k_lots = data.frame(k = seq(from = 1, to = 100, by = 10)) set.seed(1234) knn_reg_cv_10 &lt;- train(x = X_train, y = Y_train, method = &quot;knn&quot;, tuneGrid = k_lots, trControl = train_control) knn_reg_cv_10 ## k-Nearest Neighbors ## ## 561 samples ## 1 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 505, 505, 505, 505, 505, 505, ... ## Resampling results across tuning parameters: ## ## k RMSE Rsquared MAE ## 1 110214.34 0.3643214 82765.68 ## 11 85909.14 0.5432898 64417.87 ## 21 84446.88 0.5597473 63221.52 ## 31 83781.76 0.5661561 62191.50 ## 41 83649.72 0.5677168 62191.46 ## 51 83129.92 0.5739339 61646.80 ## 61 83280.96 0.5755190 61578.59 ## 71 83438.69 0.5769822 61763.68 ## 81 83535.20 0.5783986 61765.92 ## 91 83890.01 0.5773097 61848.97 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was k = 51. ggplot(knn_reg_cv_10$results, aes(x = k, y = Rsquared)) + geom_point() + geom_line() Here we see that the smallest \\(RMSE\\) is from the model where \\(k\\) = 51. Thus the best \\(k\\) for this model is 51. 8.7 Assessing model goodness with the test set Next we re-train our k-nn model on the entire training data set (do not perform cross validation) and then predict on the test data set to assess how well our model does. In the case of k-nn regression we use the function defaultSummary instead of confusionMatrix (which we used with knn classification). This is because our predictions are not class labels, but values, and as such the type of model goodness score is calculated differently. defaultSummary expects a data frame where one column is the observed target/response variable values from the test data, and a second column of the predicted values for the test data. k = data.frame(k = 51) set.seed(1234) knn_reg_final &lt;- train(x = X_train, y = Y_train, method = &quot;knn&quot;, tuneGrid = k) test_pred &lt;- predict(knn_reg_final, X_test) modelvalues &lt;- data.frame(obs = Y_test, pred = test_pred) test_results &lt;- defaultSummary(modelvalues) test_results ## RMSE Rsquared MAE ## 9.162040e+04 5.785061e-01 6.370486e+04 Our test error as assessed by \\(RMSE\\) is 91620.4. "]
]
