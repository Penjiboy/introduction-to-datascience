[
["classification-continued.html", "Chapter 7 Classification continued 7.1 Overview 7.2 Learning objectives 7.3 Assessing how good your classifier is", " Chapter 7 Classification continued 7.1 Overview Metrics for classification accuracy; cross-validation to choose the number of neighbours; scaling of variables and other practical considerations. 7.2 Learning objectives By the end of the chapter, students will be able to: Describe what a validation data set is and how it is used in classification. Using R, evaluate classification accuracy using a validation data set and appropriate metrics. Using R, execute cross-validation in R to choose the number of neighbours. Identify when it is necessary to scale variables before classification and do this using R In a dataset with &gt; 2 attributes, perform k-nearest neighbour classification in R using caret::train(method = “knn”, …) to predict the class of a test dataset. Describe advantages and disadvantages of the k-nearest neighbour classification algorithm. 7.3 Assessing how good your classifier is Sometimes our classifier might make the wrong prediction. A classifier does not need to be right 100% of the time to be useful, though we don’t want the classifier to make too many wrong predictions. How do we measure how “good” our classifier? One way to assess our classifier’s performance can be done by splitting our data into a training set and a validation set. When we split the data, we make the assumption that there is no order to our originally collected data set. However, if we think that there might be some order to the original data set, then we can randomly shuffle the data before splitting it into a training and validation set. The training set is used to build the classifer. Then we can give the observations from the validation set (without the labels/classes) to our classifier and predict the labels/classes as if these were new observations that we didn’t have the labels/classes for. Then we can see how well our predictions match the true labels/classes for the observations in the validation set. If our predictions match the true labels/classes for the observations in the validation set very well then we have some confidence that our classifier might also do a good job of predicting the class labels for new observations that we do not have the class labels for. How exactly can we assess how well our predictions match the true labels/classes for the observations in the validation set? One way we can do this is to calculate the prediction accuracy. This is essentially the proportion of time the classifier was correct. To calculate this we divide the number of correct predictions by the number of predictions made. Other measures for how well our classifier did include precision and recall (which will not be discussed here, but are discussed in other more advanced courses on this topic). We try to illustrate this below: 7.3.1 Assessing your classifier in R We can use the caret package in R to not only perform k-nn classification, but also to assess how well our classification worked. Let’s start by loading the necessary libraries, data (we’ll continue exploring the breast cancer data set from last chapter) and making a quick scatter plot of tumour cell concavity versus perimeter, labelling the points be diagnosis class. # load libraries library(tidyverse) library(caret) #load data cancer &lt;- read_csv(&quot;data/clean-wdbc.data.csv&quot;) # colour palette cbPalette &lt;- c(&quot;#56B4E9&quot;, &quot;#E69F00&quot;,&quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#999999&quot;) # create scatter plot of tumour cell concavity versus perimeter, # labelling the points be diagnosis class perim_concav &lt;- cancer %&gt;% ggplot(aes(x = Perimeter, y = Concavity, color = Class)) + geom_point(alpha = 0.5) + labs(color = &quot;Diagnosis&quot;) + scale_color_manual(labels = c(&quot;Benign&quot;, &quot;Malignant&quot;), values = cbPalette) perim_concav Next, lets split our data into a training and a validation set using caret’s createDataPartition function. When using this function to split a data set into a training and validation set it takes 3 arguments: the class labels (must be a vector), the proportion of the data you would like in the training data set, and list = FALSE The createDataPartition function returns the row numbers for the training set. set.seed(1234) # makes the random selection of rows reproducible set_rows &lt;- cancer %&gt;% select(Class) %&gt;% unlist() %&gt;% # converts Class from a tibble to a vector createDataPartition(p = 0.60, list = FALSE) head(set_rows) ## Resample1 ## [1,] 1 ## [2,] 3 ## [3,] 4 ## [4,] 5 ## [5,] 7 ## [6,] 8 You will also see in the code above that we use the set.seed function. This is because createDataPartition uses random sampling to choose which rows will be in the training set, and if we use set.seed to specify where the random number generator starts for this process then we can make our analysis reproducible (always get the same random set of observations in the training set). Now that we have the row numbers for the training set, we can use the slice function to get the rows from the original data set (here cancer) to create the training set and the validation sets. training_set &lt;- cancer %&gt;% slice(set_rows) validation_set &lt;- cancer %&gt;% slice(-set_rows) head(training_set) ## # A tibble: 6 x 12 ## ID Class Radius Texture Perimeter Area Smoothness Compactness ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 8.42e5 M 1.89 -1.36 2.30 2.00 1.31 2.61 ## 2 8.43e7 M 1.51 -0.0240 1.35 1.46 0.527 1.08 ## 3 8.43e7 M -0.281 0.134 -0.250 -0.550 3.39 3.89 ## 4 8.44e7 M 1.30 -1.47 1.34 1.22 0.220 -0.313 ## 5 8.44e5 M 1.37 0.323 1.37 1.27 0.518 0.0212 ## 6 8.45e7 M 0.164 0.401 0.0994 0.0288 1.45 0.724 ## # ... with 4 more variables: Concavity &lt;dbl&gt;, Concave_points &lt;dbl&gt;, ## # Symmetry &lt;dbl&gt;, Fractal_dimension &lt;dbl&gt; "]
]
