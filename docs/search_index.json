[
["index.html", "Introduction to Data Science Chapter 1 Introduction to Data Science 1.1 Chapter learning objectives 1.2 Jupyter notebooks 1.3 Loading a tabular dataset 1.4 Assigning value to an object 1.5 Subsetting data frames with select &amp; filter 1.6 Combining functions using the pipe operator: %&gt;% 1.7 Creating Visualizations in R", " Introduction to Data Science Tiffany-Anne Timbers Melissa Lee Trevor Campbell 2019-08-08 Chapter 1 Introduction to Data Science This is an open source textbook aimed at introducing undergraduate students to Data Science. It was originally written for the University of British Columbia’s DSCI 100 - Introduction to Data Science course. This course uses Jupyter and the R programming language to illustrate how to solve 3 common problems in Data Science: Predicting a class/category for a new observation/measurement (e.g., cancerous or benign tumour) Finding previously unknown/unlabelled subgroups in your data (e.g., products commonly bought together on Amazon) Predicting a value for a new observation/measurement (e.g., 10 km race time for 30-35 year old males with a BMI &gt; 25). This book/course is structured so that learners spend the first four chapters learning how to use R to load, wrangle/clean and plot data. The remaining 4 chapters cover solutions to the three solutions provided above. 1.1 Chapter learning objectives By the end of the chapter, students will be able to: use a Jupyter notebook to execute provided R code edit code and markdown cells in a Jupyter notebook create new code and markdown cells in a Jupyter notebook load the tidyverse library into R create new variables and objects in R using the assignment symbol use the help and documentation tools in R match the names of the following functions from the tidyverse library to their documentation descriptions: read_csv select mutate filter ggplot aes chain together two functions using the pipe operator, %&gt;% 1.2 Jupyter notebooks Jupyter Notebooks are documents that contain a mix of computer code (and its output) and formattable text. Given that they can mix these two things in a single document (code is not separate from the output or written report), they are one of the leading tools to create reproducible data analysis. The name Jupyter came from combining the names of the three programming language that it was initially targeted for (Julia, Python, and R), and now many other languages can be used with Jupyter notebooks. A notebook looks like this: We have created a short demo video to help you get started and introduce you to Jupyter: However, the best way to learn how to write and run code and formattable text in a Jupyter notebook is to do it. So we have also created this worksheet as a step-by-step guide through it: https://github.com/UBC-DSCI/dsci-100/blob/master/materials/worksheet_01/worksheet_01.ipynb 1.3 Loading a tabular dataset Often, the first thing we need to do in data analysis is to load a dataset into R. Once we get our data into R, we would like to work with it as a data frame object. You can think of data frames as a spreadsheet-like object, where the rows are the observations collected/measured and the columns are the variables. The first data set we will work with is in a comma-separated file, or .csv, format. There are many functions available in R we could use to load a .csv file, but we are going to use read_csv() from the tidyverse package because it is fast it creates a “special” variant of the base R data frame object with several nice properties we’ll discuss in further detail later in the course. In it’s most basic use case, read_csv() expects that: column names are present , is the delimiter/separator there are no row names in the dataset Let’s explore how to do this! We will load a .csv file named “state_property_data.csv” that is in the same directory/folder as our notebook/script. This data file is from https://datausa.io/ and has US state-level property, income and population data from 2015. If we were to open this data in a plain text editor, it would look like this: state,med_income,med_prop_val,population,mean_commute_minutes AK,64222,197300,733375,10.46830207 AL,36924,94800,4830620,25.30990746 AR,35833,83300,2958208,22.40108933 AZ,44748,128700,6641928,20.58786 CA,53075,252100,38421464,23.38085172 Here we now load the data into R. To do this, we first need to load the tidyverse library. We do this using the library function. Next we call the read_csv() and pass it a single argument, the path to the file - “state_property_data.csv” - as a string. This is the only argument we need to provide for this file because it satisfies the default arguments of the read_csv function that we just discussed. Later in the course, we’ll learn more about how to deal with files where the default arguments are not appropriate, for example tab separated files or files with no headers. library(tidyverse) read_csv(&quot;state_property_data.csv&quot;) # A tibble: 52 x 5 state med_income med_prop_val population mean_commute_minutes &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; 1 AK 64222 197300 733375 10.5 2 AL 36924 94800 4830620 25.3 3 AR 35833 83300 2958208 22.4 4 AZ 44748 128700 6641928 20.6 5 CA 53075 252100 38421464 23.4 6 CO 48098 198900 5278906 19.5 7 CT 69228 246450 3593222 24.3 8 DC 70848 475800 647484 28.3 9 DE 54976 228500 926454 24.5 10 FL 43355 125600 19645772 24.8 # ... with 42 more rows 1.4 Assigning value to an object When we loaded the US state-level property, income and population data in the last slide using read_csv(), we did not assign the output of this function to an object, so it was merely printed to the console and not stored anywhere. This is not that useful - what we would like to do is assign the output of the read_csv() function, a data frame, to an object so that we can use it later for analysis and visualization. To assign value to an object in R, there are two possible ways - using either the assignment symbol or the equals symbol (as is done in Python). From a style perspective, the assignment symbol is prefered and is what we will use in this course. Do note however, that the assignment symbol has a very different meaning with function argument definitions and is only used in special cases. In general, we use the equals symbol during function argument definitions. Let’s now use the assignment symbol to store the US state-level property, income and population data in a data frame object that we call prop_data. prop_data &lt;- read_csv(&quot;state_property_data.csv&quot;) print(prop_data) To preview the prop_data object, we can use the print() function. # A tibble: 52 x 5 state med_income med_prop_val population mean_commute_minutes &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; 1 AK 64222 197300 733375 10.5 2 AL 36924 94800 4830620 25.3 3 AR 35833 83300 2958208 22.4 4 AZ 44748 128700 6641928 20.6 5 CA 53075 252100 38421464 23.4 6 CO 48098 198900 5278906 19.5 7 CT 69228 246450 3593222 24.3 8 DC 70848 475800 647484 28.3 9 DE 54976 228500 926454 24.5 10 FL 43355 125600 19645772 24.8 # ... with 42 more rows 1.5 Subsetting data frames with select &amp; filter Now, we are going to learn how to select subsets of data from a data frame in R using the tidyverse functions select() and filter(). select() allows you to subset columns of a data frame, while filter() allows you to subset rows with specific values. Before we start using select() and filter(), let’s take a look at the US state-level property, income and population data again to familiarize ourselves with it. We will do this by printing the data we loaded earlier in the chapter to the screen. print(prop_data) # A tibble: 52 x 5 state med_income med_prop_val population mean_commute_minutes &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; 1 AK 64222 197300 733375 10.5 2 AL 36924 94800 4830620 25.3 3 AR 35833 83300 2958208 22.4 4 AZ 44748 128700 6641928 20.6 5 CA 53075 252100 38421464 23.4 6 CO 48098 198900 5278906 19.5 7 CT 69228 246450 3593222 24.3 8 DC 70848 475800 647484 28.3 9 DE 54976 228500 926454 24.5 10 FL 43355 125600 19645772 24.8 # ... with 42 more rows When peak at the data frame we see there are 5 columns: 1. US state abbreviation 2. Median household income 3. Median property value 4. US state population 5. Mean communte time in minutes There are 52 rows in this dataset (corresponding to the 51 US states and the US territory, Puerto Rico). Let’s use select() to subset the state column from this data frame. To use select() to subset the state column, we’ll provide the function with two arguments. The first argument is the name of the data frame object, here prop_data. The second argument is the column name that we want to subset, here state. select() returns a single column (the state column that we asked for) as a data frame object. select(prop_data, state) # A tibble: 52 x 1 state &lt;chr&gt; 1 AK 2 AL 3 AR 4 AZ 5 CA 6 CO 7 CT 8 DC 9 DE 10 FL # ... with 42 more rows 1.5.1 Using select to subset multiple columns We can also use select() to subset multiple columns. Again, the first argument is the name of the data frame. Then we list all the columns we want as arguments separated by commas. Here the list was three columns: state, median property value and mean commute time in minutes. select(prop_data, state, med_prop_val, mean_commute_minutes) # A tibble: 52 x 3 state med_prop_val mean_commute_minutes &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 AK 197300. 10.5 2 AL 94800. 25.3 3 AR 83300. 22.4 4 AZ 128700. 20.6 5 CA 252100. 23.4 6 CO 198900. 19.5 7 CT 246450. 24.3 8 DC 475800. 28.3 9 DE 228500. 24.5 10 FL 125600. 24.8 # ... with 42 more rows 1.5.2 Using select to subset a range of columns We can also use select() to subest a range of columns using the colon. For example, to get all the columns from state to med_prop_val we provide the second arugument to select() as state:med_prop_val. select(prop_data, state:med_prop_val) # A tibble: 52 x 3 state med_income med_prop_val &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 AK 64222. 197300. 2 AL 36924. 94800. 3 AR 35833. 83300. 4 AZ 44748. 128700. 5 CA 53075. 252100. 6 CO 48098. 198900. 7 CT 69228. 246450. 8 DC 70848. 475800. 9 DE 54976. 228500. 10 FL 43355. 125600. # ... with 42 more rows 1.5.3 Using filter to subset a single column We can use the filter() function to subset rows with desired values from a data frame of interest. Again, our first argument is the name of the data frame object, prop_data. The second argument is a logical statement to filter the rows, here we say that we are interested in rows where state equals NY (for New York). Filter returns a data frame object that has all the columns of the input data frame but with only the rows we asked for in our filter statement. filter(prop_data, state == &quot;NY&quot;) # A tibble: 1 x 5 state med_income med_prop_val population mean_commute_minutes &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; 1 NY 50839. 134150. 19673174 24.4 1.5.4 Using filter to get rows with values above a threshold If we are interested in finding information about the states who have a longer mean commute time than New York, whose mean commute time is 21.5 minutes, then we can provide an expression to filter to obtain rows where the value of mean_commute_minutes is greater than 21.5. We see that filter() returns to use a data frame with 33 rows indicating that there are 33 states with longer commute times on average than New York. filter(prop_data, mean_commute_minutes &gt; 21.5) # A tibble: 33 x 5 state med_income med_prop_val population mean_commute_minutes &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; 1 AL 36924. 94800. 4830620 25.3 2 AR 35833. 83300. 2958208 22.4 3 CA 53075. 252100. 38421464 23.4 4 CT 69228. 246450. 3593222 24.3 5 DC 70848. 475800. 647484 28.3 6 DE 54976. 228500. 926454 24.5 7 FL 43355. 125600. 19645772 24.8 8 GA 37865. 101700. 10006693 24.5 9 IL 47898. 97350. 12873761 22.6 10 IN 47194. 111800. 6568645 23.5 # ... with 23 more rows 1.6 Combining functions using the pipe operator: %&gt;% Now we will learn about an efficient way to combine functions in R; the pipe operator %&gt;%. You can think of the pipe as a physical pipe - taking the output from the expression on the left-handside of the pipe and passing it as the input to first expression on the right-handside of the pipe. Let’s look at some examples of this. 1.6.1 Using %&gt;% to combine filter and select Remembering our prop_data data frame: print(prop_data) # A tibble: 52 x 5 state med_income med_prop_val population mean_commute_minutes &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; 1 AK 64222 197300 733375 10.5 2 AL 36924 94800 4830620 25.3 3 AR 35833 83300 2958208 22.4 4 AZ 44748 128700 6641928 20.6 5 CA 53075 252100 38421464 23.4 6 CO 48098 198900 5278906 19.5 7 CT 69228 246450 3593222 24.3 8 DC 70848 475800 647484 28.3 9 DE 54976 228500 926454 24.5 10 FL 43355 125600 19645772 24.8 # ... with 42 more rows If we would like to subset the values for median income and median property value for the state of California (“CA”), we can use the functions filter() and select() to do this. First we use filter() to create a data frame object called ca_prop_data that only contains the values for the state of California. We can then use select() on this data frame to subset the values for median income and median property value for California: ca_prop_data &lt;- filter(prop_data, state == &quot;CA&quot;) select(ca_prop_data, med_income, med_prop_val) # A tibble: 1 x 2 med_income med_prop_val &lt;dbl&gt; &lt;dbl&gt; 1 53075. 252100. Although this is a valid approach, there is a more efficient and readable approach we could take by using the pipe. With the pipe, we do not need to create an intermediate object to store the output from filter(), instead we can use the pipe to directly send the output of filter() to the input of select(): filter(prop_data, state == &quot;CA&quot;) %&gt;% select(med_income, med_prop_val) # A tibble: 1 x 2 med_income med_prop_val &lt;dbl&gt; &lt;dbl&gt; 1 53075. 252100. But wait? Why does our select() function call look different in these two examples? Well, when you use the pipe, the output of the function on the left is automatically provided as the value for the first argument for the function on the right, and thus you do not specify the value for that argument in that function call. As you can see, both of these approaches give us the same output but the second approach is more efficient and readable. 1.6.2 Using %&gt;% with more than two functions The %&gt;% can be used with any function in R. Additionally, we can pipe together more than two functions. In this example, we will pipe together 3 functions to get the values for median income and which presidential party the states of Washington DC and Mississippi voted for in the 2016 US presidential election. To do this, we will load another dataset that contains the party each state voted for in the 2016 US presidential election and join that dataset to our US state-level property, income and population dataset. We will use the read_csv()) function, as we did earlier in the chapter, to load this data set. vote_data &lt;- read_csv(&quot;2016_presidential_election_state_vote.csv&quot;) print(vote_data) # A tibble: 51 x 2 party state &lt;chr&gt; &lt;chr&gt; 1 republican AL 2 republican AK 3 republican AZ 4 republican AR 5 democrat CA 6 democrat CO 7 democrat CT 8 democrat DE 9 democrat DC 10 republican FL # ... with 41 more rows Given that both data sets have a “state” column, we can join the two datasets on that column using the left_join() function in R. From this larger, joined dataset, we will now be able to filter() and select() the values for median income and which presidential party the states of Washington DC and Mississippi voted for in the 2016 US presidential election. left_join(prop_data, vote_data, by = &quot;state&quot;) # A tibble: 52 x 6 state med_income med_prop_val population mean_commute_minutes party &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; 1 AK 64222 197300 733375 10.5 republican 2 AL 36924 94800 4830620 25.3 republican 3 AR 35833 83300 2958208 22.4 republican 4 AZ 44748 128700 6641928 20.6 republican 5 CA 53075 252100 38421464 23.4 democrat 6 CO 48098 198900 5278906 19.5 democrat 7 CT 69228 246450 3593222 24.3 democrat 8 DC 70848 475800 647484 28.3 democrat 9 DE 54976 228500 926454 24.5 democrat 10 FL 43355 125600 19645772 24.8 republican # ... with 42 more rows We can pipe the output of our join to filter() and the rows for the states of New York and South Dakota. We can filter() on two states by using the or operator. Again, we can use the pipe to send the output of filter() to select(), and use select() to finally subset the values from the state, median income and party voted for columns. left_join(prop_data, vote_data, by = &quot;state&quot;) %&gt;% filter(state == &quot;DC&quot; | state == &quot;MS&quot;) %&gt;% select(state, med_income, party) # A tibble: 2 x 3 state med_income party &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 DC 70848. democrat 2 MS 33748. republican 1.7 Creating Visualizations in R Creating data visualizations is an essential piece to any data analysis. For the remainder of Chapter 1, we will learn how we can use some of the tidyverse functions to make visualizations to explore the relationship between median household income and median propery value across US states, as well as how this relates to which party each state voted for in the 2016 US election. 1.7.1 Using ggplot to create a scatter plot First, take another look at the dataset we have been focusing on - the US state-level property, income and population data from 2015. We can see that there is a row/observation for each state. And the two variables we are interested in visualizing, median household income and median property value, are each in separate columns - thus the data are what we call a tidy data format. This is really important for ggplot() and many of the other tidyverse functions (as you will learn more in later chapters). print(prop_data) # A tibble: 52 x 5 state med_income med_prop_val population mean_commute_minutes &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; 1 AK 64222 197300 733375 10.5 2 AL 36924 94800 4830620 25.3 3 AR 35833 83300 2958208 22.4 4 AZ 44748 128700 6641928 20.6 5 CA 53075 252100 38421464 23.4 6 CO 48098 198900 5278906 19.5 7 CT 69228 246450 3593222 24.3 8 DC 70848 475800 647484 28.3 9 DE 54976 228500 926454 24.5 10 FL 43355 125600 19645772 24.8 # ... with 42 more rows 1.7.2 Using ggplot to create a scatter plot To create a scatter plot of these two variables using ggplot(), we would do the following: call the ggplot() function provide the name of the data frame as the first argument call the aesthetic function to map a column to the x-axis and a column to the y-axis add a “+” symbol at the end of the ggplot() call to add a layer to the plot call the geom_point() function to tell R that we want to represent the data points (which we passed to the aesthetic function) as dots/points to create a scatter plot. ggplot(prop_data, aes(x = med_income, y = med_prop_val)) + geom_point() 1.7.3 Formatting ggplot objects To format, or change an aspect of your ggplot() object, one common and easy way to do this is to add additional layers to your plot object using the + symbol. Here we use the xlab() and ylab() functions to add layers where we specify the labels for the x and y axis, respectively. There are many more layers we can add to format the plot further, and we will explore these in later chapters. ggplot(prop_data, aes(x = med_income, y = med_prop_val)) + geom_point() + xlab(&quot;Income (USD)&quot;) + ylab(&quot;Median property value (USD)&quot;) 1.7.4 Coloring points by group Another common thing to do with scatter plots is to colour points by a group/category in the dataset. For example, if we combine our US state-level property, income and population data with the dataset of which party each state voted for in the 2016 US presidential election, we can colour the points in our previous scatter plot to represent who each stated voted for. In this example, we’ll walk through code to accomplish this from loading the dataset to creating a visulazation. First, we load the tidyverse library: library(tidyverse) Second, we read the data into variables. Here we call the US state-level property, income and population data “prop_data” and the state-level 2016 US presidential election data “vote_data”: prop_data &lt;- read_csv(&quot;data/state_property_data.csv&quot;) vote_data &lt;- read_csv(&quot;data/2016_presidential_election_state_vote.csv&quot;) Third, we create a new object called “combined_data” that we make by piping prop_data into the filter() function to remove the territory of PR (because they do not vote in the presidential election), and then we pipe that resultant dataframe to left_join to combine this dataset with the state-level 2016 US presidential election data: combined_data &lt;- prop_data %&gt;% filter(state != &quot;PR&quot;) %&gt;% left_join(vote_data) Finally, we use ggplot() to visualize this combined dataset as a scatter plot. We colour each point by the party they voted for in the 2016 US presidential election by adding another argument to our aes() function in our ggplot() call: color = party: ggplot(combined_data, aes(x = med_income, y = med_prop_val, color = party)) + geom_point() + xlab(&quot;Income (USD)&quot;) + ylab(&quot;Median property value (USD)&quot;) Below, we put it all together in one code chunk, and you can see here that in relatively few lines of R code we are able to create an entire data science workflow: library(tidyverse) prop_data &lt;- read_csv(&quot;data/state_property_data.csv&quot;) vote_data &lt;- read_csv(&quot;data/2016_presidential_election_state_vote.csv&quot;) combined_data &lt;- prop_data %&gt;% filter(state != &quot;PR&quot;) %&gt;% left_join(vote_data) ggplot(combined_data, aes(x = med_income, y = med_prop_val, color = party)) + geom_point() + xlab(&quot;Income (USD)&quot;) + ylab(&quot;Median property value (USD)&quot;) 1.7.5 What’s next? In the next chapter we are going to dig in and spend more time learning how to load various differently formatted rectangular data sets into R, as well as how to scrape data from the web! "],
["reading.html", "Chapter 2 Reading in data locally and from the web 2.1 Overview 2.2 Chapter learning objectives 2.3 Absolute and relative file paths 2.4 Reading tabular data into R 2.5 Scraping data off the web using R 2.6 Additional readings/resources", " Chapter 2 Reading in data locally and from the web 2.1 Overview Learn to read in various cases of tabular data sets locally and from the web. Once read in, these data sets will be used to walk through a real world Data Science application that includes wrangling the data into a useable format and creating an effective data visualization. 2.2 Chapter learning objectives By the end of the chapter, students will be able to: define the following: absolute file path relative file path url match the following tidyverse read_* function arguments to their descriptions: file delim col_names skip choose the appropriate tidyverse read_* function and function arguments to load a given tabular data set into R use the rvest html_nodes, html_text and html_attr functions to scrape data from a .html file on the web compare downloading tabular data from a plain text file (e.g., .csv) from the web versus scraping data from a .html file 2.3 Absolute and relative file paths When you load in a data set a plain text file (e.g., .csv), you need to tell R where that files lives on the computer you are using. We call this the “path” to the file. There are two kinds of paths, relative paths and absolute paths. A relative path is where the file is in respect to where you currently are on the computer (e.g., where the Jupyter notebook file you are working in is). Whereas an absolute path is where the file is in respect to the base or root folder of the computer’s filesystem. If our computer’s filesystem looked like the picture below, and we were working in the Jupyter notebook titled “homework_02.ipynb” and we wanted to read in the .csv file named avocado_prices.csv into our Jupyter notebook using R, we could do this using either a relative or an absolute path. We show what both would be below. |-- Users |-- guest |-- Documents |-- dawkins |-- Documents |-- dsci-100 |-- homework_01 |-- homework_02 |-- homework_02.ipynb |-- data |-- avocado_prices.csv |-- homework_03 |-- Desktop 2.3.0.0.1 Loading avocado_prices.csv using a relative path: avocado_data &lt;- read_csv(&quot;data/avocado_prices.csv&quot;) 2.3.0.0.2 Loading avocado_prices.csv using an absolute path: avocado_data &lt;- read_csv(&quot;/Users/dawkins/Documents/dsci-100/homework_02/data/avocado_prices.csv&quot;) So which one should you use? Well to ensure your code can be run across different machines, you should choose to use the relative path (and it’s also less typing!). See this video for another explanation: 2.4 Reading tabular data into R Now we will learn more about reading tabular data into R, as well as how to write tabular data to a file. Last chapter we learned about using the tidyverse read_csv when the file we read it matches that functions expected defaults (column names are present, , is the delimiter/separator and there are no row names in the dataset ). We will now learn how to read files where that is not the case. Before we jump into the cases where the tidyverse read_csv functions expected defaults are not the case, let’s revisit how we use this with one that does and thus the only argument we need to give to the function is the path to the file, here “historical_vote.csv”. Here is how the file would look in plain text editor: election_num,election_year,winner,winner_party,elec_coll_votes_count,elec_coll_votes_perc,pop_votes_perc,pop_votes_perc_marg,pop_votes_count,pop_votes_count_marg,runner-up,runner-up_party,turnout 10,1824,John Quincy Adams,D.-R.,84/261,32.18%,30.92%,−10.44%,&quot;113,142&quot;,&quot;−38,221&quot;,Andrew Jackson,D.-R.,26.9% 23,1876,Rutherford Hayes,Rep.,185/369,50.14%,47.92%,−3.00%,&quot;4,034,142&quot;,&quot;−252,666&quot;,Samuel Tilden,Dem.,82.6% Using read_csv to load in R: library(tidyverse) read_csv(&quot;historical_vote.csv&quot;) # A tibble: 49 x 13 election_num election_year winner winne… elec_… elec_… pop_v… pop_v… pop_vo… pop_… `run… `run… turn… &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 10 1824 John Quincy Adams D.-R. 84/261 32.18% 30.92% −10.4… 1.13e⁵ −38,… Andr… D.-R. 26.9% 2 23 1876 Rutherford Hayes Rep. 185/3… 50.14% 47.92% −3.00% 4.03e⁶ −252… Samu… Dem. 82.6% 3 58 2016 Donald Trump Rep. 304/5… 56.50% 45.98% −2.10% 6.30e⁷ −2,8… Hill… Dem. 60.2% 4 26 1888 Benjamin Harrison Rep. 233/4… 58.10% 47.80% −0.83% 5.44e⁶ −94,… Grov… Dem. 80.5% 5 54 2000 George W. Bush Rep. 271/5… 50.37% 47.87% −0.51% 5.05e⁷ −543… Al G… Dem. 54.2% 6 24 1880 James Garfield Rep. 214/3… 57.99% 48.31% 0.09% 4.45e⁶ 1,898 Winf… Dem. 80.5% 7 44 1960 John Kennedy Dem. 303/5… 56.42% 49.72% 0.17% 3.42e⁷ 112,… Rich… Rep. 63.8% 8 25 1884 Grover Cleveland Dem. 219/4… 54.61% 48.85% 0.57% 4.91e⁶ 57,5… Jame… Rep. 78.2% 9 46 1968 Richard Nixon Rep. 301/5… 55.95% 43.42% 0.70% 3.18e⁷ 511,… Hube… Dem. 62.5% 10 15 1844 James Polk Dem. 170/2… 61.82% 49.54% 1.45% 1.34e⁶ 39,4… Henr… Whig 79.2% # ... with 39 more rows 2.4.1 read_delim as a more flexible method to get tabular data into R When our tabular data comes in a different format, we can use the read_delim function() instead. For example, a different version of this historical votes dataset has no column names and uses tabs as the delimiter instead of commas. Here is how the file would look in plain text editor: 10 1824 John Quincy Adams D.-R. 84/261 32.18% 30.92% −10.44% 113,142 −38,221 Andrew Jackson D.-R. 26.9% 23 1876 Rutherford Hayes Rep. 185/369 50.14% 47.92% −3.00% 4,034,142 −252,666 Samuel Tilden Dem. 82.6% 58 2016 Donald Trump Rep. 304/538 56.50% 45.98% −2.10% 62,979,636 −2,864,974 Hillary Rodham Clinton Dem. 60.2% To get this into R using the read_delim() function, we specify the first argument as the path to the file (as done with read_csv), and then provide values to the delim argument (here a tab) and the col_names argument (here false). Both read_csv() and read_delim() have a col_names argument and the default is True. library(tidyverse) read_delim(&quot;historical_vote_no_header.tsv&quot;, delim = &quot;\\t&quot;, col_names = FALSE) # A tibble: 49 x 13 X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12 X13 &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 10 1824 John Quincy Adams D.-R. 84/261 32.18% 30.92% −10.44% 113142 −38,221 Andrew Jacks… D.-R. 26.9% 2 23 1876 Rutherford Hayes Rep. 185/369 50.14% 47.92% −3.00% 4034142 −252,666 Samuel Tilden Dem. 82.6% 3 58 2016 Donald Trump Rep. 304/538 56.50% 45.98% −2.10% 62979636 −2,864,974 Hillary Rodh… Dem. 60.2% 4 26 1888 Benjamin Harrison Rep. 233/401 58.10% 47.80% −0.83% 5443633 −94,530 Grover Cleve… Dem. 80.5% 5 54 2000 George W. Bush Rep. 271/538 50.37% 47.87% −0.51% 50460110 −543,816 Al Gore Dem. 54.2% 6 24 1880 James Garfield Rep. 214/369 57.99% 48.31% 0.09% 4453337 1,898 Winfield Sco… Dem. 80.5% 7 44 1960 John Kennedy Dem. 303/537 56.42% 49.72% 0.17% 34220984 112,827 Richard Nixon Rep. 63.8% 8 25 1884 Grover Cleveland Dem. 219/401 54.61% 48.85% 0.57% 4914482 57,579 James Blaine Rep. 78.2% 9 46 1968 Richard Nixon Rep. 301/538 55.95% 43.42% 0.70% 31783783 511,944 Hubert Humph… Dem. 62.5% 10 15 1844 James Polk Dem. 170/275 61.82% 49.54% 1.45% 1339570 39,413 Henry Clay Whig 79.2% # ... with 39 more rows 2.4.2 Reading tabular data directly from a URL We can also use read_csv() or read_delim() (and related functions) to read in tabular data directly from a url that contains tabular data. In this case, we provide the url as a string to read_csv() as the path to the file instead of a path to a local file on our computer. All other arguments that we use are the same as when using these functions with a local file on our computer. library(tidyverse) read_csv(&quot;https://github.com/swcarpentry/r-novice-gapminder/raw/gh-pages/data/gapminder_wide.csv&quot;) # A tibble: 142 x 38 conti… count… gdpPe… gdpPe… gdpPe… gdpP… gdpP… gdpP… gdpP… gdpP… gdpP… gdpP… gdpP… gdpP… life… life… &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Africa Alger… 2449 3014 2551 3247 4183 4910 5745 5681 5023 4797 5288 6223 43.1 45.7 2 Africa Angola 3521 3828 4269 5523 5473 3009 2757 2430 2628 2277 2773 4797 30.0 32.0 3 Africa Benin 1063 960 949 1036 1086 1029 1278 1226 1191 1233 1373 1441 38.2 40.4 4 Africa Botsw… 851 918 984 1215 2264 3215 4551 6206 7954 8647 11004 12570 47.6 49.6 5 Africa Burki… 543 617 723 795 855 743 807 912 932 946 1038 1217 32.0 34.9 6 Africa Burun… 339 380 355 413 464 556 560 622 632 463 446 430 39.0 40.5 7 Africa Camer… 1173 1313 1400 1508 1684 1783 2368 2603 1793 1694 1934 2042 38.5 40.4 8 Africa Centr… 1071 1191 1193 1136 1070 1109 957 845 748 741 739 706 35.5 37.5 9 Africa Chad 1179 1308 1390 1197 1104 1134 798 952 1058 1005 1156 1704 38.1 39.9 10 Africa Comor… 1103 1211 1407 1876 1938 1173 1267 1316 1247 1174 1076 986 40.7 42.5 # ... with 132 more rows, and 22 more variables: lifeExp_1962 &lt;dbl&gt;, lifeExp_1967 &lt;dbl&gt;, lifeExp_1972 # &lt;dbl&gt;, lifeExp_1977 &lt;dbl&gt;, lifeExp_1982 &lt;dbl&gt;, lifeExp_1987 &lt;dbl&gt;, lifeExp_1992 &lt;dbl&gt;, lifeExp_1997 # &lt;dbl&gt;, lifeExp_2002 &lt;dbl&gt;, lifeExp_2007 &lt;dbl&gt;, pop_1952 &lt;dbl&gt;, pop_1957 &lt;dbl&gt;, pop_1962 &lt;dbl&gt;, # pop_1967 &lt;dbl&gt;, pop_1972 &lt;dbl&gt;, pop_1977 &lt;dbl&gt;, pop_1982 &lt;dbl&gt;, pop_1987 &lt;dbl&gt;, pop_1992 &lt;dbl&gt;, # pop_1997 &lt;dbl&gt;, pop_2002 &lt;int&gt;, pop_2007 &lt;int&gt; 2.4.3 Previewing a data file before reading it into R In all the examples above, we gave you previews of the data file before we read it into R. This is essential so you can see whether or not there are column names, what the delimiters are, and if there are lines you need to skip. You should do this yourself when trying to read in data files. In Jupyter, you can do this by using the Jupyter home menu to navigate to the file and clicking on it to preview it as a plain text file. We demonstrate this in the video below: 2.5 Scraping data off the web using R In the first part of this chapter we learned how to read in data from plain text files that are usually “rectangular” in shape using the tidyverse read_* functions. Sadly, not all data comes in this simple format, but happily there are many other tools we can use to read in more messy/wild data formats. One common place people often want/need to read in data from is websites. Such data exists in an a non-rectangular format. One quick and easy solution to get this data is to copy and paste it, however this becomes painstakingly long and boring when there is a lot of data that needs gathering, and anytime you start doing a lot of copying and pasting it is very likely you will introduce errors. The formal name for gathering non-rectangular data from the web and transforming it into a more useful format for data analysis is web scraping. There are two different ways to do web scraping: 1) screen scraping (similar to copying and pasting from a website, but done in a programmatic way to minimize errors and maximize efficiency) and 2) web APIs (application programming interface) (a website that provides a programatic way of returning the data as JSON or XML files via http requests). In this course we will explore the first method, screen scraping using R’s rvest package. 2.5.1 HTML and CSS selectors Before we jump into scraping, let’s set up some motivation and learn a little bit about what the “source code” of a website looks like. Say we are interested in knowing the average rental price (per square footage) of the most recently available 1 bedroom apartments in Vancouver from https://vancouver.craigslist.org. When we visit the Vancouver Craigslist website and search for 1 bedroom apartments, this is what we are shown: From that page, it’s pretty easy for our human eyes to find the apartment price and square footage. But how can we do this programmatically so we don’t have to copy and paste all these numbers? Well, we have to deal with the webpage source code, which we show a snippet of below (and link to the entire source code here): &lt;span class=&quot;result-meta&quot;&gt; &lt;span class=&quot;result-price&quot;&gt;$800&lt;/span&gt; &lt;span class=&quot;housing&quot;&gt; 1br - &lt;/span&gt; &lt;span class=&quot;result-hood&quot;&gt; (13768 108th Avenue)&lt;/span&gt; &lt;span class=&quot;result-tags&quot;&gt; &lt;span class=&quot;maptag&quot; data-pid=&quot;6786042973&quot;&gt;map&lt;/span&gt; &lt;/span&gt; &lt;span class=&quot;banish icon icon-trash&quot; role=&quot;button&quot;&gt; &lt;span class=&quot;screen-reader-text&quot;&gt;hide this posting&lt;/span&gt; &lt;/span&gt; &lt;span class=&quot;unbanish icon icon-trash red&quot; role=&quot;button&quot; aria-hidden=&quot;true&quot;&gt;&lt;/span&gt; &lt;a href=&quot;#&quot; class=&quot;restore-link&quot;&gt; &lt;span class=&quot;restore-narrow-text&quot;&gt;restore&lt;/span&gt; &lt;span class=&quot;restore-wide-text&quot;&gt;restore this posting&lt;/span&gt; &lt;/a&gt; &lt;/span&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;result-row&quot; data-pid=&quot;6788463837&quot;&gt; &lt;a href=&quot;https://vancouver.craigslist.org/nvn/apa/d/north-vancouver-luxury-1-bedroom/6788463837.html&quot; class=&quot;result-image gallery&quot; data-ids=&quot;1:00U0U_lLWbuS4jBYN,1:00T0T_9JYt6togdOB,1:00r0r_hlMkwxKqoeq,1:00n0n_2U8StpqVRYX,1:00M0M_e93iEG4BRAu,1:00a0a_PaOxz3JIfI,1:00o0o_4VznEcB0NC5,1:00V0V_1xyllKkwa9A,1:00G0G_lufKMygCGj6,1:00202_lutoxKbVTcP,1:00R0R_cQFYHDzGrOK,1:00000_hTXSBn1SrQN,1:00r0r_2toXdps0bT1,1:01616_dbAnv07FaE7,1:00g0g_1yOIckt0O1h,1:00m0m_a9fAvCYmO9L,1:00C0C_8EO8Yl1ELUi,1:00I0I_iL6IqV8n5MB,1:00b0b_c5e1FbpbWUZ,1:01717_6lFcmuJ2glV&quot;&gt; &lt;span class=&quot;result-price&quot;&gt;$2285&lt;/span&gt; &lt;/a&gt; &lt;p class=&quot;result-info&quot;&gt; &lt;span class=&quot;icon icon-star&quot; role=&quot;button&quot;&gt; &lt;span class=&quot;screen-reader-text&quot;&gt;favorite this post&lt;/span&gt; &lt;/span&gt; &lt;time class=&quot;result-date&quot; datetime=&quot;2019-01-06 12:06&quot; title=&quot;Sun 06 Jan 12:06:01 PM&quot;&gt;Jan 6&lt;/time&gt; &lt;a href=&quot;https://vancouver.craigslist.org/nvn/apa/d/north-vancouver-luxury-1-bedroom/6788463837.html&quot; data-id=&quot;6788463837&quot; class=&quot;result-title hdrlnk&quot;&gt;Luxury 1 Bedroom CentreView with View - Lonsdale&lt;/a&gt; This is not easy for our human eyeballs to read! However, it is easy for us to use programmatic tools to extract the data we need by specifying which HTML tags (things inside &lt; and &gt; in the code above). For example, if we look in the code above and search for lines with a price, we can also look at the tags that are near that price and see if there’s a common “word” we can use that is near the price but doesn’t exist on other lines that have information we are not interested in: &lt;span class=&quot;result-price&quot;&gt;$800&lt;/span&gt; and &lt;span class=&quot;result-price&quot;&gt;$2285&lt;/span&gt; What we can see is there is a special “word” here, “result-price”, which appears only on the lines with prices and not on the other lines (that have information we are not interested in). This special word and the context in which is is used (learned from the other words inside the HTML tag) can be combined to create something called a CSS selector. The CSS selector can then be used by R’s rvest package to select the information we want (here price) from the website source code. Now, many websites are quite large and complex, and so then is their website source code. And as you saw above, it is not easy to read and pick out the special words we want with our human eyeballs. So to make this easier, we will use the SelectorGadget tool. It is an open source tool that simplifies generating and finding CSS selectors. We recommend you use the Chrome web browser to use this tool, and install the selector gadget tool from the Chrome Web Store. Here is a short video on how to install and use the SelectorGadget tool to get a CSS selector for use in web scraping: From installing and using the selectorgadget as shown in the video above, we get the two CSS selectors .housing and .result-price that we can use to scrape information about the square footage and the rental price, respectively. The selector gadget returns them to us as a comma separated list (here .housing , .result-price), which is exactly the format we need to provide to R if we are using more than one CSS selector. 2.5.2 Are you allowed to scrape that website? BEFORE scraping data from the web, you should always check whether or not you are ALLOWED to scrape it! There are two documents that are important for this: the robots.txt file and reading the website’s Terms of Service document. The website’s Terms of Service document is probably the more important of the two, and so you should look there first. What happens when we look at Craigslist’s Terms of Service document? Well we read this: “You agree not to copy/collect CL content via robots, spiders, scripts, scrapers, crawlers, or any automated or manual equivalent (e.g., by hand).” source: https://www.craigslist.org/about/terms.of.use Want to learn more about the legalities of web scraping and crawling? Read this interesting blog post titled “Web Scraping and Crawling Are Perfectly Legal, Right?” by Benoit Bernard (this is optional, not required reading). So what to do now? Well, we shouldn’t scrape Craigslist! Let’s instead scrape some data on the population of Canadian cities from Wikipedia (who’s Terms of Service document does not explicilty say do not scrape). In this video below we demonstrate using the selectorgadget tool to get CSS Selectors from Wikipedia’s Canada page to scrape a table that contains city names and their populations from the 2016 Canadian Census: 2.5.3 Using rvest Now that we have our CSS selectors we can use rvest R package to scrape our desired data from the website. First we start by loading the rvest package: library(rvest) library(rvest) gives error… If you get an error about R not being able to find the package (e.g., Error in library(rvest) : there is no package called ‘rvest’) this is likely because it was not installed. To install the rvest package, run the following command once inside R (and then delete that line of code): install.packages(&quot;rvest&quot;). Next, we tell R what page we want to scrape by providing the webpage’s URL in quotations to the function read_html: page &lt;- read_html(&quot;https://en.wikipedia.org/wiki/Canada&quot;) Then we send the page object to the html_nodes function. We also provide that function with the CSS selectors we obtained from the selectorgadget tool. These should be surrounded by quotations. The html_nodes function select nodes from the HTML document using CSS selectors. nodes are the HTML tag pairs as well as the content between the tags. For our CSS selector td:nth-child(5) and example node that would be selected would be: &lt;td style=&quot;text-align:left;background:#f0f0f0;&quot;&gt;&lt;a href=&quot;/wiki/London,_Ontario&quot; title=&quot;London, Ontario&quot;&gt;London&lt;/a&gt;&lt;/td&gt; population_nodes &lt;- html_nodes(page, &quot;td:nth-child(5) , td:nth-child(7) , .infobox:nth-child(122) td:nth-child(1) , .infobox td:nth-child(3)&quot;) head(population_nodes) ## {xml_nodeset (6)} ## [1] &lt;td style=&quot;text-align:right;&quot;&gt;5,928,040&lt;/td&gt; ## [2] &lt;td style=&quot;text-align:left;background:#f0f0f0;&quot;&gt;&lt;a href=&quot;/wiki/Londo ... ## [3] &lt;td style=&quot;text-align:right;&quot;&gt;494,069\\n&lt;/td&gt; ## [4] &lt;td style=&quot;text-align:right;&quot;&gt;4,098,927&lt;/td&gt; ## [5] &lt;td style=&quot;text-align:left;background:#f0f0f0;&quot;&gt;\\n&lt;a href=&quot;/wiki/St. ... ## [6] &lt;td style=&quot;text-align:right;&quot;&gt;406,074\\n&lt;/td&gt; Next we extract the meaningful data from the HTML nodes using the html_text function. For our example, this functions only required argument is the an html_nodes object, which we named rent_nodes. In the case of this example node: &lt;td style=&quot;text-align:left;background:#f0f0f0;&quot;&gt;&lt;a href=&quot;/wiki/London,_Ontario&quot; title=&quot;London, Ontario&quot;&gt;London&lt;/a&gt;&lt;/td&gt;, the html_text function would return London. population_text &lt;- html_text(population_nodes) head(population_text) ## [1] &quot;5,928,040&quot; &quot;London&quot; ## [3] &quot;494,069\\n&quot; &quot;4,098,927&quot; ## [5] &quot;St. Catharines–Niagara&quot; &quot;406,074\\n&quot; Are we done? Not quite… If you look at the data closely you see that the data is not in an optimal format for data analysis. Both the city names and population are encoded as characters in a single vector instead of being in a data frame with one character column for city and one numeric column for population (think of how you would organize the data in a spreadsheet). Additionally, the populations contain commas (not useful for programmatically dealing with numbers), and some even contain a line break character at the end (\\n). Next chapter we will learn more about data wrangling using R so that we can easily clean up this data with a few lines of code. 2.6 Additional readings/resources Data import chapter from R for Data Science by Garrett Grolemund &amp; Hadley Wickham "],
["wrangling.html", "Chapter 3 Cleaning and wrangling data 3.1 Overview 3.2 Chapter learning objectives 3.3 Vectors and Data frames 3.4 The dplyr functions 3.5 Tidy Data 3.6 Using purrr’s map* functions to iterate 3.7 Additional readings/resources", " Chapter 3 Cleaning and wrangling data 3.1 Overview This chapter will be centered around tools for cleaning and wrangling data. Again, this will be in the context of a real world Data Science application and we will continue to practice working through a whole case study 3.2 Chapter learning objectives By the end of the chapter, students will be able to: define the term “tidy data” discuss the advantages and disadvantages from storing data in a tidy data format recall and use the following tidyverse functions and operators for their intended data wrangling tasks: select filter map mutate summarise group_by gather separate %in% 3.3 Vectors and Data frames At this point, we know how to load flat, tabular data files into R using the tidyverse functions. When we do this, the data is represented in R as a data frame object. So now, we will spend some time learning more about these types of objects in R and what they are made up of so that we have a better understanding of how we can use and manipulate these objects. 3.3.1 What is a data frame? Let’s first start by defining exactly what a data frame is. From a data perspective, it is a rectangle where the rows are the observations. And the columns are the variables. Data frames in R are built-in, base R objects. From a computer science type/object perspective, in R, a data frame is a special sub-type of a list object whose elements (columns) are vectors. 3.3.2 What is a vector? In R, vectors are objects that can contain 1 or more elements. The vector elements are ordered, and they must all be of the same type. They are also mutable. Vectors are also built-in, base R objects. 3.3.3 How are vectors different from a list? Vectors and lists differ by the requirement of element type consistency. All elements within a single vector must be of the same type (e.g., all elements are strings), whereas elements within a single list can be of different types (e.g., strings and numbers can be elements in the same list). 3.3.4 What does this have to do with data frames? As mentioned earlier, data frames are really specialized lists of vectors that allow us to easily work with our data in a rectangular/spreadsheet like manner. This allows us have columns/vectors of different characteristics associated/linked in one object similar to a table in a database. 3.4 The dplyr functions We haven’t explicitly said this yet, but the tidyverse is actually a meta R package that installs a collection of R packages that all follow the tidy data philosophy (more on this below). One of the tidyverse packages is dplyr - a data wrangling workhorse. You have already met 3 of the dplyr function (select, filter and mutate). To learn more about those three and meet a few more useful ones, read the post at this link: http://stat545.com/block010_dplyr-end-single-table.html#where-were-we 3.5 Tidy Data There are many ways a spreadsheet-like dataset can be organized. In this chapter we are going to focus on the tidy data format of organization, and how to make your messy data tidy. This is because a variety of tools we would like to be able to use in R are designed to work most effectively (and efficiently) with tidy data. 3.5.1 What is tidy data? A tidy data is one that is satified by these three criteria: each row is a single observation, each variable is a single column, and each value is a single cell (i.e., its row, column position in the data frame is not shared with another value) image source: R for Data Science by Garrett Grolemund &amp; Hadley Wickham 3.5.2 Why is tidy data important in R? First, many tidyverse data cleaning/wrangling tools work best with tidy data. Second, one of the most popular plotting tools in R, the ggplot2 functions, expect data in tidy format. Third most statistical analysis functions expect data in tidy format. In contrast to Python and other programming languages, in R you do not need to manually create dummy variables to represent your categorical data for its statistical or machine learning tools. R does this for you under the hood through factorization, but your data must be tidy for this to work. Given that all of these tasks are central features to virtually any data analysis project it is well worth spending the time to get your data into a tidy format up front. Now let’s explore how we can do this in R. 3.5.3 Going from wide to long (or tidy!) using gather One common thing that often has to be done to get data into a tidy format is to gather columns so that each row is a single observation and each column is a single variable. Often times data does not come this way, as although tidy data is a better organization structure for data analysis, it is not as intuitive of a data organization structure for human readability and understanding. For example, the we read in data below is not in tidy format, but is in a very intuitive format for human understanding: library(tidyverse) hist_vote_wide &lt;- read_csv(&quot;historical_vote_wide.csv&quot;) head(hist_vote_wide) # A tibble: 6 x 3 election_year winner runnerup &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 2016 Donald Trump Hillary Clinton 2 2012 Barack Obama Mitt Romney 3 2008 Barack Obama John McCain 4 2004 George Bush John Kerry 5 2000 George Bush Al Gore 6 1996 Bill Clinton Bob Dole What is wrong with our untidy format above? From a data analysis perspective, this format is not idead because in this format the outcome of the variable “result” (winner or runner up) is stored as column names and not easily accessible for the functions we will desire to apply to our data set. Additionally, the values of the “candidate” variable is spread across two columns and will require some sort of binding or joining to get them into one single column to allow us to do our desired visualization and statistical tasks later on. To accomplish this data tranformation we will use the tidyverse function gather. To use gather we need to specify: the dataset the key which is the name of a new column that will be created and whose values will come from names of the columns that we want to combine (here result) the value which is the name of a new column that will be created and whose values will come from the values of the columns we want to combine (here value) the names of the columns that we want to combine (we list these after specifying the key and value and separate the column names with commas) For our example we would use gather to combine the winner and runnerup columns into a single column called candidate, and create a column called result that contains the outcome of the election for each candidate: hist_vote_tidy &lt;- hist_vote_wide %&gt;% gather(key = result, value = candidate, winner, runnerup) print(hist_vote_tidy) # A tibble: 20 x 3 election_year result candidate &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 2016 winner Donald Trump 2 2012 winner Barack Obama 3 2008 winner Barack Obama 4 2004 winner George W Bush 5 2000 winner George W Bush 6 1996 winner Bill Clinton 7 1992 winner Bill Clinton 8 1988 winner George HW Bush 9 1984 winner Ronald Reagan 10 1980 winner Ronald Reagan 11 2016 runnerup Hillary Clinton 12 2012 runnerup Mitt Romney 13 2008 runnerup John McCain 14 2004 runnerup John Kerry 15 2000 runnerup Al Gore 16 1996 runnerup Bob Dole 17 1992 runnerup George HW Bush 18 1988 runnerup Michael Dukakis 19 1984 runnerup Walter Mondale 20 1980 runnerup Jimmy Carter 3.5.4 Using separate to deal with multiple delimiters Data is also not considered tidy when multiple values are stored in the same cell. We can see that in addition to the previous untidy problem we faced with the earlier version of this data set, the one we show below is even messier because the winner and runnerup columns contain both the candidate’s name as well as the party they were a member of. To make this messy data tidy we’ll have to fix both of these issues. hist_vote_messy &lt;- read_csv(&quot;historical_vote_messy.csv&quot;) print(hist_vote_messy) # A tibble: 10 x 3 election_year winner runnerup &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 2016 Donald Trump/Rep Hillary Clinton/Dem 2 2012 Barack Obama/Dem Mitt Romney/Rep 3 2008 Barack Obama/Dem John McCain/Rep 4 2004 George W Bush/Rep John Kerry/Dem 5 2000 George W Bush/Rep Al Gore/Dem 6 1996 Bill Clinton/Dem Bob Dole/Rep 7 1992 Bill Clinton/Dem George HW Bush/Rep 8 1988 George HW Bush/Rep Michael Dukakis/Dem 9 1984 Ronald Reagan/Rep Walter Mondale/Dem 10 1980 Ronald Reagan/Rep Jimmy Carter/Dem First we’ll use gather to create the result and candidate column as we did previously: gather(hist_vote_messy, key = result, value = candidate, winner, runnerup) # A tibble: 20 x 3 election_year result candidate &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 2016 winner Donald Trump/Rep 2 2012 winner Barack Obama/Dem 3 2008 winner Barack Obama/Dem 4 2004 winner George W Bush/Rep 5 2000 winner George W Bush/Rep 6 1996 winner Bill Clinton/Dem 7 1992 winner Bill Clinton/Dem 8 1988 winner George HW Bush/Rep 9 1984 winner Ronald Reagan/Rep 10 1980 winner Ronald Reagan/Rep 11 2016 runnerup Hillary Clinton/Dem 12 2012 runnerup Mitt Romney/Rep 13 2008 runnerup John McCain/Rep 14 2004 runnerup John Kerry/Dem 15 2000 runnerup Al Gore/Dem 16 1996 runnerup Bob Dole/Rep 17 1992 runnerup George HW Bush/Rep 18 1988 runnerup Michael Dukakis/Dem 19 1984 runnerup Walter Mondale/Dem 20 1980 runnerup Jimmy Carter/Dem And then we separate to split the candidate column into two columns, one called candidate that now contains only the candidate’s name, and called party that contains a short identifier for which political party the candidate belonged to: gather(hist_vote_messy, key = result, value = candidate, winner, runnerup) %&gt;% separate(col = candidate, into = c(&quot;candidate&quot;, &quot;party&quot;), sep = &quot;/&quot;) # A tibble: 20 x 4 election_year result candidate party &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 2016 winner Donald Trump Rep 2 2012 winner Barack Obama Dem 3 2008 winner Barack Obama Dem 4 2004 winner George W Bush Rep 5 2000 winner George W Bush Rep 6 1996 winner Bill Clinton Dem 7 1992 winner Bill Clinton Dem 8 1988 winner George HW Bush Rep 9 1984 winner Ronald Reagan Rep 10 1980 winner Ronald Reagan Rep 11 2016 runnerup Hillary Clinton Dem 12 2012 runnerup Mitt Romney Rep 13 2008 runnerup John McCain Rep 14 2004 runnerup John Kerry Dem 15 2000 runnerup Al Gore Dem 16 1996 runnerup Bob Dole Rep 17 1992 runnerup George HW Bush Rep 18 1988 runnerup Michael Dukakis Dem 19 1984 runnerup Walter Mondale Dem 20 1980 runnerup Jimmy Carter Dem 3.6 Using purrr’s map* functions to iterate Where should you turn when you discover the next step in your data wrangling/cleaning process requires you to apply a function to each column in a data frame? For example, if you wanted to know the maximum value of each column in a data frame, you could use purrr’s map function to apply the max function to each column. For example, let’s do that and find that maximum value of each column of the mtcars data frame (a built-in data set that comes with R). First, let’s peak at an become familiar with the data: head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Next, we can use map to apply the max function to each column. map takes two arguments, an object (a vector, data frame or list) that you want to apply the function to, and the function that you would like to apply. Here our arguments will be mtcars and max: max_of_columns &lt;- map(mtcars, max) max_of_columns ## $mpg ## [1] 33.9 ## ## $cyl ## [1] 8 ## ## $disp ## [1] 472 ## ## $hp ## [1] 335 ## ## $drat ## [1] 4.93 ## ## $wt ## [1] 5.424 ## ## $qsec ## [1] 22.9 ## ## $vs ## [1] 1 ## ## $am ## [1] 1 ## ## $gear ## [1] 5 ## ## $carb ## [1] 8 3.6 Note: purrr is part of the tidyverse, and so like the dplyr and ggplot functions, once we call library(tidyverse) we do not need to separately load the purrr package. Our output looks a bit weird though? We passed in a data frame, but our output doesn’t look like a data frame… And that it because it isn’t, its a plain vanilla list: typeof(max_of_columns) ## [1] &quot;list&quot; So what do we do? Convert this to a data frame? No need, what we should do instead is to use a different map* function from the purrr package. There are quite a few to choose from, they all work similarly and their name refects the type of output you want from the mapping operation: map function Output map() list map_lgl() logical vector map_int() integer vector map_dbl() double vector map_chr() character vector map_df() data frame So let’s get the column maximum’s again, but this time use the map_df function to return the output as a data frame: max_of_columns &lt;- map_df(mtcars, max) max_of_columns ## # A tibble: 1 x 11 ## mpg cyl disp hp drat wt qsec vs am gear carb ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 33.9 8 472 335 4.93 5.42 22.9 1 1 5 8 Do we always want to use map_df? Nope! It depends what we are trying to do. So before you choose your map* function, think about your downstream usage of the output of that function. What if you need to add other arguments to the functions you want to map? For example, what if there were NA values in our columns that we wanted to know the maximum of? Well then we also need to add the argument na.rm = TRUE to the max function so that we get a more useful value than NA returned (remember that is what happens with many of the built-in R statistical functions when NA’s are present…). What we need to do in that case is do what is called “creating an anonymous function” within the map_df function. We do that in the place where we previously specified our max function. Here we will put the two calls to map_df right after each other so you can see the difference: # no additional arguments to the max function map_df(mtcars, max) versus # adding the na.rm = TRUE argument to the max function map_df(mtcars, function(df) max(df, na.rm = TRUE)) You can see that’s quite a bit of extra typing… So the creators of purrr have made a shortcut for this because it is so commonly done. In the shortcut we replace function(VARIABLE) with a ~ and replace the VARIABLE in the function call with a ., see the example below: # adding the na.rm = TRUE argument to the max function using the shortcut map_df(mtcars, ~ max(., na.rm = TRUE)) 3.6.1 A bit more about the map* functions The map functions are generally quite useful for solving many iteration/repetition problems. It’s use does not have to be limited to columns of a data frame. It can be used to apply functions to elements of a vector or list, rows of a data frame (using pmap), and even to lists of data frames, or nested data frames. 3.7 Additional readings/resources Data transformation chapter from R for Data Science by Garrett Grolemund &amp; Hadley Wickham Tidy data chapter from R for Data Science by Garrett Grolemund &amp; Hadley Wickham The map functions from R for Data Science by Garrett Grolemund &amp; Hadley Wickham "],
["viz.html", "Chapter 4 Effective data visualization 4.1 Overview 4.2 Chapter learning objectives 4.3 ggplot2 for data visualization in R 4.4 Making effective vizualizations", " Chapter 4 Effective data visualization 4.1 Overview Expand your data visualization knowledge and tool set beyond what we have seen and practiced so far. We will move beyond scatter plots and learn other effective ways to visualize data, as well as some general rules of thumb to follow when creating visualizations. All visualization tasks this week will be applied to real world data sets. 4.2 Chapter learning objectives Define the three key aspects of ggplot objects: aesthetic mappings geometric objects scales Use the ggplot2 function in R to create the following visualizations: 2-D scatter plot 2-D scatter plot with a third variable that stratifies the groups count bar chart for multiple groups proportion bar chart for multiple groups stacked bar chart for multiple groups List the rules of thumb for effective visualizations Given a visualization and a sentence describing it’s intended task, evaluate it’s effectiveness and suggest ways to improve the visualization with respect to that intended task 4.3 ggplot2 for data visualization in R (or alternate title, “how” to use R and ggplot2 to make plots) The creator of ggplot2, Hadley Wickham, has written a wonderful chapter on how to use this library to create visualizations in R, so we suggest you read that for this week’s pre-reading: Data visualisation chapter from R for Data Science by Garrett Grolemund &amp; Hadley Wickham 4.4 Making effective vizualizations Just being able to make vizualizations in R with ggplot2 (or any other tool for that matter) doesn’t mean that your vizualization is effective at communicating to others what you are trying to communicate. There is a large body of research behind what makes effective visualizations and it seems it is really dependent on what humans can see and process. Claus Wilke, a Professor of Integrative Biology at The University of Texas at Austin, has written down some (digestible) guiding principles for making effectice visualizations based on this literature in his new book, Fundamentals of Data Visualization. Below we give a high level listing of these, but we strongly recommend you read and/or refer Part II: Principles of figure design when creating your visualizations in this class (or any other time you are doing this task!). 4.4.1 Some guiding principles for making effectice visualizations Only make the plot area (where the dots, lines, bars are) as big as needed (simple plots can, and should be quite small) Don’t adjust the axes to zoom in on small differences (if the difference is small, show that its small!) Show the data (don’t hide the shape/distribution of the data behind a bar) Be wary of overplotting (if your plot has too many dots or lines and it starts to look like a mess, then you need to do something different) Use colors sparingly (too many different colors can distract and even create false patterns) Use legends and labels so that your visualization is understandable without reading the surrounding text Ensure the text on your visualization is big enough to be easily read Do not use pie charts (its harder for us to compare the sizes of the slices of a pie, than it is to compare sizes of bars). Do not use 3D (we don’t see in 3D, not even as we’re walking around in space) "],
["GitHub.html", "Chapter 5 Tools for Collaborating 5.1 Overview &amp; Learning Objectives 5.2 Why do we need special tools for this? 5.3 Working Online with Colab 5.4 Version Control with Git and GitHub 5.5 Additional Resources 5.6 Videos to learn about version control with GitHub and Git 5.7 Git command cheatsheet 5.8 Terminal cheatsheet", " Chapter 5 Tools for Collaborating 5.1 Overview &amp; Learning Objectives This chapter covers tools that will help you collaborate effectively with others on software and data analysis projects. This includes Colaboratory, an online Jupyter notebook server; Git, a version control tool for maintaining, keeping track of, merging, and switching between different versions of code; and GitHub, a hosting service for Git repositories that helps you keep track of bugs, communicate with collaborators, share your project, and keep it safely backed up in the cloud. By the end of this chapter, you’ll be able to: [TODO] colab create a git code repository on GitHub download the repository to your own computer make changes to your code and upload them to GitHub download changes made by others from GitHub revert to an older version of your code Here is a list of videos you might want to watch to familiarize yourself further with these tools, as well as a cheatsheet of Git and terminal commands. 5.2 Why do we need special tools for this? When you’re working on your own, when you’re working with others 5.3 Working Online with Colab what is colab 5.3.1 Creating a notebook 5.3.2 Sharing a notebook 5.3.3 Reverting a notebook 5.3.4 Best Practices 5.4 Version Control with Git and GitHub What is git and github, difference 5.4.1 Creating a Repository 5.4.2 Downloading to Your Computer 5.4.3 Best Practices no big files check what others have done before you work issues 5.5 Additional Resources bitbucket gitlab video links more advanced git tutorials git is distribution; no one master repository continual integration branching, diffs, merging 5.6 Videos to learn about version control with GitHub and Git 5.6.1 Creating a GitHub repository 5.6.2 Exploring a GitHub repository 5.6.3 Directly editing files on GitHub 5.6.4 Logging changes and pushing them to GitHub 5.7 Git command cheatsheet Because we are writing code on a server, we need to use Git in the terminal to get files from GitHub, and to send back changes to the files that we make on the server. Below is a cheat sheet of the commands you will need and what they are for: 5.7.1 Getting a repository from GitHub onto the server for the first time This is done only once for a repository when you want to copy it to a new computer. git clone https://github.com/USERNAME/GITHUB_REPOSITORY_NAME.git 5.7.2 Logging changes After editing and saving your files (e.g., a Jupyter notebook): git add FILENAME git commit -m &quot;some message about the changes you made&quot; 5.7.3 Sending your changes back to GitHub After logging your changes (as shown above): git push 5.7.4 Getting changes To get the changes your collaborator just sent to GitHub onto your server: git pull 5.8 Terminal cheatsheet We need to run the above Git commands from inside the repository/folder that we cloned from GitHub. To navigate there in the terminal, you will need to use the following commands: 5.8.1 See where you are: pwd 5.8.2 See what is inside the directory where you are: ls 5.8.3 Move to a different directory cd DIRECTORY_PATH "],
["classification.html", "Chapter 6 Classification 6.1 Overview 6.2 Learning objectives 6.3 Classification 6.4 Wisconsin Breast Cancer Example: 6.5 Additional readings/resources", " Chapter 6 Classification 6.1 Overview This chapter serves as an introduction to classification using K-nearest neighbours (k-nn) in the case where we have two quantitative variables that we want to use to predict the class of a third, categorical variable. 6.2 Learning objectives Recognize situations where a simple classifier would be appropriate for making predictions. Explain the k-nearest neighbour classification algorithm. Interpret the output of a classifier. Compute, by hand, the straight-line (Euclidean) distance between points on a graph when there are two explanatory variables/predictors. Describe what a training data set is and how it is used in classification. In a dataset with two explanatory variables/predictors, perform k-nearest neighbour classification in R using caret::train(method = &quot;knn&quot;, ...) to predict the class of a single new observation. 6.3 Classification In many situations, we want to learn how to make predictions based on our experience from past examples. For instance, a doctor wants to diagnose a patient as either diseased or healthy based on some observed characteristics, an email provider would like to assign a given email as “spam” or “non-spam”, or an online store wants to predict if an order is fraudulent (or not). These are all examples of classification tasks. Classification is the problem of predicting a qualitative or categorical class/label for an observation (set of data collected from an object, such as a person or an email). It involves assigning an observation to a class (e.g. disease or healthy) on the basis of how similar they are to other observations that have already been classified. These already classified observations that we use as a basis to predict classes for new, unclassfied observations is called a training set. We call them a “training set” because we use these observations to train, or teach, our classifier so that we can use it to make predictions on new data that we have not seen previously. There are many possible classifier methods that we could use to predict a qualitative or categorical class/label for an observation. These classification methods can perform binary classification, where only two classes are involved (e.g. disease or healthy patient), as well as multiclass classification, which involves assigning an object to one of several classes (e.g., private, public, or not for-profit organization). Here we will focus on a simple, and widely used method of classification called K-nearest neighbors, but other examples include decision trees, support vector machines and logistic regression. 6.4 Wisconsin Breast Cancer Example: Let’s start by looking at some Breast Cancer data, which was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg. Each row in the data set represents an observation which includes the tumour diagnosis (benign/non-cancerous or malignant/cancerous) and several other measurements about the tumour cells (e.g., cell nuclei texture, perimeter, etc.). Diagnosis of the tumour was determined by Physicians. The question here is whether we can use some, or all, of the measurements available to us about the tumour cells to do a good job of predicting whether a future tumour, that we don’t have a diagnosis from a Physician for, is benign or malignant. Answering this question is important because traditional, non-data driven methods for tumour diagnosis are quite subjective and dependent upon how skilled and experienced the diagnosing Physician is. Furthermore, benign tumours are not normally dangerous, the cells stay in the same place and the tumour stops growing before it gets very large, whereas in malignant tumours, the cells invade the surrounding tissue and spread into nearby organs where they can cause serious damage (Learn more here). Thus it is important to quickly and accurately diagnose the tumour type to guide patient treatment protocols. 6.4.1 Data Exploration As usual, we start by loading the necessary libraries for our analysis. We have learned about the tidyverse before. Today we’ll also be loading a new library, forcats that allows us to easily manipulate factors in R. Factors are a special categorical type of variable in R that are very helpful when doing statistical inference and machine learning with categorical variables. library(tidyverse) library(forcats) The data file we need to read in is a plain vanilla csv with headers, and thus we can use the read_csv function with no additional arguments: cancer &lt;- read_csv(&quot;data/clean-wdbc.data.csv&quot;) head(cancer) ## # A tibble: 6 x 12 ## ID Class Radius Texture Perimeter Area Smoothness Compactness ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 8.42e5 M 1.89 -1.36 2.30 2.00 1.31 2.61 ## 2 8.43e5 M 1.80 -0.369 1.53 1.89 -0.375 -0.430 ## 3 8.43e7 M 1.51 -0.0240 1.35 1.46 0.527 1.08 ## 4 8.43e7 M -0.281 0.134 -0.250 -0.550 3.39 3.89 ## 5 8.44e7 M 1.30 -1.47 1.34 1.22 0.220 -0.313 ## 6 8.44e5 M -0.165 -0.314 -0.115 -0.244 2.05 1.72 ## # … with 4 more variables: Concavity &lt;dbl&gt;, Concave_points &lt;dbl&gt;, ## # Symmetry &lt;dbl&gt;, Fractal_dimension &lt;dbl&gt; 6.4.1.1 Variable descriptions Breast tumours can be diagnosed by performing a biopsy, a process where tissue is removed from the body to discover the presence of a disease. Traditionally these procedures were quite invasive, but now fine needle asipiration is a type of biopsy that uses a thin needle to examine a small amount of tissue from the tumour. With this method, 10 different variables are typically measured of cell nuclei from a digital image of a fine needle aspirate (FNA) of a breast mass. Source: https://www.semanticscholar.org/paper/Breast-Cancer-Diagnosis-and-Prognosis-Via-Linear-P-Mangasarian-Street/3721bb14b16e866115c906336e9d70db096c05b9/figure/0 A magnified image of a malignant breast Fine Needle Aspiration image. White lines denote the boundary of the cell nuclei. ID number Class - diagnosis (M = malignant, B = benign) radius (mean of distances from center to points on the perimeter) texture (standard deviation of gray-scale values) perimeter area smoothness (local variation in radius lengths) compactness (\\(perimeter^2 / area - 1.0\\)) concavity (severity of concave portions of the contour) concave points (number of concave portions of the contour) symmetry fractal dimension (\\(&quot;coastline\\: approximation&quot; - 1\\)) The “worst” (mean of the three largest values) values of these variable were computed for each image. As part of the data preparation, the data have been scaled (we will discuss what this means and why we do it in the next chapter). Below we use glimpse to preview the data frame. This function is similar to head, but can be easier to read when we have a lot of columns: glimpse(cancer) ## Observations: 569 ## Variables: 12 ## $ ID &lt;dbl&gt; 842302, 842517, 84300903, 84348301, 84358402, … ## $ Class &lt;chr&gt; &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;… ## $ Radius &lt;dbl&gt; 1.8850310, 1.8043398, 1.5105411, -0.2812170, 1… ## $ Texture &lt;dbl&gt; -1.35809849, -0.36887865, -0.02395331, 0.13386… ## $ Perimeter &lt;dbl&gt; 2.30157548, 1.53377643, 1.34629062, -0.2497195… ## $ Area &lt;dbl&gt; 1.999478159, 1.888827020, 1.455004298, -0.5495… ## $ Smoothness &lt;dbl&gt; 1.306536657, -0.375281748, 0.526943750, 3.3912… ## $ Compactness &lt;dbl&gt; 2.61436466, -0.43006581, 1.08198014, 3.8899746… ## $ Concavity &lt;dbl&gt; 2.10767182, -0.14661996, 0.85422232, 1.9878391… ## $ Concave_points &lt;dbl&gt; 2.29405760, 1.08612862, 1.95328166, 2.17387323… ## $ Symmetry &lt;dbl&gt; 2.7482041, -0.2436753, 1.1512420, 6.0407261, -… ## $ Fractal_dimension &lt;dbl&gt; 1.93531174, 0.28094279, 0.20121416, 4.93067187… We can see from the summary of the data above that Class is of type character. We are going to be working with Class as a categorical statistical variable so we will convert it to factor using the function as.factor. cancer &lt;- cancer %&gt;% mutate(Class = as.factor(Class)) Factors have what are called “levels”, which you can think of as categories. We can ask for the levels from the Class column by using the levels function. This function should return the name of each category in that column. Given that we only have 2 different values in our Class column, “B” and “M”, we only expect to get two names back. If we had 4 difference values in the column, we would expect to get 4 back. Note the use of unlist to between select and levels. This is because select outputs a data frame (even though we only select a single column), and levels expects a vector. cancer %&gt;% select(Class) %&gt;% unlist() %&gt;% # turns a data frame into a vector levels() ## [1] &quot;B&quot; &quot;M&quot; Before we start doing any modelling, Let’s explore out dataset. Below we use the tidyverse’s group_by + summarize function to see that we have 357 (63%) benign and 212 (37%) malignant tumour observations. num_obs &lt;- nrow(cancer) cancer %&gt;% group_by(Class) %&gt;% summarize(n = n(), percentage = n() / num_obs * 100) ## # A tibble: 2 x 3 ## Class n percentage ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 B 357 62.7 ## 2 M 212 37.3 Next, let’s draw a scatter plot to visualize the relationship between the perimeter and concavity variables. To avoid ggplot's default pallete, we define our own here and specify to use it in the scale_color_manual function. In that function we also make the category labels of “B” and “M” something more readable, “Benign” and “Malignant”, respectively. # colour palette cbPalette &lt;- c(&quot;#56B4E9&quot;, &quot;#E69F00&quot;,&quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#999999&quot;) perim_concav &lt;- cancer %&gt;% ggplot(aes(x = Perimeter, y = Concavity, color = Class)) + geom_point(alpha = 0.5) + labs(color = &quot;Diagnosis&quot;) + scale_color_manual(labels = c(&quot;Benign&quot;, &quot;Malignant&quot;), values = cbPalette) perim_concav In this visualization, we can see that the observations that are labelled as benign, typically fall in the the lower, left-hand side of the plot area. Whereas, the observations that are labelled as malignant typically fall in upper right-hand side of the plot. Suppose we have a new observation that is not in the current data set that we plotted and we do not have a Physician’s diagnosis for the tumour class. But what is we knew this new observation had a perimeter value of 1 and concavity value of 1. Could we use this information to classify that observation as benign or malignant? What about a new observation with perimeter value of -1 and concavity value of -0.5? What about 0 and 1? It seems like we can do this, at least visually. Now we will explore how we can use the K-Nearest Neighbour classification method to do this using R. 6.4.2 K-Nearest Neighbour Classifier To classify a new observation as benign or malignant, we find some observations in the training set that are “nearest” to our new observation, and then use their diagnoses (benign or malignant) to make a prediction for the new observation’s diagnosis. Let’s walk through an example; suppose we have a new observation, with perimeter of 2 and concavity of 4 (labelled in red on the scatterplot), whose diagnosis “Class” is unknown. We see that the nearest point to this new observation is located at the coordinates (2.3, 3.2). The idea here is that if a point is close to one another in the scatterplot then the perimeter and concavity values are similar so we may expect that they would have the same diagnosis. Suppose we have another new observation with perimeter 0.38 and concavity of 1.8. Looking at the scatterplot below, how would you classify this red observation? The nearest neighbour to this new point is a benign observation at (0.2, 1.8). Does this seem like the right prediction to make? Probably not if you consider the other nearby points… So instead of just using the one nearest neighbour, we can consider several neighbouring points, say \\(k = 3\\), that are closest to the new red observation to predict its diagnosis class. Among those 3 closest points, we look at their class and use the majority class as our prediction for the new observation. We see that the diagnoses of 2 of the 3 nearest neighbours to our new observation are malignant so we take majority vote and classify our new red observation as malignant. Here we chose the \\(k=3\\) nearest observations, but there is nothing special about \\(k=3\\). We could have used \\(k=4, 5\\) or more, though we may want to choose an odd number to avoid ties. We will discuss more about choosing \\(k\\) in the next section. 6.4.2.1 Distance Between Points When There are two explanatory variables/predictors How do we decide which points are “nearest” to our new observation? We can compute the distance between any pair of points using the following formula: \\[Distance = \\sqrt{(x_a -x_b)^2 + (y_a - y_b)^2}\\] Suppose we want to classify a new observation with perimeter of -1 and concavity of 4.2. Let’s calculate the distances between our new point and each of the observations in the training set to find the \\(k=5\\) observations in the training data that are nearest to our new point. new_obs_Perimeter &lt;- -1 new_obs_Concavity &lt;- 4.2 cancer %&gt;% select(ID, Perimeter, Concavity, Class) %&gt;% mutate(dist_from_new = sqrt((Perimeter - new_obs_Perimeter)^2 + (Concavity - new_obs_Concavity)^2)) %&gt;% arrange(dist_from_new) %&gt;% head(n = 5) ## # A tibble: 5 x 5 ## ID Perimeter Concavity Class dist_from_new ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 859471 -1.24 4.70 B 0.553 ## 2 84501001 -0.286 3.99 M 0.744 ## 3 8710441 -1.08 2.63 B 1.57 ## 4 9013838 -0.461 2.72 M 1.57 ## 5 925622 0.638 4.30 M 1.64 From this, we see that 3 of the 5 nearest neighbours to our new observation are malignant so classify our new observation as malignant. We circle those 5 in the plot below: It can be difficult sometimes to read code as math, so here we mathematically show the calculation of distance for each of the 5 closest points. ID Perimeter Concavity Distance Class 8.5947110^{5} -1.24 4.7 \\(\\sqrt{-1 - (-1.24))^2 + (4.2 - 4.7)^2}=\\) 0.55 B 8.450100110^{7} -0.29 3.99 \\(\\sqrt{(-1 - (-0.29))^2 + (4.2 - 3.99)^2} =\\) 0.74 M 8.71044110^{6} -1.08 2.63 \\(\\sqrt{(-1 - (-1.08))^2 + (4.2 - 2.63)^2} =\\) 1.57 B 9.01383810^{6} -0.46 2.72 \\(\\sqrt{(-1 - (-0.46))^2 + (4.2 - 2.72)^2} =\\) 1.57 M 9.2562210^{5} 0.64 4.3 \\(\\sqrt{(-1 - 0.64)^2 + (4.2 - 4.3)^2} =\\) 1.64 M 6.4.2.1.1 Summary: In order to classify a new observation using a k-nearest neighbor classifier, we have to do the follow steps: Step 1: Compute the distance between the new observation and each observation in our training set. Step 2: Sort the data table in ascending order according to the distances. Step 3: Choose the top \\(k\\) rows of the sorted table. Step 4: Classify the new observation based on majority vote. 6.4.3 K-Nearest Neighbours in R We will use the k-nearest neighbour (k-nn) algorithm in R by making use of the caret (Classification And REgression Training) package. caret contains a set of tools to help the process of making predictive models. Why do we now switch the using caret to perform k-nn as opposed to just writing the code to do it ourselves as we did above? Well, first, our code would have to get a bit more complicated to predict the classes for multiple new observations. Second, our code would also have to get a bit more complicated as we add more variables to our model. Thus for those two reasons, it makes sense to use the caret package to keep our code simple, readable and accurate (the less we type, the less mistakes we are likely to make). We start off by loading the caret library: library(caret) Let’s again suppose we have a new observation with perimeter -1 and concavity 4.2, but its diagnosis is unknown (as in our example above). Suppose we again want to use the perimeter and concavity explanatory variables/predictors to predict the diagnosis class of this observation. Let’s pick out our 2 desired variables and store it as a new dataset named cancer_train cancer_train &lt;- cancer %&gt;% select(&quot;Perimeter&quot;, &quot;Concavity&quot;) head(cancer_train) ## # A tibble: 6 x 2 ## Perimeter Concavity ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2.30 2.11 ## 2 1.53 -0.147 ## 3 1.35 0.854 ## 4 -0.250 1.99 ## 5 1.34 0.613 ## 6 -0.115 1.26 Next, we store the diagnosis class labels (column Class) as a vector. cancer_labels &lt;- cancer %&gt;% select(Class) %&gt;% unlist() head(cancer_labels) ## Class1 Class2 Class3 Class4 Class5 Class6 ## M M M M M M ## Levels: B M We will use the function train(), where the argument x is a data frame object containing the explanatory variables/predictors, and y is a numeric or factor vector containing the outcomes/labels/classes. x and y should come from your original data frame and be in the same order. The argument tuneGrid should be a dataframe with possible “tuning values”. For now, just know that this is where we will specify our \\(k\\) (the number of nearest neighbours) and we will use \\(k =5\\) (we will discuss how to choose \\(k\\) in a later section). We will use “knn” as our method. Note - the caret package expects data.frames and not tibbles (which are special kind data frames). This is a bit annoying, and I expect this to change in the future, but for now we have to change tibbles to data.frames when using caret. k &lt;- data.frame(k = 5) model_knn &lt;- train(x = data.frame(cancer_train), y = cancer_labels, method=&#39;knn&#39;, tuneGrid = k) Now we can create a data.frame with our new observation and predict the label of the new observation using the predict function: new_obs &lt;- data.frame(Perimeter = -1, Concavity = 4.2) predict(object = model_knn, new_obs) ## [1] M ## Levels: B M Our model classifies this new observation as malignant. How do we know how well our model did? In later sections, we will discuss ways to evaluate our model. 6.4.4 More than two explanatory variables/predictors So far we have seen how to build a classifier based on only explanatory variables/predictors, but we can use the k-nearest neighbours classifier in higher dimensional space. Let’s make a scatterplot with 3 variables instead of 2: Normally we recommend against 3D plots, but here for learning purposes we want to illustrate what happens when we go to higher dimensions. Each explanatory variable/predictor can give us new information to help create our classifier. The distance formula for 3-dimensions is \\[Distance = \\sqrt{(x_a -x_b)^2 + (y_a - y_b)^2 + (z_a - z_b)^2}\\] We can generalize for n-dimensions by summing up the squares of the differences between each individual coordinate taking the square root of the sum 6.5 Additional readings/resources The caret Package "],
["classification-continued.html", "Chapter 7 Classification continued 7.1 Overview 7.2 Learning objectives 7.3 Assessing how good your classifier is 7.4 Cross-validation for assessing classifier quality 7.5 Choosing the number of neighbours for k-nn classification 7.6 Other ways to increase accuracy 7.7 Test data set 7.8 Scaling your data 7.9 Strengths and limitations of k-nn classification 7.10 Additional readings/resources", " Chapter 7 Classification continued 7.1 Overview Metrics for classification accuracy; cross-validation to choose the number of neighbours; scaling of variables and other practical considerations. 7.2 Learning objectives By the end of the chapter, students will be able to: Describe what a validation data set is and how it is used in classification. Using R, evaluate classification accuracy using a validation data set and appropriate metrics. Using R, execute cross-validation in R to choose the number of neighbours. Identify when it is necessary to scale variables before classification and do this using R In a dataset with &gt; 2 attributes, perform k-nearest neighbour classification in R using caret::train(method = &quot;knn&quot;, ...) to predict the class of a test dataset. Describe advantages and disadvantages of the k-nearest neighbour classification algorithm. 7.3 Assessing how good your classifier is Sometimes our classifier might make the wrong prediction. A classifier does not need to be right 100% of the time to be useful, though we don’t want the classifier to make too many wrong predictions. How do we measure how “good” our classifier is? One way to assess our classifier’s performance can be done by splitting our data into a training set and a validation set. When we split the data, we make the assumption that there is no order to our originally collected data set. However, if we think that there might be some order to the original data set, then we can randomly shuffle the data before splitting it into a training and validation set. The training set is used to build the classifer. Then we can give the observations from the validation set (without the labels/classes) to our classifier and predict the labels/classes as if these were new observations that we didn’t have the labels/classes for. Then we can see how well our predictions match the true labels/classes for the observations in the validation set. If our predictions match the true labels/classes for the observations in the validation set very well then we have some confidence that our classifier might also do a good job of predicting the class labels for new observations that we do not have the class labels for. How exactly can we assess how well our predictions match the true labels/classes for the observations in the validation set? One way we can do this is to calculate the prediction accuracy. This is essentially the proportion of time the classifier was correct. To calculate this we divide the number of correct predictions by the number of predictions made. Other measures for how well our classifier did include precision and recall (which will not be discussed here, but are discussed in other more advanced courses on this topic). We try to illustrate this below: 7.3.1 Assessing your classifier in R We can use the caret package in R to not only perform k-nn classification, but also to assess how well our classification worked. Let’s start by loading the necessary libraries, data (we’ll continue exploring the breast cancer data set from last chapter) and making a quick scatter plot of tumour cell concavity versus smoothness, labelling the points be diagnosis class. # load libraries library(tidyverse) library(caret) #load data cancer &lt;- read_csv(&quot;data/clean-wdbc.data.csv&quot;) %&gt;% mutate(Class = as.factor(Class)) # because we will be doing statistical analysis on a categorical variable # colour palette cbPalette &lt;- c(&quot;#56B4E9&quot;, &quot;#E69F00&quot;,&quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#999999&quot;) # create scatter plot of tumour cell concavity versus smoothness, # labelling the points be diagnosis class perim_concav &lt;- cancer %&gt;% ggplot(aes(x = Smoothness, y = Concavity, color = Class)) + geom_point(alpha = 0.5) + labs(color = &quot;Diagnosis&quot;) + scale_color_manual(labels = c(&quot;Benign&quot;, &quot;Malignant&quot;), values = cbPalette) perim_concav 7.3.1.1 Splitting into training and validation sets Next, lets split our data into a training and a validation set using caret’s createDataPartition function. When using this function to split a data set into a training and validation set it takes 3 arguments: y (the class labels, must be a vector), p (the proportion of the data you would like in the training data set), and list = FALSE (says we want the data back as a matrix instead of a list). The createDataPartition function returns the row numbers for the training set. set.seed(1234) # makes the random selection of rows reproducible set_rows &lt;- cancer %&gt;% select(Class) %&gt;% unlist() %&gt;% # converts Class from a tibble to a vector createDataPartition(p = 0.75, list = FALSE) head(set_rows) ## Resample1 ## [1,] 1 ## [2,] 2 ## [3,] 3 ## [4,] 4 ## [5,] 5 ## [6,] 6 You will also see in the code above that we use the set.seed function. This is because createDataPartition uses random sampling to choose which rows will be in the training set, and if we use set.seed to specify where the random number generator starts for this process then we can make our analysis reproducible (always get the same random set of observations in the training set). We should always set a seed before any function that uses a random process. We’ll point out where as we work through this code. Now that we have the row numbers for the training set, we can use the slice function to get the rows from the original data set (here cancer) to create the training set and the validation sets. training_set &lt;- cancer %&gt;% slice(set_rows) validation_set &lt;- cancer %&gt;% slice(-set_rows) glimpse(training_set) ## Observations: 427 ## Variables: 12 ## $ ID &lt;dbl&gt; 842302, 842517, 84300903, 84348301, 84358402, … ## $ Class &lt;fct&gt; M, M, M, M, M, M, M, M, M, M, M, M, B, B, B, M… ## $ Radius &lt;dbl&gt; 1.8850310, 1.8043398, 1.5105411, -0.2812170, 1… ## $ Texture &lt;dbl&gt; -1.35809849, -0.36887865, -0.02395331, 0.13386… ## $ Perimeter &lt;dbl&gt; 2.30157548, 1.53377643, 1.34629062, -0.2497195… ## $ Area &lt;dbl&gt; 1.99947816, 1.88882702, 1.45500430, -0.5495376… ## $ Smoothness &lt;dbl&gt; 1.306536657, -0.375281748, 0.526943750, 3.3912… ## $ Compactness &lt;dbl&gt; 2.61436466, -0.43006581, 1.08198014, 3.8899746… ## $ Concavity &lt;dbl&gt; 2.10767182, -0.14661996, 0.85422232, 1.9878391… ## $ Concave_points &lt;dbl&gt; 2.2940576, 1.0861286, 1.9532817, 2.1738732, 0.… ## $ Symmetry &lt;dbl&gt; 2.7482041, -0.2436753, 1.1512420, 6.0407261, -… ## $ Fractal_dimension &lt;dbl&gt; 1.93531174, 0.28094279, 0.20121416, 4.93067187… glimpse(validation_set) ## Observations: 142 ## Variables: 12 ## $ ID &lt;dbl&gt; 84458202, 84501001, 84610002, 846226, 846381, … ## $ Class &lt;fct&gt; M, M, M, M, M, M, M, M, M, M, M, M, M, M, B, B… ## $ Radius &lt;dbl&gt; 0.16361901, -0.24397494, 0.85880462, 0.9705308… ## $ Texture &lt;dbl&gt; 0.400695342, 2.440961268, 0.260772799, 0.69355… ## $ Perimeter &lt;dbl&gt; 0.099361153, -0.286026354, 0.870136167, 1.3224… ## $ Area &lt;dbl&gt; 0.028834057, -0.297147713, 0.734893708, 0.7928… ## $ Smoothness &lt;dbl&gt; 1.4466882, 2.3182555, 0.3167164, -1.2556086, -… ## $ Compactness &lt;dbl&gt; 0.7241483, 5.1083824, 1.9489119, 0.8646116, -0… ## $ Concavity &lt;dbl&gt; -0.02103534, 3.99192038, 0.59586313, 0.4396013… ## $ Concave_points &lt;dbl&gt; 0.62364699, 1.61859101, 1.01006256, 0.94464575… ## $ Symmetry &lt;dbl&gt; 0.4772206, 2.3683599, 1.4405702, 0.4448934, -0… ## $ Fractal_dimension &lt;dbl&gt; 1.72491676, 6.84083682, 1.15463563, 1.01621788… We can see from glimpse in the code above that the training set contains 427 observations, while the validation set contains 142 observations. This corresponds to the training set having 75% of the observations from the original data set and the validation set having the other 25% of the observations. We specified this when we provided the argument p = 0.75 to createDataPartition. 7.3.1.2 Creating the k-nn classifier Now that we have split our original data set into a training and validation set, we can create our k-nn classifier using the training set. We explained how to do this last chapter, so here we just do it! For the time being we will just choose a single \\(k\\) of 3, and use the predictors concavity and smoothness. X_train &lt;- training_set %&gt;% select(Concavity, Smoothness) %&gt;% data.frame() Y_train &lt;- training_set %&gt;% select(Class) %&gt;% unlist() k = data.frame(k = 3) set.seed(1234) model_knn &lt;- train(x = X_train, y = Y_train, method = &quot;knn&quot;, tuneGrid = k) model_knn ## k-Nearest Neighbors ## ## 427 samples ## 2 predictor ## 2 classes: &#39;B&#39;, &#39;M&#39; ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 427, 427, 427, 427, 427, 427, ... ## Resampling results: ## ## Accuracy Kappa ## 0.8399266 0.6575533 ## ## Tuning parameter &#39;k&#39; was held constant at a value of 3 We set the seed above calling the train function because if there were a tie, the winning class is randomly chosen. In k-nn classification where \\(k\\) = 3 and there are only 2 classes this will never happen, however, if our \\(k\\) was divisible by 2, or if we had more than two possible class labels we could find ourselves in a situtation where we do have a tie. 7.3.1.3 Predict class labels for the validation set Now that we have a k-nn classifier object, we can use it to predict the class labels for our validation set: X_validation &lt;- validation_set %&gt;% select(Concavity, Smoothness) %&gt;% data.frame() Y_validation_predicted &lt;- predict(object = model_knn, X_validation) head(Y_validation_predicted) ## [1] B M M B B M ## Levels: B M 7.3.1.4 Assessing our classifier’s accuracy Finally we can assess our classifier’s accuracy. To do this we need to create a vector containing the class labels for the validation set. Next we use the function confusionMatrix to get the statistics about the quality of our model, this includes the statistic we are interested: accuracy. confusionMatrix takes two arguments: data (the predicted class labels for the validation set), and reference (the original/measured class labels for the validation set). Y_validation &lt;- validation_set %&gt;% select(Class) %&gt;% unlist() model_quality &lt;- confusionMatrix(data = Y_validation_predicted, reference = Y_validation) model_quality ## Confusion Matrix and Statistics ## ## Reference ## Prediction B M ## B 75 15 ## M 14 38 ## ## Accuracy : 0.7958 ## 95% CI : (0.72, 0.8588) ## No Information Rate : 0.6268 ## P-Value [Acc &gt; NIR] : 1.068e-05 ## ## Kappa : 0.5618 ## ## Mcnemar&#39;s Test P-Value : 1 ## ## Sensitivity : 0.8427 ## Specificity : 0.7170 ## Pos Pred Value : 0.8333 ## Neg Pred Value : 0.7308 ## Prevalence : 0.6268 ## Detection Rate : 0.5282 ## Detection Prevalence : 0.6338 ## Balanced Accuracy : 0.7798 ## ## &#39;Positive&#39; Class : B ## A lot of information is output from confusionMatrix, but what we are interested in at this point is accuracy (found on the 6th line of printed output). That single value can be obtained from the confusionMatrix object using base/built-in R subsetting: model_quality$overall[1] ## Accuracy ## 0.7957746 From a value of accuracy of 0.7957746, we can say that our k-nn classifier predicted the correct class label ~ 80% of the time. 7.4 Cross-validation for assessing classifier quality Is that the best estimate of accuracy that we can get? What would happen if we again shuffled the observations in our training and validation sets, would we get the same accuracy? Let’s do and experiment and see. By changing the set.seed value, we can get a different shuffle of the data when we create our training and validation data sets. Using set.seed(4321) set.seed(4321) # makes the random selection of rows reproducible set_rows &lt;- cancer %&gt;% select(Class) %&gt;% unlist() %&gt;% # converts Class from a tibble to a vector createDataPartition(p = 0.75, list = FALSE) training_set &lt;- cancer %&gt;% slice(set_rows) validation_set &lt;- cancer %&gt;% slice(-set_rows) X_train &lt;- training_set %&gt;% select(Concavity, Smoothness) %&gt;% data.frame() Y_train &lt;- training_set %&gt;% select(Class) %&gt;% unlist() k = data.frame(k = 3) model_knn &lt;- train(x = X_train, y = Y_train, method = &quot;knn&quot;, tuneGrid = k) X_validation &lt;- validation_set %&gt;% select(Concavity, Smoothness) %&gt;% data.frame() Y_validation_predicted &lt;- predict(object = model_knn, X_validation) Y_validation &lt;- validation_set %&gt;% select(Class) %&gt;% unlist() model_quality &lt;- confusionMatrix(data = Y_validation_predicted, reference = Y_validation) model_quality$overall[1] ## Accuracy ## 0.8661972 Using set.seed(8765) set.seed(8765) # makes the random selection of rows reproducible set_rows &lt;- cancer %&gt;% select(Class) %&gt;% unlist() %&gt;% # converts Class from a tibble to a vector createDataPartition(p = 0.75, list = FALSE) training_set &lt;- cancer %&gt;% slice(set_rows) validation_set &lt;- cancer %&gt;% slice(-set_rows) X_train &lt;- training_set %&gt;% select(Concavity, Smoothness) %&gt;% data.frame() Y_train &lt;- training_set %&gt;% select(Class) %&gt;% unlist() k = data.frame(k = 3) model_knn &lt;- train(x = X_train, y = Y_train, method = &quot;knn&quot;, tuneGrid = k) X_validation &lt;- validation_set %&gt;% select(Concavity, Smoothness) %&gt;% data.frame() Y_validation_predicted &lt;- predict(object = model_knn, X_validation) Y_validation &lt;- validation_set %&gt;% select(Class) %&gt;% unlist() model_quality &lt;- confusionMatrix(data = Y_validation_predicted, reference = Y_validation) model_quality$overall[1] ## Accuracy ## 0.7676056 When we have 3 different shuffles of the data, we get 3 different values for accuracy! 0.7957746, 0.8661972 and 0.7676056! Which one is correct? Sadly, there is no easy answer to that question. The best we can do is to do this many times and take the average of the accuracies. Typically this is done is a more structured way so that each observation in the data set is used in a validation set only a single time. The name for this strategy is called cross-validation and we illustrate it below: In the picture above, 5 different folds/partitions of the data set are shown, and consequently we call this 5-fold cross-validation. To do 5-fold cross-validation in R with caret, we use another function called trainControl. This function passes additional information to the train function we use to create our classifier. The arguments we pass trainControl are: method (method used for assessing classifier quality, here we specify &quot;cv&quot; for cross-validation) number (how many folds/partitions of the data set we want to use for cross validation) train_control &lt;- trainControl(method=&quot;cv&quot;, number = 5) Then when we create our classifier we add an additional argument to train, called trControl where we give it the name of the object we created with the trainControl function. Additionally, we do not need to specify a training and a validation set because we are telling train that we are doing cross validation (it will take care creating the folds, calculating the accuracy for each fold and averaging the accuracies for us). X_cancer &lt;- cancer %&gt;% select(Concavity, Smoothness) %&gt;% data.frame() Y_cancer &lt;- cancer %&gt;% select(Class) %&gt;% unlist() k = data.frame(k = 3) set.seed(1234) knn_model_cv_5fold &lt;- train(x = X_cancer, y = Y_cancer, method = &quot;knn&quot;, tuneGrid = k, trControl = train_control) knn_model_cv_5fold ## k-Nearest Neighbors ## ## 569 samples ## 2 predictor ## 2 classes: &#39;B&#39;, &#39;M&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 456, 455, 454, 455, 456 ## Resampling results: ## ## Accuracy Kappa ## 0.8329714 0.6439188 ## ## Tuning parameter &#39;k&#39; was held constant at a value of 3 This time we set the seed when we call train not only because of the potential for ties, but also because we are doing cross-validation. Cross-validation uses a random process to select which observations are included in which folds. We can choose any number of folds, typically the more the better. However we are limited by computational power. The more folds we choose, the more computation it takes, and hence the more time it takes to run the analysis. So for each time you do cross-validation you need to consider the size of the data, and the speed of the algorithm (here k-nn) and the speed of your computer. In practice this is a trial and error process. Here we show what happens when we do 10 folds: train_control &lt;- trainControl(method=&quot;cv&quot;, number = 10) set.seed(1234) knn_model_cv_10fold &lt;- train(x = X_cancer, y = Y_cancer, method = &quot;knn&quot;, tuneGrid = k, trControl = train_control) knn_model_cv_10fold ## k-Nearest Neighbors ## ## 569 samples ## 2 predictor ## 2 classes: &#39;B&#39;, &#39;M&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 513, 512, 513, 512, 511, 512, ... ## Resampling results: ## ## Accuracy Kappa ## 0.8400203 0.6608629 ## ## Tuning parameter &#39;k&#39; was held constant at a value of 3 7.5 Choosing the number of neighbours for k-nn classification From 5- and 10-fold cross-validate we estimate that the prediction accuracy of our classifier to be ~ 83%. This could be not too bad of an accuracy, however what accuracy you aim for always depends on the downstream application of your analysis. Here, we are trying to predict a very important outcome, tumour cell diagnosis class. And the class label we assign to a real patient may have life or death consequences. Hence, we’d like to do better for this application than 83%. To do this we can use cross-validation in an even bigger way, we can choose a range of possible \\(k\\)’s and perform cross-validation to calculate the accuracy for each \\(k\\), and then choose the smallest \\(k\\) which gives us the best cross-validation accuracy. To do this, we will create a vector of values for \\(k\\) instead of providing just 1. train_control &lt;- trainControl(method=&quot;cv&quot;, number = 10) k = data.frame(k = c(1, 3, 5, 7, 9, 11, 13, 15, 17)) set.seed(1234) knn_model_cv_10fold &lt;- train(x = X_cancer, y = Y_cancer, method = &quot;knn&quot;, tuneGrid = k, trControl = train_control) knn_model_cv_10fold ## k-Nearest Neighbors ## ## 569 samples ## 2 predictor ## 2 classes: &#39;B&#39;, &#39;M&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 513, 512, 513, 512, 511, 512, ... ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 1 0.8155475 0.6064615 ## 3 0.8400203 0.6608629 ## 5 0.8416515 0.6626622 ## 7 0.8434059 0.6650644 ## 9 0.8506406 0.6800368 ## 11 0.8612296 0.7025280 ## 13 0.8525484 0.6852467 ## 15 0.8576582 0.6942935 ## 17 0.8576582 0.6938875 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 11. Then to help us choose \\(k\\) it is very useful to visualize the accuracies as we increase \\(k\\). This will help us choose the smallest \\(k\\) with the biggest accuracy. We can access the results from the cross-validation by accessing theresults attribute of the train object (our classifier). accuracies &lt;- knn_model_cv_10fold$results accuracies ## k Accuracy Kappa AccuracySD KappaSD ## 1 1 0.8155475 0.6064615 0.05316182 0.11272822 ## 2 3 0.8400203 0.6608629 0.02453021 0.05161054 ## 3 5 0.8416515 0.6626622 0.03184238 0.07022825 ## 4 7 0.8434059 0.6650644 0.03710158 0.08316742 ## 5 9 0.8506406 0.6800368 0.03879376 0.08643835 ## 6 11 0.8612296 0.7025280 0.04205264 0.09318005 ## 7 13 0.8525484 0.6852467 0.03735006 0.08372607 ## 8 15 0.8576582 0.6942935 0.03449374 0.07757659 ## 9 17 0.8576582 0.6938875 0.03245033 0.07574435 Now we can plot accuracy versus k: accuracy_vs_k &lt;- ggplot(accuracies, aes(x = k, y = Accuracy)) + geom_point() + geom_line() accuracy_vs_k Based off of the visualization above we typically would choose \\(k\\) to be ~ 11, given that at this value of \\(k\\) our accuracy is a high as it can be with much larger values of \\(k\\). As you can see there is no exact or perfect answer here, what we are looking for is a value for \\(k\\) where we get a roughly optimal increase of accuracy but at the same time we want to keep \\(k\\) small. Why do we want to keep \\(k\\) small? Well this is because if we keep increasing \\(k\\) our accuracy actually starts to decrease! Take a look as the plot below as we vary \\(k\\) from 1 to almost the number of observations in the data set: train_control &lt;- trainControl(method=&quot;cv&quot;, number = 10) k_lots = data.frame(k = seq(from = 1, to = 499, by = 10)) set.seed(1234) knn_model_cv_10fold_lots &lt;- train(x = X_cancer, y = Y_cancer, method = &quot;knn&quot;, tuneGrid = k_lots, trControl = train_control) accuracies_lots &lt;- knn_model_cv_10fold_lots$results accuracy_vs_k_lots &lt;- ggplot(accuracies_lots, aes(x = k, y = Accuracy)) + geom_point() + geom_line() accuracy_vs_k_lots 7.6 Other ways to increase accuracy By using cross-validation to choose \\(k\\) we were able to slightly increase our accuracy, but can we still do better? Perhaps. We can start to explore this by taking a look at what is called the training accuracy. Training accuracy is our accuracy if we asked our classifier to make predictions on the training data and then we assessed how well the predictions matched up to the true labels we have for our training data. If they don’t match up really well (training accuracy is low), our classification model might be too simple and adding more information (e.g., additional predictors/explanatory variables) could potentially help. The situation where the training accuracy is low is often called underfitting, or high bias. The training error can be obtained from using the classifier object returned from train (when you don’t perform cross-validation) to predict on the training data. Then passing the predictions on the training data and the true observed labels into confusionMatrix. k = data.frame(k = 11) set.seed(1234) knn_model &lt;- train(x = X_cancer, y = Y_cancer, method = &quot;knn&quot;, tuneGrid = k) training_pred &lt;- predict(knn_model, X_cancer) results &lt;- confusionMatrix(training_pred, Y_cancer) results ## Confusion Matrix and Statistics ## ## Reference ## Prediction B M ## B 322 36 ## M 35 176 ## ## Accuracy : 0.8752 ## 95% CI : (0.8452, 0.9012) ## No Information Rate : 0.6274 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.7329 ## ## Mcnemar&#39;s Test P-Value : 1 ## ## Sensitivity : 0.9020 ## Specificity : 0.8302 ## Pos Pred Value : 0.8994 ## Neg Pred Value : 0.8341 ## Prevalence : 0.6274 ## Detection Rate : 0.5659 ## Detection Prevalence : 0.6292 ## Balanced Accuracy : 0.8661 ## ## &#39;Positive&#39; Class : B ## From the complex output, we can see the training accuracy, here 0.8752. Again we can use base/built-in subsetting syntax to directly get the value: results$overall[1] ## Accuracy ## 0.8752197 Here we see that our training accuracy is high, 0.8752197, but there is still room for improvement! (If it were 1.0 there wouldn’t be and we would have a different problem). So let’s see if adding additional information (predictors/explanatory variables) might help our model training and consequently (and more importantly) validation accuracy. Note - when adding more information to the model, \\(k\\) = 11 may no longer be the “best” \\(k\\). So we will want to also choose \\(k\\) again. # set-up training data X_cancer_all &lt;- cancer %&gt;% select(-Class, -ID) %&gt;% data.frame() Y_cancer_all &lt;- cancer %&gt;% select(Class) %&gt;% unlist() # set-up classifier specifications train_control &lt;- trainControl(method=&quot;cv&quot;, number = 10) k = data.frame(k = seq(from = 1, to = 29, by = 2)) # create classifier set.seed(1234) knn_model_all &lt;- train(x = X_cancer_all, y = Y_cancer_all, method = &quot;knn&quot;, tuneGrid = k, trControl = train_control) # assess training accuracy training_pred_all &lt;- predict(knn_model_all, X_cancer_all) results_all &lt;- confusionMatrix(training_pred_all, Y_cancer_all) results_all$overall[1] ## Accuracy ## 0.9718805 We can see that by including more information (making our classifier more complex by adding additional predictors/explanatory variables) we increased the training accuracy. What about the cross-validation accuracy? And what \\(k\\) should we choose? accuracies_all &lt;- knn_model_all accuracy_vs_k_all &lt;- ggplot(accuracies_all, aes(x = k, y = Accuracy)) + geom_point() + geom_line() accuracy_vs_k_all From the plot above, it seems as though now with more information in our model, we should choose a \\(k\\) of 7. Additionally, with this extra information our validation accuracy has also increased with \\(k = 7\\). 7.7 Test data set In addition to a training and validation sets, in practice we really split our data set into 3 different sets: training validation testing What is the testing set and what purpose does this third set serve? Typically create the testing set at the very beginning of our analysis, leave it a the locked box so that it plays no role while we a fiddling with things (usually through cross validation) like choosing \\(k\\), or the number of predictors/explanatory variables to put in the classifier. After we have settled on all the settings (e.g., \\(k\\) and number of predictors) for our classifier and we have no plans to EVER change them again we re-train the classifier on the entire training set (i.e., don’t do cross validation) with those settings and then predict on testing set observations. We then take those predictions and compare them to the true labels of the test set and come up with a test accuracy measure. This typically looks something like this: source: https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6 Why do we have this super special test set? This is so we do not violate the golden rule of statistical/machine learning: YOU CANNOT USE THE TEST DATA TO BUILD THE MODEL!!! If you do, this is analagous to a student cheating on a midterm. 7.8 Scaling your data For a knn classifier, the scale of the variables matter. Since the knn classifier predicts classes by identifying observations that are nearest to it, any variables that are on a large scale will have a much larger effect than variables on a small scale. (Even though they might not actually be more important!) For example, suppose your dataset has two attributes: salary (in dollars) and years of education. For the knn classifier, a difference of $1000 is huge compared to a difference of 10 years of education. For our conceptual understanding and answering of the problem, its the opposite! 10 years of education is huge compared to a difference of $1000 in yearly salary! So to prevent an overpowering influence of salary on the distance function we need to standardize our predictors/explanatory variables so that our variables will be on a comparable scale. We can do this with the scale() function in R. To illustrate the difference between scaled and non-scaled data, and the effect it can have on k-nn, we will read in the original, un-scaled Wisconsin breast cancer data set (we have been using a scaled version of the data set in the textbook and worksheets up until now). Here we get the data from the UCI Machine learning repository, and then get the equivalent columns that we have been working with in the scaled version of the data set (“worst” measures). We also give the data column names (the data set comes with none). unscaled_cancer &lt;- read_csv(&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data&quot;, col_names = FALSE) %&gt;% select(X1, X2, X11:X20) ## Parsed with column specification: ## cols( ## .default = col_double(), ## X2 = col_character() ## ) ## See spec(...) for full column specifications. colnames(unscaled_cancer) &lt;- colnames(cancer) head(unscaled_cancer) ## # A tibble: 6 x 12 ## ID Class Radius Texture Perimeter Area Smoothness Compactness ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 8.42e5 M 0.242 0.0787 1.10 0.905 8.59 153. ## 2 8.43e5 M 0.181 0.0567 0.544 0.734 3.40 74.1 ## 3 8.43e7 M 0.207 0.0600 0.746 0.787 4.58 94.0 ## 4 8.43e7 M 0.260 0.0974 0.496 1.16 3.44 27.2 ## 5 8.44e7 M 0.181 0.0588 0.757 0.781 5.44 94.4 ## 6 8.44e5 M 0.209 0.0761 0.334 0.890 2.22 27.2 ## # … with 4 more variables: Concavity &lt;dbl&gt;, Concave_points &lt;dbl&gt;, ## # Symmetry &lt;dbl&gt;, Fractal_dimension &lt;dbl&gt; Looking at the unscaled data above, you can see that the difference between the values for smoothness measurements are much larger than those for area. Let’s make a scatter plot for these two predictors below (colouring by diagnosis) and then overlay that with a new observation we would like to classify as either benign or malignant with a red dot. We’ll do this for the unscaled and scaled data sets, and put them side-by-side: In the plot with the unscaled data above, its very clear that k-nn would classify the red dot (new observation) as malignant. However, once we scale the data, the diagnosis class labelling becomes less clear and appears it would depend upon the choice of \\(k\\). Hopefully this graphic drives home the message that scaling the data changes things in an important way when we are using algorithms like k-nn that use distance between points as a part of their decision making process. How do we scale data in R? The code below demonstrates how we can use the scale function in R to take the unscaled_cancer data and scale it: scaled_cancer &lt;- unscaled_cancer %&gt;% select(-c(ID,Class)) %&gt;% scale(center = FALSE) scaled_cancer &lt;- data.frame(ID = unscaled_cancer$ID, Class = unscaled_cancer$Class, scaled_cancer) head(scaled_cancer) ## ID Class Area Smoothness ## 1 842302 M 0.6731696 2.4446014 ## 2 842517 M 0.5457187 0.9671389 ## 3 84300903 M 0.5851288 1.3049828 ## 4 84348301 M 0.8595869 0.9805160 ## 5 84358402 M 0.5809648 1.5477637 ## 6 843786 M 0.6619414 0.6310026 Scaling your data should be a part of the pre-processing you do before you start doing an analysis where distance between points plays a role. And as we learned in the last two chapters, k-nn classification is one of these. What about centering the data? I’ve heard that is also a data pre-processing step? Yes, it can be helpful but it depends on the algorithm. For k-nn classification centering doesn’t help, nor does it hurt. So you can do it if you would like but you do not have to. 7.9 Strengths and limitations of k-nn classification 7.9.1 Strengths of k-nn classification Simple and easy to understand No assumptions about what the data must look like Works easily for binary (two-class) and multi-class (&gt; 2 classes) classification problems 7.9.2 Limitations of k-nn classification As data gets bigger and bigger, k-nn gets slower and slower, quite quickly Does not perform well with a large number of predictors Does not perform well when classes are imbalanced (when many more observations are in one of the classes compared to the others) 7.10 Additional readings/resources The caret Package "],
["regression1.html", "Chapter 8 Introduction to regression through K-nearest neighbours 8.1 Overview 8.2 Learning objectives 8.3 Regression 8.4 Sacremento real estate example 8.5 K-nearest neighbours regression 8.6 Assessing a knn regression model 8.7 How do different k’s affect k-nn regression predictions 8.8 Assessing model goodness with the test set 8.9 Strengths and limitations of k-nn regression", " Chapter 8 Introduction to regression through K-nearest neighbours 8.1 Overview Introduction to regression using K-nearest neighbours (k-nn). We will focus on prediction in cases where there is a response variable of interest and a single explanatory variable. 8.2 Learning objectives By the end of the chapter, students will be able to: Recognize situations where a simple regression analysis would be appropriate for making predictions. Explain the k-nearest neighbour (k-nn) regression algorithm and describe how it differs from k-nn classification. Interpret the output of a k-nn regression. In a dataset with two variables, perform k-nearest neighbour regression in R using caret::train() to predict the values for a test dataset. Using R, execute cross-validation in R to choose the number of neighbours. Using R, evaluate k-nn regression prediction accuracy using a test data set and an appropriate metric (e.g., root means square prediction error). Describe advantages and disadvantages of the k-nearest neighbour regression approach. 8.3 Regression We can use regression as a method to answer a very similar question to classification (can we use past information to predict future observations?), but in the case of regression the goal is to predict numerical values instead of class labels. An example regression prediction question would be: can we use hours spent on exercise each week to predict marathon race time? And another example regression prediction question is: can we use house size (livable square feet) to predict house sale price? We will use regression to explore this question in the rest of this chapter, using a real estate data set from Sacremento, California that is available in the caret package. Note: in addition to prediction, regression can also be used to model the relationship between two or more variables, but here we will focus only on prediction. 8.4 Sacremento real estate example Let’s start by loading the libraries we need and previewing the data set. The data set comes with the caret package, so as soon as we load the caret library and type data(Sacramento) we are able to access it as a data frame named Sacramento. library(tidyverse) library(scales) library(caret) library(gridExtra) data(Sacramento) head(Sacramento) ## city zip beds baths sqft type price latitude longitude ## 1 SACRAMENTO z95838 2 1 836 Residential 59222 38.63191 -121.4349 ## 2 SACRAMENTO z95823 3 1 1167 Residential 68212 38.47890 -121.4310 ## 3 SACRAMENTO z95815 2 1 796 Residential 68880 38.61830 -121.4438 ## 4 SACRAMENTO z95815 2 1 852 Residential 69307 38.61684 -121.4391 ## 5 SACRAMENTO z95824 2 1 797 Residential 81900 38.51947 -121.4358 ## 6 SACRAMENTO z95841 3 1 1122 Condo 89921 38.66260 -121.3278 We are interested in the columns sqft (which is the house size in livable square feet) and price, which is the house size in US dollars (USD). Let’s now visualize the data as a scatter plot where we place the predictor/explanatory variable, house size, on the x-axis and the target/response variable, price, on the y-axis (this is what we would like to predict): eda &lt;- ggplot(Sacramento, aes(x = sqft, y = price)) + geom_point(alpha = 0.4) + xlab(&quot;House size (square footage)&quot;) + ylab(&quot;Price (USD)&quot;) + scale_y_continuous(labels = dollar_format()) eda From looking at the visualization above, we see that as house size (square footage) increases, so does house price. Thus, we can reason that house size might be a useful predictor of house price and perhaps we can use the size of the house to predict the price a house will be sold at (for a home that has not yet sold and thus consequently we do not know the house price). 8.5 K-nearest neighbours regression Let’s take a small sample of the data above and walk through how K-nearest neighbours (knn) regression works before we dive in to creating our model and assessing how well it predicts house price. This subsample is taken only to be able to illustrate the mechanics of k-nn regression with a few data points, later in this chapter we will use all the data. To take a small random sample of size 30 , we’ll use the function sample_n. This function takes two arguments: tbl (a data frame-like object to sample from) size (the number of observations/rows to be randomly selected/sampled) set.seed(2019) small_sacramento &lt;- sample_n(Sacramento, size = 30) Next let’s say we come across a new house we are interested in purchasing, and it is 2000 square feet! Its advertised list price is $350,000 should we give them what they are asking? Or is that overpriced and we should offer less? Perhaps we cannot directly answer that, but we can get close by using the data we have to predict the sale price given the sale prices we have already observed. Given the data in the plot below, we have no observations of a house that has sold that is 2000 square feet, so how can we predict the price? small_plot &lt;- ggplot(small_sacramento, aes(x = sqft, y = price)) + geom_point() + xlab(&quot;House size (square footage)&quot;) + ylab(&quot;Price (USD)&quot;) + scale_y_continuous(labels=dollar_format()) + geom_vline(xintercept = 2000, linetype = &quot;dotted&quot;) small_plot What we can do is use the neighbouring points to suggest/predict what the price should be. For the example above, below we find and label the 5 nearest neighbours to our observation of a house that is 2000 square feet: small_sacramento %&gt;% mutate(diff = abs(2000 - sqft)) %&gt;% arrange(diff) %&gt;% head(5) ## city zip beds baths sqft type price latitude longitude ## 1 SACRAMENTO z95835 4 2 1910 Residential 300500 38.67745 -121.4948 ## 2 SACRAMENTO z95825 3 2 1720 Residential 290000 38.56786 -121.4107 ## 3 ANTELOPE z95843 3 2 1669 Residential 168750 38.71846 -121.3709 ## 4 LINCOLN z95648 3 2 1650 Residential 276500 38.84412 -121.2748 ## 5 SACRAMENTO z95827 5 3 2367 Residential 315537 38.55599 -121.3404 ## diff ## 1 90 ## 2 280 ## 3 331 ## 4 350 ## 5 367 small_plot + geom_segment(aes(x = 2000, y = 300500, xend = 1910, yend = 300500), col = &quot;orange&quot;) + geom_segment(aes(x = 2000, y = 290000, xend = 1720, yend = 290000), col = &quot;orange&quot;) + geom_segment(aes(x = 2000, y = 168750, xend = 1669, yend = 168750), col = &quot;orange&quot;) + geom_segment(aes(x = 2000, y = 276500, xend = 1650, yend = 276500), col = &quot;orange&quot;) + geom_segment(aes(x = 2000, y = 315537, xend = 2367, yend = 315537), col = &quot;orange&quot;) Now that we have the 5 nearest neighbours to our new observation that we would like to predict the price for, we can use their values to predict a selling price for the home we are interested in buying that is 2000 square feet. Specifically, we can take the mean (or average) of these 5 values as our predicted value. prediction &lt;- small_sacramento %&gt;% mutate(diff = abs(2000 - sqft)) %&gt;% arrange(diff) %&gt;% head(5) %&gt;% summarise(predicted = mean(price)) prediction ## predicted ## 1 270257.4 Our predicted price is $270,257.40 (shown as a red point above), which is much less than $350,000, and so perhaps we might want to offer less than the list price that the house is advertised at. Simple right? Not quite. We have all the same unanswered questions here with k-nn regression that we had with k-nn classification. Which \\(k\\) do we choose? And, is our model any good at making predictions? We’ll shortly address how to answer these questions in the context of k-nn regression. 8.6 Assessing a knn regression model As usual, we should start by putting some test data away in a lock box that we can come back to after we choose our final model, so let’s take care of that business now. For the remainder of the chapter we’ll work with the full data set. set.seed(2019) # makes the random selection of rows reproducible training_rows &lt;- Sacramento %&gt;% select(price) %&gt;% unlist() %&gt;% # converts Class from a tibble to a vector createDataPartition(p = 0.6, list = FALSE) X_train &lt;- Sacramento %&gt;% select(sqft) %&gt;% slice(training_rows) %&gt;% data.frame() Y_train &lt;- Sacramento %&gt;% select(price) %&gt;% slice(training_rows) %&gt;% unlist() X_test &lt;- Sacramento %&gt;% select(sqft) %&gt;% slice(-training_rows) %&gt;% data.frame() Y_test &lt;- Sacramento %&gt;% select(price) %&gt;% slice(-training_rows) %&gt;% unlist() Next, we’ll use cross-validation to choose \\(k\\). In k-nn classification, we used accuracy to see how well our predictions matched the true labels. Here in the context of k-nn regression we will use root mean square error (\\(RMSE\\)) instead. If the predictions are very close to the true values, then \\(RMSE\\) will be small. If, on the other-hand, the predictions are very different to the true values, then \\(RMSE\\) will be quite large. Thus, when we are doing cross validation to choose \\(k\\), we want to choose the \\(k\\) that gives us the smallest \\(RMSE\\). The mathematical formula for calculating \\(RMSE\\) is shown below: \\[RMSE = \\sqrt{\\frac{1}{n}\\sum\\limits_{i=1}^{n}(y_i - \\hat{y_i})^2}\\] Where: \\(n\\) is the number of observations \\(y_i\\) is the observed value for the \\(ith\\) observation \\(\\hat{y_i}\\) is the forcasted/predicted value for the \\(ith\\) observation A key feature the formula for RMSE is the distance between the observed target/response variable value, \\(y\\), and the prediction target/response variable value, \\(\\hat{y_i}\\), for each observation (from 1 to \\(i\\)). Now that we know how we can assess how well our model predicts a numerical value, let’s use R to perform cross-validation and to choose the optimal \\(k\\). train_control &lt;- trainControl(method = &quot;cv&quot;, number = 10) # makes a column of k&#39;s, from 1 to 500 in increments of 5 k_lots = data.frame(k = seq(from = 1, to = 500, by = 5)) set.seed(1234) knn_reg_cv_10 &lt;- train(x = X_train, y = Y_train, method = &quot;knn&quot;, tuneGrid = k_lots, trControl = train_control) ggplot(knn_reg_cv_10$results, aes(x = k, y = RMSE)) + geom_point() + geom_line() knn_reg_cv_10$method ## [1] &quot;knn&quot; Here we see that the smallest \\(RMSE\\) is from the model where \\(k\\) = 51. Thus the best \\(k\\) for this model is 51. 8.7 How do different k’s affect k-nn regression predictions Below we plot the predicted values for house price from our k-nn regression models for 6 different values for \\(k\\) where the only predictor is home size. For each model, we predict a price for every possible home size across the range of home sizes we observed in the data set (here 500 to 4250 square feet) and we plot the predicted prices as a blue line: From the plots above, we see that when \\(k\\) = 1, the blue line runs perfectly through almost all of our training observations. This happens because our predicted values for a given region, depend on just a single observation. A model like this has high variance and low bias. It has high variance because the flexible blue line follows the training observations very closely, and if we were to change any one of the training observation data points we would change the flexible blue line quite a lot. This means that the blue line matches the data we happen to have in this training data set, however, if we were to collect another training data set from the Sacramento real estate market it likely wouldn’t match those observations as well. However, it has low bias because the model/predicted values matches the actual observed values in this training data set very well. Another term that we use to collectively describe this phenomenon is overfitting. What about the plot where \\(k\\) is quite large, say \\(k\\) = 450, for example? When \\(k\\) = 450 for this data set, the blue line is extremely smooth, and almost flat. This happens because our predicted values for a given x value (here home size), depend on many many neighbouring observations, 450 to be exact! A model like this has low variance and high bias. It has low variance because the smooth, inflexible blue line does not follow the training observations very closely, and if we were to change any one of the training observation data points it really wouldn’t affect the shape of the smooth blue line at all. This means that although the blue line matches does not match the data we happen to have in this particular training data set perfectly, if we were to collect another training data set from the Sacramento real estate market it likely would match those observations equally as well as it matches those in this training data set. This model also has high bias because the model/predicted values does not match the actual observed values very well. Another term that we use to collectively describe this kind of model is underfitting. Ideally, what we want is neither of the two examples discussed above. Instead, we would like a model with low variance (so that it will transer/generalize well to other data sets, meaning that it isn’t too dependent on the observations that happen to be in the training set we had) and low bias (one where the model/predicted values matches the actual observed values very well). If we explore the other values for \\(k\\), in particular \\(k\\) = 51 (the optimal \\(k\\) as suggested by cross-validation), we can see it has a lower bias than our model with a very high \\(k\\) (e.g., 450), and thus the model/predicted values better match the actual observed values than the high \\(k\\) model. Additionally, it has lower variance than our model with a very low \\(k\\) (e.g., 1) and thus it should better transer/generalize to other data sets compared to the low \\(k\\) model. Finally, how the choice of \\(k\\) affects k-nn regression also carries over to how the choice of \\(k\\) affects k-nn classification. It is simply easier to show this through pictures for a k-nn regression problem which is why we waited until now to discuss this. 8.8 Assessing model goodness with the test set Next we re-train our k-nn model on the entire training data set (do not perform cross validation) and then predict on the test data set to assess how well our model does. In the case of k-nn regression we use the function defaultSummary instead of confusionMatrix (which we used with knn classification). This is because our predictions are not class labels, but values, and as such the type of model goodness score is calculated differently. defaultSummary expects a data frame where one column is the observed target/response variable values from the test data, and a second column of the predicted values for the test data. k = data.frame(k = 51) set.seed(1234) knn_reg_final &lt;- train(x = X_train, y = Y_train, method = &quot;knn&quot;, tuneGrid = k) test_pred &lt;- predict(knn_reg_final, X_test) modelvalues &lt;- data.frame(obs = Y_test, pred = test_pred) test_results &lt;- defaultSummary(modelvalues) test_results ## RMSE Rsquared MAE ## 9.162040e+04 5.785061e-01 6.370486e+04 Our final model’s test error as assessed by \\(RMSE\\) is 91620.4. But what does this \\(RMSE\\) score mean? When we calculated test set prediction accuracy when we performed k-nn classification the highest possible value was 1, and if we got an value close to that it was easy to assess how well our model did on at least one new data set that had never been used to choose our model (so we didn’t violate the golden rule of statistical/machine learning). So what about \\(RMSE\\), what is it out of? Unfortunately there is no scale for \\(RMSE\\) (instead it is measured in the units of the target/response variable), and so it is a bit hard to interpret. For now, let’s consider this approach to thinking about \\(RMSE\\) from our testing data set: as long as its not WAY worse than the cross-validation \\(RMSPE\\) of our best model then we can say that we’re not doing too much worse on the test data than we did on the training data, and so it appears to be generalizing OK to a new data set it has never seen before. In future courses on statistical/machine learning we will learn more about how to interpret \\(RMSE\\) from our testing data set and other ways to assess our model. And what does our final model look like when we predict across all possible house sizes we might encounter in the Sacramento area? We plotted it above where we explored how \\(k\\) affects k-nn regression, but we show it again now, along with the code for how we generated it: set.seed(1234) predictions_all &lt;- data.frame(sqft = seq(from = 500, to = 4250, by = 1)) predictions_all$price &lt;- predict(knn_reg_final, data.frame(sqft = seq(from = 500, to = 4250, by = 1))) train_data &lt;- bind_cols(X_train, data.frame(price = Y_train)) #combines X_train and Y_train to be on data set plot_final &lt;- ggplot(train_data, aes(x = sqft, y = price)) + geom_point(alpha = 0.4) + xlab(&quot;House size (square footage)&quot;) + ylab(&quot;Price (USD)&quot;) + scale_y_continuous(labels = dollar_format()) + geom_line(data = predictions_all, aes(x = sqft, y = price), color = &quot;blue&quot;) + ggtitle(&quot;k = 51&quot;) plot_final 8.9 Strengths and limitations of k-nn regression As with k-nn classification (or any prediction algorithm for that manner), k-nn regression has both strengths and weaknesses. Some are listed here: 8.9.1 Strengths of k-nn regression Simple and easy to understand No assumptions about what the data must look like Works well with non-linear relationships (i.e., if the relationship is not a straight line) 8.9.2 Limitations of k-nn regression As data gets bigger and bigger, k-nn gets slower and slower, quite quickly Does not perform well with a large number of predictors unless the size of the training set is exponentially larger Does not predict well beyond the range of values input in your training data "],
["regression2.html", "Chapter 9 Regression, continued 9.1 Overview 9.2 Learning objectives 9.3 \\(RMSE\\) versus \\(RMPSE\\) 9.4 Linear regression 9.5 Linear regression in R using caret 9.6 Comparing linear and k-nn regression 9.7 Additional readings/resources", " Chapter 9 Regression, continued 9.1 Overview Introduction to linear regression models. We will also begin to compare k-nn to linear models in the context of regression. 9.2 Learning objectives By the end of the chapter, students will be able to: In the context of k-nn regression, compare and contrast goodness of fit and prediction properties (namely RMSE vs RMSPE). In a dataset with 2 variables, perform simple ordinary least squares regression in R using caret’s train with method = &quot;lm&quot; to predict the values for a test dataset. Compare and contrast predictions obtained from k-nearest neighbour regression to those obtained using simple ordinary least squares regression from the same dataset. In R, overlay the ordinary least squares regression lines from geom_smooth on a single plot. 9.3 \\(RMSE\\) versus \\(RMPSE\\) The error output we have been getting from caret to assess how well our k-nn regression models predict is labelled as RMSE for root mean squared error. The equation for calculating \\(RMSE\\) is shown in the previous chapter. So far, we have called this error \\(RMSE\\) in this textbook as well, however, in certain contexts the correct term becomes \\(RMPSE\\), which stands for root mean squared prediction error. The same formula is used to calculate \\(RMPSE\\) and \\(RMSE\\), however these separate terms exist to specify what the error is being calculated on. \\(RMPSE\\) is specifically referring to the error when predicting on future data (e.g., the validation set(s), or the testing set), not the training set. Whereas, \\(RMSE\\) is specifically referring to the error when predicting on the training data set and is an attempt to get at model goodness of fit on the data used to fit the model. From this point on in the course, we will be using the terms \\(RMPSE\\) and \\(RMSE\\) in their appropriate contexts. 9.4 Linear regression k-nn is not the only type of regression, there are many, and one common and quite useful type of regression is called linear regression. Linear regression is similar to k-nn regression in that the target/response variable is expected to be quantitative, however, one way it varies quite differently is how the training data is used to predict a value for a new observation. Instead of looking at the \\(k\\)-nearest neighbours and averaging over their values for a prediction, in linear regression all the training data points are used to create a straight line of “best fit”, and then the line is used to “look-up” the predicted value. For example, let’s revisit the smaller version of the Sacramento housing data set and the prediction case where we come across a new house we are interested in purchasing, and it is 2000 square feet! Its advertised list price is $350,000 should we give them what they are asking? Or is that overpriced and we should offer less? To answer this question using linear regression, we use the data we have to draw the straight line of “best fit” through our existing data points: Then we can use this line to “look up” the predicted price given the value we have for the predictor/explanatory variable (here 2000 square feet). ## [1] 287178.8 Using linear regression on this small data set to predict the sale price for a 2000 square foot house we get a predicted value of $287178.80 USD. But wait a minute… How exactly does linear regression choose the line of “best fit”? Many different lines could be drawn through the data points, we show some examples below: Linear regression chooses the straight line of “best fit” by choosing the line that minimzes the average vertical distance between itself and each of the observed data points. From the lines shown above, that is the blue line. What exactly do we mean by the vertical distance between the predicted values (which fall along the line of “best fit”) and the observed data points? We illustrate these distances in the plot below with a red line: How do we assess the predictive accuracy of a linear regression model? We use the same measure of predictive performance we used with k-nn regression, \\(RMPSE\\) (note the use of \\(RMPSE\\) versus \\(RMSE\\) here as discussed in the first section of this chapter). 9.5 Linear regression in R using caret We can perform linear regression in R using the caret package in a very similar manner to how we performed k-nn regression, using the train function. To do this, instead of setting method = &quot;knn&quot; we instead set method = &quot;lm&quot;. Another difference is that we do not need to choose \\(k\\) in the context of linear regression and so we do not need to perform cross validation. Below we illustrate how we can use the caret package to predict house sale price given house size using a linear regression approach using the full Sacramento real estate data set: As usual, we should start by putting some test data away in a lock box that we can come back to after we choose our final model, so let’s take care of that business now. set.seed(2019) # makes the random selection of rows reproducible training_rows &lt;- Sacramento %&gt;% select(price) %&gt;% unlist() %&gt;% # converts Class from a tibble to a vector createDataPartition(p = 0.6, list = FALSE) X_train &lt;- Sacramento %&gt;% select(sqft) %&gt;% slice(training_rows) %&gt;% data.frame() Y_train &lt;- Sacramento %&gt;% select(price) %&gt;% slice(training_rows) %&gt;% unlist() X_test &lt;- Sacramento %&gt;% select(sqft) %&gt;% slice(-training_rows) %&gt;% data.frame() Y_test &lt;- Sacramento %&gt;% select(price) %&gt;% slice(-training_rows) %&gt;% unlist() Now that we have our training data, we fit our linear regression model: lm_model &lt;- train(x = X_train, y = Y_train, method = &quot;lm&quot;) And finally, we predict on the test data set to assess how well our model does: test_pred &lt;- predict(lm_model, X_test) lm_modelvalues &lt;- data.frame(obs = Y_test, pred = test_pred) lm_test_results &lt;- defaultSummary(lm_modelvalues) lm_test_results ## RMSE Rsquared MAE ## 8.668847e+04 6.152298e-01 6.232425e+04 Our final model’s test error as assessed by \\(RMSPE\\) is 86688.47. Remember that this is in units of the target/response variable, and here that is US Dollars (USD). Does this mean our model is “good” at predicting house sale price based off of the predictor of home size? Again answering this is tricky to answer and requires to use domain knowledge and think about the application you are using the prediction for. And what does our final linear regression model look like when we predict across all possible house sizes we might encounter in the Sacremento area? There is a plotting function in the tidyverse, geom_smooth, that allows us to do this easily by adding a layer on our plot with the linear regression predicted line of “best fit”. The default for this adds a plausible range to this line that we are not interested in at this point, so to avoid plotting it, we provide the argument se = FALSE in our call to geom_smooth. train_data &lt;- bind_cols(X_train, data.frame(price = Y_train)) lm_plot_final &lt;- ggplot(train_data, aes(x = sqft, y = price)) + geom_point(alpha = 0.4) + xlab(&quot;House size (square footage)&quot;) + ylab(&quot;Price (USD)&quot;) + scale_y_continuous(labels = dollar_format()) + geom_smooth(method = &quot;lm&quot;, se = FALSE) lm_plot_final 9.6 Comparing linear and k-nn regression Now that we have a general understanding of both linear and k-nn regression, we can start to compare and contrast these methods as well as the predictions made by them. To start, let’s look at the visualization of the linear regression model predictions for the Sacramento real estate data (predicting price from house size) and the “best” k-nn regression model obtained from the same problem: What differences do we observe from the visualization above? One obvious difference is the shape of the blue lines. In linear regression we are restricted to a straight line, whereas in k-nn regression our line is much more flexible and can be quite wiggly. There can be an advantage to limiting the model to a straight line, as linear regression does, in that a straight line model is quite interpretable and can be defined by two numbers, the y-intercept and the slope. The slope is particularly meaningful for interpretation, as it tells us what unit increase in the target/response variable we predict given a unit increase in the predictor/explanatory variable. Additionally, because our model is restricted to a straight line, we can even use the mathematical equation for a straight line as a basis to write a mathematical expression of our model. Remembering that the equation for a straight line is: \\[Y = \\beta_0 + \\beta_1X\\] Where: \\(\\beta_0\\) is the y-intercept of the line (the value where the line cuts the y-axis) \\(\\beta_1\\) is the slope of the line We can then write: \\[house\\: price = \\beta_0 + \\beta_1house\\: size\\] And finally, fill in the values for \\(\\beta_0\\) and \\(\\beta_1\\) from the straight line (we will show you how to get these from R next week) to get: \\[house\\: price = -64542.2 + 175.9*house\\: size\\] k-nn regression, as simple as it is to implement and understand, has no such interpretability from it’s wiggly line. There can however also be a disadvantage to using a linear regression model in some cases, particularly when the relationship between the target and the predictor is not linear, but instead some other shape, such as curved or circular. In these cases the prediction model from a linear regression will have high bias, meaning that model/predicted values does not match the actual observed values very well. Such a model would probably have a quite high \\(RMSE\\) when assessing model goodness of fit on the training data and a quite high \\(RMPSE\\) when assessing model prediction quality on a test data set. On such a data set, k-nn regression may fare better. Additionally, there are other types of regression you can learn about in future courses that may do even better at predicting with such data. How do these two models compare on this data set? On the visualizations above we also printed the \\(RMPSE\\) as calculated from predicting on the test data set that was not used to train/fit the models. The \\(RMPSE\\) for the linear regression model is less than the \\(RMPSE\\) for the k-nn regression model, and thus if were were comparing these in practice we would choose to use the linear regression model to make our predictions because of this (in addition to the fact the the linear regression model is more interpretable). 9.7 Additional readings/resources Pages 59-71 of Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani Pages 104 - 109 of An Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani The caret Package "],
["regression3.html", "Chapter 10 Regression, continued some more… 10.1 Overview 10.2 Learning objectives 10.3 Multivariate k-nn regression 10.4 Multivariate linear regression 10.5 The other side of regression 10.6 Additional readings/resources", " Chapter 10 Regression, continued some more… 10.1 Overview This week we will work through some examples of multiple regression in the prediction context. We will emphasize the interpretation and relevance of the mix of negative/positive slopes in this context. We will then discuss another application of regression; modelling the relationship between two or more variables so that we can better understand or describe it. We will emphasize that this is a jumping off point for the study of statistical inference. 10.2 Learning objectives By the end of the chapter, students will be able to: In a dataset with &gt; 2 variables, perform k-nn regression in R using caret’s train with method = &quot;k-nn&quot; to predict the values for a test dataset. In a dataset with &gt; 2 variables, perform simple ordinary least squares regression in R using caret’s train with method = &quot;lm&quot; to predict the values for a test dataset. 10.3 Multivariate k-nn regression As in k-nn classification, in k-nn regression we can have multiple predictors. When we have multiple predictors in k-nn regression, we have the same concern regarding the scale of the predictors. This is because as in k-nn classification and regression, predictions are made by identifying the \\(k\\) observations that are nearest to the new point we want to predict, and any variables that are on a large scale will have a much larger effect than variables on a small scale. Thus, once we start performing multivariate k-nn regression we need to use the scale function in R on our predictors to ensure this doesn’t happen. We will now demonstrate a multi-variate k-nn regression analysis usin the caret package on the Sacramento real estate data (also used in the the last two chapters in this book). This time we will use house size (measured in square feet) as well as number of bathrooms as our predictors, and continue to use house sale price as our outcome/target variable that we are trying to predict. Let’s first load the libraries and the data: library(tidyverse) library(caret) library(GGally) data(&quot;Sacramento&quot;) head(Sacramento) ## city zip beds baths sqft type price latitude longitude ## 1 SACRAMENTO z95838 2 1 836 Residential 59222 38.63191 -121.4349 ## 2 SACRAMENTO z95823 3 1 1167 Residential 68212 38.47890 -121.4310 ## 3 SACRAMENTO z95815 2 1 796 Residential 68880 38.61830 -121.4438 ## 4 SACRAMENTO z95815 2 1 852 Residential 69307 38.61684 -121.4391 ## 5 SACRAMENTO z95824 2 1 797 Residential 81900 38.51947 -121.4358 ## 6 SACRAMENTO z95841 3 1 1122 Condo 89921 38.66260 -121.3278 It is always a good practice to do exploratory data analysis, such as visualizing the data, before we start modeling the data. Thus the first thing we will do is use ggpairs (from the GGally package) to plot all the variables we are interested in using in our analyses: plot_pairs &lt;- Sacramento %&gt;% select(price, sqft, baths) %&gt;% ggpairs() plot_pairs From this we can see that generally, as both house size and number of bathrooms increase, so does price. Does adding the number of baths to our model improve our ability to predict house price? To answer that question, we will have to come up with the test error for a k-nn regression model using house size and number of baths, and then we can compare it to the test error for the model we previously came up with that only used house size to see if it is smaller (decreased test error indicates increased prediction quality). Let’s do that now! Looking at the data above, we can see that sqft and beds (number of bedrooms) are on vastly different scales. Thus we need to apply the scale function to these columns before we start our analysis: scaled_Sacramento &lt;- Sacramento %&gt;% select(price, sqft, baths) %&gt;% mutate(sqft = scale(sqft, center = FALSE), baths = scale(baths, center = FALSE)) head(scaled_Sacramento) ## price sqft baths ## 1 59222 0.4564854 0.459221 ## 2 68212 0.6372231 0.459221 ## 3 68880 0.4346440 0.459221 ## 4 69307 0.4652220 0.459221 ## 5 81900 0.4351901 0.459221 ## 6 89921 0.6126515 0.459221 Now we can split our data into a trained and test set as we did before: set.seed(2019) # makes the random selection of rows reproducible training_rows &lt;- scaled_Sacramento %&gt;% select(price) %&gt;% unlist() %&gt;% # converts Class from a tibble to a vector createDataPartition(p = 0.6, list = FALSE) X_train &lt;- scaled_Sacramento %&gt;% select(sqft, baths) %&gt;% slice(training_rows) %&gt;% data.frame() Y_train &lt;- scaled_Sacramento %&gt;% select(price) %&gt;% slice(training_rows) %&gt;% unlist() X_test &lt;- scaled_Sacramento %&gt;% select(sqft, baths) %&gt;% slice(-training_rows) %&gt;% data.frame() Y_test &lt;- scaled_Sacramento %&gt;% select(price) %&gt;% slice(-training_rows) %&gt;% unlist() Next, we’ll use 10-fold cross-validation to choose \\(k\\): train_control &lt;- trainControl(method = &quot;cv&quot;, number = 10) # makes a column of k&#39;s, from 1 to 500 in increments of 5 k_lots = data.frame(k = seq(from = 1, to = 500, by = 5)) set.seed(1234) knn_reg_cv_10 &lt;- train(x = X_train, y = Y_train, method = &quot;knn&quot;, tuneGrid = k_lots, trControl = train_control) ggplot(knn_reg_cv_10$results, aes(x = k, y = RMSE)) + geom_point() + geom_line() knn_reg_cv_10$method ## [1] &quot;knn&quot; Here we see that the smallest \\(RMSE\\) is from the model where \\(k\\) = 31. Thus the best \\(k\\) for this model, with two predictors, is 31. Now that we have chosen \\(k\\), we need to re-train the model on the entire training data set with \\(k\\) = 31, and after that we can use that model to predict on the test data to get our test error. At that point we will also visualize the model predictions overlaid on top of the data. This time the predictions will be a plane in 3-D space, instead of a line in 2-D space, as we have 2 predictors instead of 3. k = data.frame(k = 31) set.seed(1234) knn_mult_reg_final &lt;- train(x = X_train, y = Y_train, method = &quot;knn&quot;, tuneGrid = k) test_pred &lt;- predict(knn_mult_reg_final, X_test) modelvalues &lt;- data.frame(obs = Y_test, pred = test_pred) knn_mult_test_results &lt;- defaultSummary(modelvalues) knn_mult_test_results[[1]] ## [1] 90108.49 This time when we performed k-nn regression on the same data set, but also included number of bathrooms as a predictor we obtained a RMSPE test error of 90108.49. This compares to a RMSPE test error of 91620.40 when we used only house size as the single predictor. What do the predictions from this model look like overlaid on the data? We can see that the predictions in this case, where we have 2 predictors, form a plane instead of a line. Because the newly added predictor, number of bathrooms, is correlated with price (USD) (meaning as price changes, so does number of bathrooms) we get additional and useful information for making our predictions. For example, in this model we would predict that the cost of a house with a scaled house size of ~ 0.52 and a scaled number bathrooms of ~ 1.13 would cost less than the same sized house with a higher scaled number bathrooms (e.g., ~ 2.11). Without having the additional predictor of number of bathrooms, we would predict the same price for these two houses. 10.4 Multivariate linear regression We could also try to create a prediction model for the data above using linear regression instead of k-nn regression. To do this, we follow a very similar approach to what we did using caret for k-nn regression, however, we do not need to use cross-validation to choose \\(k\\). We also do not need to scale the data for linear regression as it does not use a distance between points calculation in its algorithm. We demonstrate how to do this below: Now we can split our data into a trained and test set as we did before: set.seed(2019) # makes the random selection of rows reproducible training_rows &lt;- Sacramento %&gt;% select(price) %&gt;% unlist() %&gt;% # converts Class from a tibble to a vector createDataPartition(p = 0.6, list = FALSE) lm_X_train &lt;- Sacramento %&gt;% select(sqft, baths) %&gt;% slice(training_rows) %&gt;% data.frame() lm_Y_train &lt;- Sacramento %&gt;% select(price) %&gt;% slice(training_rows) %&gt;% unlist() lm_X_test &lt;- Sacramento %&gt;% select(sqft, baths) %&gt;% slice(-training_rows) %&gt;% data.frame() lm_Y_test &lt;- Sacramento %&gt;% select(price) %&gt;% slice(-training_rows) %&gt;% unlist() Now we can fit the model on the training set using the method = &quot;lm&quot; argument in the train function: lm_mult_reg_final &lt;- train(x = lm_X_train, y = lm_Y_train, method = &quot;lm&quot;) What does our model predictions look like in the case of linear regression when we have two predictors? We illustrate this below: We see that the predictions from linear regression with two predictors form a plane as well, but this plane differs from the one we get from k-nn regression in its flexibility. The plane from k-nn regression above was more flexible and more closely followed the shape of the training data. The plane from linear regression however is constrained to being linear (i.e., a flat plane). As discussed this can be advantageous in one aspect, which is that for each predictor, we can get a slope from linear regression, and thus describe the plane mathematically. We can extract those slope values from our model object as shown below: lm_mult_reg_final$finalModel$coefficients ## (Intercept) sqft baths ## 25284.3344 140.2523 -7115.9565 And then use those slopes, to write a mathematical equation to describe the prediction plane: Remembering that the equation for a plane is: \\[Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2\\] Where: \\(\\beta_0\\) is the y-intercept of the line (the value where the line cuts the y-axis) \\(\\beta_1\\) is the slope for the first predictor \\(X_1\\) is the first predictor \\(\\beta_2\\) is the slope for the second predictor \\(X_2\\) is the second predictor We can then write: \\[house\\: price = \\beta_0 + \\beta_1 \\:house\\: size + \\beta_2 \\:number\\: of \\: bathrooms\\] And finally, fill in the values for \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\beta_2\\) from the model output above: \\[house\\: price = 25284.3344 + 140.2523*house\\: size -7115.9565 * \\:number\\: of \\: bathrooms\\] OK great, so this model is more interpretable than the multivariate k-nn regression model (i.e., we can write a mathematical equation that explains how each predictor is affecting the predictions), but as always, we should look at the test error and ask whether linear regression is doing a better job of predicting compared to k-nn regression in this multivariate regression case? To do that we can use this linear regression model to predict on the test data to get our test error. test_pred_lm &lt;- predict(lm_mult_reg_final, lm_X_test) lm_modelvalues &lt;- data.frame(obs = lm_Y_test, pred = test_pred_lm) lm_mult_test_results &lt;- defaultSummary(lm_modelvalues) lm_mult_test_results[[1]] ## [1] 86778.87 We get that the \\(RMSPE\\) for the multivariate linear regression model of 86778.87. This prediction error is less than the prediction error for the multivariate k-nn regression model, indicating that we should likely choose linear regression for predictions of house price on this data set. But wait, we should also ask if this more complex model is doing a better job of predicting compared to our simple linear regression model with only a single predictor (house size). Revisiting last chapter, we see that our \\(RMSPE\\) for our simple linear regression model with only a single predictor was 86688.47 which is less than that for our more complex model. Thus, again for interpretability sake as well as performance, we should in the end choose the simple linear regression model for this data set. Should we always end up choosing a simple linear regression as our model? No! And you never know what model will be the best until you go through this process. Exploratory data analysis can give you some hints, but until you look at the test error to compare the models you don’t really know. 10.5 The other side of regression So far in this textbook we have used regression only in the context of prediction, however, regression is also a powerful method to understand and/or describe the relationship between a quantitative outcome/response variable and one or more explanatory variables. Extending the case we have been working with in this chapter (where we are interested in house price as the outcome/response variable), we could also, or instead, be interested in describing the individual effects of house size and the number of bathrooms on house price, quantifying how big each of these effects are, and assessing how accurately we can estimate each of these effects. Using linear regression to answer such questions sits under the umbrella of statistical inference. Here we will only introduce this topic at a very high-level, but it is an important field that is widely used in Statistics and Data Science and is taught in many Statistical courses. In statistical inference we acknowledge that we can only measure some of all the possible observations that exist in the world for the thing we are interested in quantifying. And so what we do, is use the measurements that we do have to infer/make a guess about what the true quantity would be if we could measure every possible thing. One example of statistical inference would be to try to estimate/come up with a best guess to the proportion of undergraduates who have an iphone. We cannot possibly ask all existing undergraduates if they have an iphone, so what can we do? Well, what statistical inference suggests that instead of asking all undergraduates (which we cannot do), we instead ask a random (and representative) sample of undergraduates if they have an iphone. From this sample of data we calculate the proportion of undergraduates that have an iphone. We can then use this proportion as an estimate/best guess of the proportion of all undergradautes that have an iphone. We know the proportion we calculated is not exactly right, but we think it should be close and it’s the best we can do (since we cannot possibly ask all undergraduates and get the exact truth). Since we know that it’s not exaclty right, we also try to use the data we have collected to come up with a plausible range where we think the true proportion of all undergradautes that have an iphone lies. Now here in this chapter in the context of linear regression, we are not talking about estimating a proportion, instead we are instead usually interested in estimating the slope/coefficient of the relationship between the outcome/response variable and one or more predictor variables. Again, in this case, whatever we are measuring is usually just a sample of all the things we could measure. Here in this chapter it is a sample of observations measuring the sale and house characteristics of 932 (out of &gt; 180,000) homes in Sacramento. Fitting our linear model, we did come up with an estimate for the slope for the relationship between house sale price and home size as 175.9. We know this number is not the true slope for the relationship between these two variables, however this is the best guess/estimate we can come up with given the data that we have. The next step of regression in this statistical inference perspective would be to use the data we have come up with a plausible range for where we think the value for the true slope would lie. We will not go into the details of how to do this in this course, but this will be covered in following Statistics courses. However, if you are very interested we will say that boostrapping to generate confidence intervals would be one approach that would let you do this and we point you to the additional readings for this chapter to learn more if you are interested. 10.6 Additional readings/resources Chapters 6 - 11 of Modern Dive Statistical Inference via Data Science by Chester Ismay and Albert Y. Kim "],
["clustering.html", "Chapter 11 Clustering 11.1 Overview 11.2 Learning objectives 11.3 Clustering 11.4 K-means clustering algorithm 11.5 K-means clustering in R 11.6 Choosing K for K-means clustering 11.7 Additional readings:", " Chapter 11 Clustering 11.1 Overview Introduction to clustering using K-means. Will discuss the K-means algorithm, how we choose K (the number of clusters) and other practical considerations (such as scaling). 11.2 Learning objectives By the end of the chapter, students will be able to: Describe a case where clustering would be an appropriate tool, and what insight it would bring from the data. Explain the K-means clustering algorithm. Interpret the output of a K-means analysis. Perform kmeans clustering in R using kmeans. Visualize the output of K-means clustering in R using pair-wise scatter plots. Identify when it is necessary to scale variables before clustering and do this using R. Use the elbow method to choose the number of clusters for K-means. Describe advantages, limitations and assumptions of the K-means clustering algorithm. 11.3 Clustering While at first glance, clustering may seem very similar to classification, these two methods have some very important distinction. Most notably, classification is a supervised method (we use past information to predict the future values/labels for our target/response variable), whereas clustering is considered an unsupervised method (there is no target/response variable and we are looking to find sub-groups/clusters of observations based on how similar they are). So, where classification might be used to label future emails as spam or not spam, clustering might be instead be used to group emails into categories based on their similarity, however we would not have labels for these categories in the case of clustering. Another example problem we might try to solve with clustering is grouping Amazon customers into groups based upon their similar purchasing behaviours. Again here, we do not have, nor need, labels for customer groups. Another way to think about it is, that classification is really about predicting something that you might have a scientific question about and/or hypothesis for, whereas, clustering is very often a hypothesis generating process (you identify things that are similar to each other that might be unexpected, and from those observations, you might generate a question and hypothesis that you might follow-up with classification). Another major difference between clustering and classification is in how success is determined. With classification we are able to use a test data set to assess prediction performance, in clustering we must use variance metrics to determine how well our defined clusters fit the data. The two metrics used to determine success are between- and within- variation. Ideally we want clusters where the between-variance is large (so that the clusters are well separated) and the within- variation is small (so that the clusters are composed of close/tight-knit observations). 11.3.1 A toy example What if we had some customer data, and we wanted to learn more about the types of customers we had so that we could come up with better products and/or promotions to increase our business in a data-driven way. For example, let’s consider this data below, where we have assessed customer loyalty and customer satisfaction: data modified from: http://www.segmentationstudyguide.com/using-cluster-analysis-for-market-segmentation/ From this data we might ask whether there are sub-groups within our customers? For example do we have customers with high loyalty and high satisfaction? What about low satisfaction and high loyalty? One way to answer such a question is to apply K-means clustering analysis. When we do such an analysis on this data set we identify 3 customer subgroups within our data set: What are the labels for these groups? We don’t really have any, only cluster numbers are output from the clustering algorithm. In a simple case like this, where we can easily visualize the clusters on a scatter plot, we can give labels to these groups after clustering using the positions of the groups on the plot: low loyalty and low satisfaction (green cluster), high loyalty and low satisfaction (pink cluster), and high loyalty and high satisfaction (blue cluster). Once we have such data we can use it to inform our future business decisions, and/or ask questions like, why did we not observe customers who had high satisfaction but low loyalty? 11.4 K-means clustering algorithm Watch the video linked to below for an explanation of the K-means clustering algorithm: - https://www.coursera.org/lecture/machine-learning-data-analysis/what-is-a-k-means-cluster-analysis-p94tY note - when the advertisement pops up to register for this course, you can just click to ignore it (i.e., no need to sign up to watch the entire video) 11.5 K-means clustering in R Let’s take a look at the data we plotted above: head(marketing_data) ## # A tibble: 6 x 2 ## loyalty csat ## &lt;dbl&gt; &lt;dbl&gt; ## 1 7 1 ## 2 7.5 1 ## 3 8 2 ## 4 7 2 ## 5 3 2 ## 6 1 3 To peform Kmeans clustering in R, we use the kmeans function. It takes at least two arguments, the data frame containing the data you wish to cluster, and K, the number of clusters (here we choose K = 3). Given that the K-means algorithm uses a random start to begin the algorithm, to make this reproducible, we need to set the seed. set.seed(1234) marketing_clust &lt;- kmeans(marketing_data, centers = 3) marketing_clust ## K-means clustering with 3 clusters of sizes 5, 9, 5 ## ## Cluster means: ## loyalty csat ## 1 7.500000 1.800000 ## 2 6.777778 7.444444 ## 3 2.400000 3.200000 ## ## Clustering vector: ## [1] 1 1 1 1 3 3 1 3 3 2 2 2 2 2 2 2 2 2 3 ## ## Within cluster sum of squares by cluster: ## [1] 3.80000 27.77778 8.00000 ## (between_SS / total_SS = 83.6 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; ## [5] &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; ## [9] &quot;ifault&quot; As you can see above, the clustering object returned has a lot of information about our analysis that we need to explore. Let’s take a look at it now. To do this, we will call in help from the broom package so that we get the model output back in a tidy data format. Let’s first start by getting the cluster identification for each point and plotting that on the scatter plot. To do that we use the augment function. Augment takes in the model and the original data frame, and returns a data frame with the data and the cluster assignments for each point: library(broom) clustered_data &lt;- augment(marketing_clust, marketing_data) head(clustered_data) ## # A tibble: 6 x 3 ## loyalty csat .cluster ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 7 1 1 ## 2 7.5 1 1 ## 3 8 2 1 ## 4 7 2 1 ## 5 3 2 3 ## 6 1 3 3 Now that we have this data frame, we can easily plot the data (i.e., cluster assignments of each point): cluster_plot &lt;- ggplot(clustered_data, aes(x = csat, y = loyalty, colour = .cluster)) + geom_point() cluster_plot 11.6 Choosing K for K-means clustering As mentioned above, we need to choose a K to perform K-means clustering. How should we choose K? We have no data labels, and so cannot perform cross-validation with some measure of model prediction error, so what can we do? What we can do in this situation is to look at the total within-cluster sum of squares for different K’s and choose the K that gives the biggest decrease in the total within-cluster sum of squares. Why total within-cluster sum of squares? This statistic lets us know how close/tight-knit (or compact) observations are within clusters. A larger number means that clusters are not close/tight-knit, but are instead more spread out. A smaller number here means that clusters are indeed close/tight-knit together. We can get at the total within-cluster sum of squares (tot.withinss) from our clustering using broom’s glance function (it gives model-level statistics). For example: glance(marketing_clust) ## # A tibble: 1 x 4 ## totss tot.withinss betweenss iter ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 241. 39.6 202. 2 Let’s calculate the total within-cluster sum of squares for our data for a variety of K’s (say 1 - 9) and then plot them against K. To do this we will create a data frame with a column named k, for each of the K’s we want to try our clustering with. Then we use map to apply the kmeans function to each K. We also use map to then apply glance to each of the clustering models we performed (one for each K). In the end we end up with a complex data frame with 3 columns, one for K, one for the models, and one for the model statistics (output of glance, which is a data frame): marketing_clust_ks &lt;- tibble(k = 1:9) %&gt;% mutate(marketing_clusts = map(k, ~kmeans(marketing_data, .x)), glanced = map(marketing_clusts, glance)) head(marketing_clust_ks) ## # A tibble: 6 x 3 ## k marketing_clusts glanced ## &lt;int&gt; &lt;list&gt; &lt;list&gt; ## 1 1 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt; ## 2 2 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt; ## 3 3 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt; ## 4 4 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt; ## 5 5 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt; ## 6 6 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt; What we need to do next, is get the value for the total within-cluster sum of squares (tot.withinss) from the glanced column. Given that each item in this column is a data frame, we will need to use the unnest function to unpack the data frames in the glanced column. clustering_statistics &lt;- marketing_clust_ks %&gt;% unnest(glanced) head(clustering_statistics) ## # A tibble: 6 x 6 ## k marketing_clusts totss tot.withinss betweenss iter ## &lt;int&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 &lt;kmeans&gt; 241. 241. 0 1 ## 2 2 &lt;kmeans&gt; 241. 110. 132. 1 ## 3 3 &lt;kmeans&gt; 241. 39.6 202. 1 ## 4 4 &lt;kmeans&gt; 241. 24.0 217. 2 ## 5 5 &lt;kmeans&gt; 241. 18.3 223. 2 ## 6 6 &lt;kmeans&gt; 241. 32.2 209. 1 Now that we have tot.withinss and k as columns in a data frame, we can make a plot to choose K: elbow_plot &lt;- ggplot(clustering_statistics, aes(x = k, y = tot.withinss)) + geom_point() + geom_line() + xlab(&quot;K&quot;) + ylab(&quot;Total within-cluster sum of squares&quot;) elbow_plot We call the plot above an “elbow plot” and we look for the “elbow” in total within-cluster sum of squares, the point where afterwards increasing K doesn’t have as much impact reducing the total within-cluster sum of squares. Here we would choose K = 3. 11.7 Additional readings: Pages 385-390 and 404-405 of Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani and the companion video linked to below: "]
]
