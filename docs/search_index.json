[
["reading.html", "Chapter 2 Reading in data locally and from the web 2.1 Overview 2.2 Chapter learning objectives 2.3 Absolute and relative file paths 2.4 Reading tabular data from a plain text file into R 2.5 Reading data from an Microsoft Excel file 2.6 Reading data from a Database 2.7 Writing data from R to a .csv file 2.8 Scraping data off the web using R 2.9 Additional readings/resources", " Chapter 2 Reading in data locally and from the web 2.1 Overview Learn to read in spreadsheet-like data of various formats from your local device and from the web. Once read in, these data sets will be used to walk through a real world Data Science application that includes wrangling the data into a useable format and creating an effective data visualization. 2.2 Chapter learning objectives By the end of the chapter, students will be able to: define the following: absolute file path relative file path url read data into R using a relative path and a url compare and contrast the following functions: read_csv read_tsv read_csv2 read_delim read_excel match the following tidyverse read_* function arguments to their descriptions: file delim col_names skip choose the appropriate tidyverse read_* function and function arguments to load a given plain text tabular data set into R use readxl library’s read_excel function and arguments to load a sheet from an excel file into R connect to a database using the DBI library’s dbConnect function list the tables in a database using the RSQLite library’s dbListTables function create a table object using the tbl function and query the database using functions from the dbplyr library optional: scrape data from the web read/scrape data from an internet URL using the rvest html_nodes and html_text functions compare downloading tabular data from a plain text file (e.g. *.csv) from the web versus scraping data from a .html file 2.3 Absolute and relative file paths When you load virtually any data set into R you need to tell R where that files lives. The file could live on your computer, or somewhere on the internet. In this section we will discuss the case where the file lives on your computer. The place the file lives on your computer is called the “path”. You can think of the path as directions to the file. There are two kinds of paths, relative paths and absolute paths. A relative path is where the file is in respect to where you currently are on the computer (e.g., where the Jupyter notebook file you are working in is). Whereas an absolute path is where the file is in respect to the base or root folder of the computer’s filesystem. If our computer’s filesystem looked like the picture below, and we were working in the Jupyter notebook titled “worksheetk_02.ipynb” and we wanted to read in the .csv file named happiness_report.csv into our Jupyter notebook using R, we could do this using either a relative or an absolute path. We show what both would be below. 2.3.0.0.1 Reading happiness_report.csv using a relative path: happiness_data &lt;- read_csv(&quot;data/happiness_report.csv&quot;) 2.3.0.0.2 Reading happiness_report.csv using an absolute path: happiness_data &lt;- read_csv(&quot;/home/jupyter/dsci-100/worksheet_02/data/happiness_report.csv&quot;) So which one should you use? Well to ensure your code can be run on a different computer, you should choose to use the relative path (and it’s also less typing!). Why would the absolute path not work on a different computer? The reason for this is that the names and the folder structure of the path between a computer’s root folder (named /) and any files and folders you are usually working with will be different depending on who owns the computer (usually there is a different user name on each computer) and where the files and folders happen to be located on that particular computer. See this video for another explanation: Source: Udacity course “Linux Command Line Basics” 2.4 Reading tabular data from a plain text file into R Now we will learn more about reading tabular data from a plain text file into R, as well as how to write tabular data to a file. Last chapter we learned about using the tidyverse read_csv when the file we read it matches that functions expected defaults (column names are present, , is the delimiter/separator and there are no row names in the dataset ). We will now learn how to read files where that is not the case. Before we jump into the cases where the tidyverse read_csv functions expected defaults are not the case, let’s revisit how we use this with one that does and thus the only argument we need to give to the function is the path to the file, here “data/state_property_vote.csv”. Here is how the file would look in plain text editor: state,med_income,med_prop_val,population,mean_commute_minutes,party AK,64222,197300,733375,10.46830207,Republican AL,36924,94800,4830620,25.30990746,Republican AR,35833,83300,2958208,22.40108933,Republican AZ,44748,128700,6641928,20.58786,Republican CA,53075,252100,38421464,23.38085172,Democrat CO,48098,198900,5278906,19.50792188,Democrat CT,69228,246450,3593222,24.349675,Democrat DC,70848,475800,647484,28.2534,Democrat DE,54976,228500,926454,24.45553333,Democrat Using read_csv to load in R: library(tidyverse) us_data &lt;- read_csv(&quot;data/state_property_vote.csv&quot;) us_data ## # A tibble: 52 x 6 ## state med_income med_prop_val population mean_commute_minutes party ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AK 64222 197300 733375 10.5 Republican ## 2 AL 36924 94800 4830620 25.3 Republican ## 3 AR 35833 83300 2958208 22.4 Republican ## 4 AZ 44748 128700 6641928 20.6 Republican ## 5 CA 53075 252100 38421464 23.4 Democrat ## 6 CO 48098 198900 5278906 19.5 Democrat ## 7 CT 69228 246450 3593222 24.3 Democrat ## 8 DC 70848 475800 647484 28.3 Democrat ## 9 DE 54976 228500 926454 24.5 Democrat ## 10 FL 43355 125600 19645772 24.8 Republican ## # … with 42 more rows 2.4.1 Skipping rows when reading in data Often times information about how data was collected, or other relevant information, is included at the top of the data file. This information is usually written in sentence and paragraph form, with no delimiter because it is not organized into columns. An example of this is shown below: Data source: https://datausa.io/ Record of how data was collected: https://github.com/UBC-DSCI/introduction-to-datascience/blob/master/data/src/retrieve_data_usa.ipynb Date collected: 2017-06-06 state,med_income,med_prop_val,population,mean_commute_minutes,party AK,64222,197300,733375,10.46830207,Republican AL,36924,94800,4830620,25.30990746,Republican AR,35833,83300,2958208,22.40108933,Republican AZ,44748,128700,6641928,20.58786,Republican CA,53075,252100,38421464,23.38085172,Democrat CO,48098,198900,5278906,19.50792188,Democrat CT,69228,246450,3593222,24.349675,Democrat DC,70848,475800,647484,28.2534,Democrat DE,54976,228500,926454,24.45553333,Democrat Using read_csv as we did previously does not allow us to correctly load the data into R. In the case of this file we end up only reading in one column of the data set: us_data &lt;- read_csv(&quot;data/state_property_vote_meta-data.csv&quot;) ## Parsed with column specification: ## cols( ## `Data source: https://datausa.io/` = col_character() ## ) ## Warning: 53 parsing failures. ## row col expected actual file ## 3 -- 1 columns 6 columns &#39;data/state_property_vote_meta-data.csv&#39; ## 4 -- 1 columns 6 columns &#39;data/state_property_vote_meta-data.csv&#39; ## 5 -- 1 columns 6 columns &#39;data/state_property_vote_meta-data.csv&#39; ## 6 -- 1 columns 6 columns &#39;data/state_property_vote_meta-data.csv&#39; ## 7 -- 1 columns 6 columns &#39;data/state_property_vote_meta-data.csv&#39; ## ... ... ......... ......... ........................................ ## See problems(...) for more details. us_data ## # A tibble: 55 x 1 ## `Data source: https://datausa.io/` ## &lt;chr&gt; ## 1 Record of how data was collected: https://github.com/UBC-DSCI/introduct… ## 2 Date collected: 2017-06-06 ## 3 state ## 4 AK ## 5 AL ## 6 AR ## 7 AZ ## 8 CA ## 9 CO ## 10 CT ## # … with 45 more rows To successfully read data like this into R, the skip argument can be useful to R how many lines to skip before reading in the data. In the example above, we would set this value to 3: us_data &lt;- read_csv(&quot;data/state_property_vote_meta-data.csv&quot;, skip = 3) ## Parsed with column specification: ## cols( ## state = col_character(), ## med_income = col_double(), ## med_prop_val = col_double(), ## population = col_double(), ## mean_commute_minutes = col_double(), ## party = col_character() ## ) us_data ## # A tibble: 52 x 6 ## state med_income med_prop_val population mean_commute_minutes party ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AK 64222 197300 733375 10.5 Republican ## 2 AL 36924 94800 4830620 25.3 Republican ## 3 AR 35833 83300 2958208 22.4 Republican ## 4 AZ 44748 128700 6641928 20.6 Republican ## 5 CA 53075 252100 38421464 23.4 Democrat ## 6 CO 48098 198900 5278906 19.5 Democrat ## 7 CT 69228 246450 3593222 24.3 Democrat ## 8 DC 70848 475800 647484 28.3 Democrat ## 9 DE 54976 228500 926454 24.5 Democrat ## 10 FL 43355 125600 19645772 24.8 Republican ## # … with 42 more rows 2.4.2 read_delim as a more flexible method to get tabular data into R When our tabular data comes in a different format, we can use the read_delim function() instead. For example, a different version of this same dataset has no column names and uses tabs as the delimiter instead of commas. Here is how the file would look in plain text editor: AK 64222 197300 733375 10.46830207 Republican AL 36924 94800 4830620 25.30990746 Republican AR 35833 83300 2958208 22.40108933 Republican AZ 44748 128700 6641928 20.58786 Republican CA 53075 252100 38421464 23.38085172 Democrat CO 48098 198900 5278906 19.50792188 Democrat CT 69228 246450 3593222 24.349675 Democrat DC 70848 475800 647484 28.2534 Democrat DE 54976 228500 926454 24.45553333 Democrat FL 43355 125600 19645772 24.78055522 Republican To get this into R using the read_delim() function, we specify the first argument as the path to the file (as done with read_csv), and then provide values to the delim argument (here a tab) and the col_names argument (here false). Both read_csv() and read_delim() have a col_names argument and the default is True. library(tidyverse) us_data &lt;- read_delim(&quot;data/state_property_vote.tsv&quot;, delim = &quot;\\t&quot;, col_names = FALSE) ## Parsed with column specification: ## cols( ## X1 = col_character(), ## X2 = col_double(), ## X3 = col_double(), ## X4 = col_double(), ## X5 = col_double(), ## X6 = col_character() ## ) us_data ## # A tibble: 52 x 6 ## X1 X2 X3 X4 X5 X6 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AK 64222 197300 733375 10.5 Republican ## 2 AL 36924 94800 4830620 25.3 Republican ## 3 AR 35833 83300 2958208 22.4 Republican ## 4 AZ 44748 128700 6641928 20.6 Republican ## 5 CA 53075 252100 38421464 23.4 Democrat ## 6 CO 48098 198900 5278906 19.5 Democrat ## 7 CT 69228 246450 3593222 24.3 Democrat ## 8 DC 70848 475800 647484 28.3 Democrat ## 9 DE 54976 228500 926454 24.5 Democrat ## 10 FL 43355 125600 19645772 24.8 Republican ## # … with 42 more rows Data frames in R need to have column names, thus if you read data into R as a data frame without column names then R assigns column names for them. If you used the read_* functions to read the data into R, then R gives each column a name of X1, X2, …, XN, where N is the number of columns in the data set. 2.4.3 Reading tabular data directly from a URL We can also use read_csv() or read_delim() (and related functions) to read in tabular data directly from a url that contains tabular data. In this case, we provide the url as a string to read_csv() as the path to the file instead of a path to a local file on our computer. All other arguments that we use are the same as when using these functions with a local file on our computer. library(tidyverse) us_data &lt;- read_csv(&quot;https://raw.githubusercontent.com/UBC-DSCI/introduction-to-datascience/master/state_property_vote.csv&quot;) ## Parsed with column specification: ## cols( ## state = col_character(), ## med_income = col_double(), ## med_prop_val = col_double(), ## population = col_double(), ## mean_commute_minutes = col_double(), ## party = col_character() ## ) us_data ## # A tibble: 52 x 6 ## state med_income med_prop_val population mean_commute_minutes party ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AK 64222 197300 733375 10.5 Republican ## 2 AL 36924 94800 4830620 25.3 Republican ## 3 AR 35833 83300 2958208 22.4 Republican ## 4 AZ 44748 128700 6641928 20.6 Republican ## 5 CA 53075 252100 38421464 23.4 Democrat ## 6 CO 48098 198900 5278906 19.5 Democrat ## 7 CT 69228 246450 3593222 24.3 Democrat ## 8 DC 70848 475800 647484 28.3 Democrat ## 9 DE 54976 228500 926454 24.5 Democrat ## 10 FL 43355 125600 19645772 24.8 Republican ## # … with 42 more rows 2.4.4 Previewing a data file before reading it into R In all the examples above, we gave you previews of the data file before we read it into R. This is essential so you can see whether or not there are column names, what the delimiters are, and if there are lines you need to skip. You should do this yourself when trying to read in data files. In Jupyter, you can do this by using the Jupyter home menu to navigate to the file and clicking on it to preview it as a plain text file. We demonstrate this in the video below: 2.5 Reading data from an Microsoft Excel file One of the most widely used spreadsheet software is Microsoft Excel, thus it is very likely that one day you will need to load data into R from an Excel spreadsheet. To be able to do this, a key thing to know is that even though a .csv file loaded in Excel looks almost identical to a .xlsx (file extension for a typical Excel file) file in Excel, these are two very different file types in reality and because of that you need to use different tools to load these types of files into R. .csv files are plain text files, meaning that the characters you see when you open the file in a plain text editor are the same that you would see in Microsoft excel. This is not the case for .xlsx files. Below we show what a .xlsx file would look like in a plain text editor: ,?&#39;O _rels/.rels???J1??&gt;E?{7? &lt;?V????w8?&#39;J???&#39;QrJ???Tf?d??d?o?wZ&#39;?ф??@&gt;?4&#39;?|??hlIo??Fǟ t 8f??3wn ????t??u&quot;/ %~Ed2Ɩ??&lt;?w?? ?Pd(??J-?E???7?&#39;t(?-GZ?????y?ߎ??c~N?g[^_r?4 yG?O ?K??G?RPX?&lt;??,?&#39;O[Content_Types].xml???n?0E%?J ]TUE袏e??O??c[???????6q??s??d?m???ƻ\\???H?^????3} ?rZY? ?:L60?^?????XTP+?|?3???&quot;~?3T1W3???ޓ,?#p?R?!??w(??R???[S?D?kP?P!XS(?i?t?$?ei оX?ܒa??4VT?,D?Jq D ?????u?]??;??L?.8AhfNv}߼?hHF*җ??Jr?Q?%?g?U??CtX&quot;8x&gt;?.|????5j?/$???JE?c??~ݜ??4iw?????E;?+?S??w?cV+?:???2l???=?2nݡl?????;|?V??????c&#39;?????9?P&amp;Bcj,?&#39;OdocProps/app.xml??1 ?0???k????A?u?U?]ȝ??{#?:;/&lt;?g?Cߩd????M+?=???Z?O??R+??u?P?X?̔ KV@??M$??a???d,??_???4??5v?R????9D????t??Fk?Ú&#39;P?=?,?&#39;OdocProps/core.xml??MO?0 ??J?{???3j?h&#39;??(q??U4J ??=i?I&#39;?b??[v?ޙ!??{gk? F2????v5yj7??&quot;J???,?d???J???C??ƚl??4?-?$ۤ??`$?4t?K?.;?%c?J-Q҉??G&lt;??ɑZ-H???? X????z???6?????~q??X??ٗ6?????q^&gt;??tH???*?D???M?g ??D?????????ݐ?d?:g).ے?3.??׉uԟ?ǟj?P?F؝,?&#39;Oxl/_rels/workbook.xml.rels??Ak1??J?{7???R?^J?kk@Hf7??I?L???E]A?Þ?{a??`f?????b?6xUQ?@o?m}??o????X{???Q?????;?y?\\? O ?YY??4΀?L??S??k?252j?? ??V ?C?g?C]??????? ? ???E??TENyf6% ?Y????|??:%???}^ N?Q?ۤ?N&#39;????)??F?\\??P?G??,?&#39;O&#39;xl/printerSettings/printerSettings1.bin?Wmn? ??Sp&gt;?G???q?# ?I??ڒ5R&#39;???q????(?L ??m??8F?5&lt; L`?ݍ-?`?A??2{dp??9R#?&gt;7??Xu???ꋝ/?X??HI?|? ??r)???\\?VA8?2dFfq???I]]o 5`??&lt;͘??6uA ? This type file representation allows Excel files to store additional things that you cannot store in a .csv file, such as fonts, text formatting, graphics, multiple sheets and more. Can we read Excel files into R if they look like that in a plain text editor? The answer is yes, but to do this easily we need to use an R package developed specifically for this purpose, called the readxl package. library(readxl) us_data &lt;- read_excel(&quot;data/state_property_vote.xlsx&quot;) us_data ## # A tibble: 52 x 6 ## state med_income med_prop_val population mean_commute_minutes party ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AK 64222 197300 733375 10.5 Republican ## 2 AL 36924 94800 4830620 25.3 Republican ## 3 AR 35833 83300 2958208 22.4 Republican ## 4 AZ 44748 128700 6641928 20.6 Republican ## 5 CA 53075 252100 38421464 23.4 Democrat ## 6 CO 48098 198900 5278906 19.5 Democrat ## 7 CT 69228 246450 3593222 24.3 Democrat ## 8 DC 70848 475800 647484 28.3 Democrat ## 9 DE 54976 228500 926454 24.5 Democrat ## 10 FL 43355 125600 19645772 24.8 Republican ## # … with 42 more rows If the .xlsx file has multiple sheets, then you have to use the sheet argument to specify either the sheet number or name. You can also specify cell ranges usig the range argument in case the sheet contains multiple tables (a sad thing that happens to many Excel spreadsheets). As with plain text files, you should always try to explore the data file before importing it into R. This helps you decide which arguments you need to use to successfully load the data into R. If you do not have the Excel program on your computer, there are other free programs you can use to preview the file. Examples include Google Sheets and Libre Office. 2.6 Reading data from a Database TBD 2.7 Writing data from R to a .csv file At the middle and end of a data analysis, we often want to write the data frame that we have changed (either through filtering and subsetting or summarizing) to a file so that we can share it with others, or use it for another step in our analysis. The most straightforward way to do this is to use the tidyverse’s write_csv function. The default arguments for this file are to include column names and use a comma (,) as the delimiter. Below we demonstrate creating a new version of the US state-level property, income, population and voting data from 2015 and 2016 that does not contain the territory of Puerto Rico, and then writing this to a .csv file: state_data &lt;- filter(us_data, state != &quot;PR&quot;) write_csv(state_data, &quot;data/us_states_only.csv&quot;) 2.8 Scraping data off the web using R In the first part of this chapter we learned how to read in data from plain text files that are usually “rectangular” in shape using the tidyverse read_* functions. Sadly, not all data comes in this simple format, but happily there are many other tools we can use to read in more messy/wild data formats. One common place people often want/need to read in data from is websites. Such data exists in an a non-rectangular format. One quick and easy solution to get this data is to copy and paste it, however this becomes painstakingly long and boring when there is a lot of data that needs gathering, and anytime you start doing a lot of copying and pasting it is very likely you will introduce errors. The formal name for gathering non-rectangular data from the web and transforming it into a more useful format for data analysis is web scraping. There are two different ways to do web scraping: 1) screen scraping (similar to copying and pasting from a website, but done in a programmatic way to minimize errors and maximize efficiency) and 2) web APIs (application programming interface) (a website that provides a programatic way of returning the data as JSON or XML files via http requests). In this course we will explore the first method, screen scraping using R’s rvest package. 2.8.1 HTML and CSS selectors Before we jump into scraping, let’s set up some motivation and learn a little bit about what the “source code” of a website looks like. Say we are interested in knowing the average rental price (per square footage) of the most recently available 1 bedroom apartments in Vancouver from https://vancouver.craigslist.org. When we visit the Vancouver Craigslist website and search for 1 bedroom apartments, this is what we are shown: From that page, it’s pretty easy for our human eyes to find the apartment price and square footage. But how can we do this programmatically so we don’t have to copy and paste all these numbers? Well, we have to deal with the webpage source code, which we show a snippet of below (and link to the entire source code here): &lt;span class=&quot;result-meta&quot;&gt; &lt;span class=&quot;result-price&quot;&gt;$800&lt;/span&gt; &lt;span class=&quot;housing&quot;&gt; 1br - &lt;/span&gt; &lt;span class=&quot;result-hood&quot;&gt; (13768 108th Avenue)&lt;/span&gt; &lt;span class=&quot;result-tags&quot;&gt; &lt;span class=&quot;maptag&quot; data-pid=&quot;6786042973&quot;&gt;map&lt;/span&gt; &lt;/span&gt; &lt;span class=&quot;banish icon icon-trash&quot; role=&quot;button&quot;&gt; &lt;span class=&quot;screen-reader-text&quot;&gt;hide this posting&lt;/span&gt; &lt;/span&gt; &lt;span class=&quot;unbanish icon icon-trash red&quot; role=&quot;button&quot; aria-hidden=&quot;true&quot;&gt;&lt;/span&gt; &lt;a href=&quot;#&quot; class=&quot;restore-link&quot;&gt; &lt;span class=&quot;restore-narrow-text&quot;&gt;restore&lt;/span&gt; &lt;span class=&quot;restore-wide-text&quot;&gt;restore this posting&lt;/span&gt; &lt;/a&gt; &lt;/span&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;result-row&quot; data-pid=&quot;6788463837&quot;&gt; &lt;a href=&quot;https://vancouver.craigslist.org/nvn/apa/d/north-vancouver-luxury-1-bedroom/6788463837.html&quot; class=&quot;result-image gallery&quot; data-ids=&quot;1:00U0U_lLWbuS4jBYN,1:00T0T_9JYt6togdOB,1:00r0r_hlMkwxKqoeq,1:00n0n_2U8StpqVRYX,1:00M0M_e93iEG4BRAu,1:00a0a_PaOxz3JIfI,1:00o0o_4VznEcB0NC5,1:00V0V_1xyllKkwa9A,1:00G0G_lufKMygCGj6,1:00202_lutoxKbVTcP,1:00R0R_cQFYHDzGrOK,1:00000_hTXSBn1SrQN,1:00r0r_2toXdps0bT1,1:01616_dbAnv07FaE7,1:00g0g_1yOIckt0O1h,1:00m0m_a9fAvCYmO9L,1:00C0C_8EO8Yl1ELUi,1:00I0I_iL6IqV8n5MB,1:00b0b_c5e1FbpbWUZ,1:01717_6lFcmuJ2glV&quot;&gt; &lt;span class=&quot;result-price&quot;&gt;$2285&lt;/span&gt; &lt;/a&gt; &lt;p class=&quot;result-info&quot;&gt; &lt;span class=&quot;icon icon-star&quot; role=&quot;button&quot;&gt; &lt;span class=&quot;screen-reader-text&quot;&gt;favorite this post&lt;/span&gt; &lt;/span&gt; &lt;time class=&quot;result-date&quot; datetime=&quot;2019-01-06 12:06&quot; title=&quot;Sun 06 Jan 12:06:01 PM&quot;&gt;Jan 6&lt;/time&gt; &lt;a href=&quot;https://vancouver.craigslist.org/nvn/apa/d/north-vancouver-luxury-1-bedroom/6788463837.html&quot; data-id=&quot;6788463837&quot; class=&quot;result-title hdrlnk&quot;&gt;Luxury 1 Bedroom CentreView with View - Lonsdale&lt;/a&gt; This is not easy for our human eyeballs to read! However, it is easy for us to use programmatic tools to extract the data we need by specifying which HTML tags (things inside &lt; and &gt; in the code above). For example, if we look in the code above and search for lines with a price, we can also look at the tags that are near that price and see if there’s a common “word” we can use that is near the price but doesn’t exist on other lines that have information we are not interested in: &lt;span class=&quot;result-price&quot;&gt;$800&lt;/span&gt; and &lt;span class=&quot;result-price&quot;&gt;$2285&lt;/span&gt; What we can see is there is a special “word” here, “result-price”, which appears only on the lines with prices and not on the other lines (that have information we are not interested in). This special word and the context in which is is used (learned from the other words inside the HTML tag) can be combined to create something called a CSS selector. The CSS selector can then be used by R’s rvest package to select the information we want (here price) from the website source code. Now, many websites are quite large and complex, and so then is their website source code. And as you saw above, it is not easy to read and pick out the special words we want with our human eyeballs. So to make this easier, we will use the SelectorGadget tool. It is an open source tool that simplifies generating and finding CSS selectors. We recommend you use the Chrome web browser to use this tool, and install the selector gadget tool from the Chrome Web Store. Here is a short video on how to install and use the SelectorGadget tool to get a CSS selector for use in web scraping: From installing and using the selectorgadget as shown in the video above, we get the two CSS selectors .housing and .result-price that we can use to scrape information about the square footage and the rental price, respectively. The selector gadget returns them to us as a comma separated list (here .housing , .result-price), which is exactly the format we need to provide to R if we are using more than one CSS selector. 2.8.2 Are you allowed to scrape that website? BEFORE scraping data from the web, you should always check whether or not you are ALLOWED to scrape it! There are two documents that are important for this: the robots.txt file and reading the website’s Terms of Service document. The website’s Terms of Service document is probably the more important of the two, and so you should look there first. What happens when we look at Craigslist’s Terms of Service document? Well we read this: “You agree not to copy/collect CL content via robots, spiders, scripts, scrapers, crawlers, or any automated or manual equivalent (e.g., by hand).” source: https://www.craigslist.org/about/terms.of.use Want to learn more about the legalities of web scraping and crawling? Read this interesting blog post titled “Web Scraping and Crawling Are Perfectly Legal, Right?” by Benoit Bernard (this is optional, not required reading). So what to do now? Well, we shouldn’t scrape Craigslist! Let’s instead scrape some data on the population of Canadian cities from Wikipedia (who’s Terms of Service document does not explicilty say do not scrape). In this video below we demonstrate using the selectorgadget tool to get CSS Selectors from Wikipedia’s Canada page to scrape a table that contains city names and their populations from the 2016 Canadian Census: 2.8.3 Using rvest Now that we have our CSS selectors we can use rvest R package to scrape our desired data from the website. First we start by loading the rvest package: library(rvest) ## Loading required package: xml2 ## ## Attaching package: &#39;rvest&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## pluck ## The following object is masked from &#39;package:readr&#39;: ## ## guess_encoding library(rvest) gives error… If you get an error about R not being able to find the package (e.g., Error in library(rvest) : there is no package called ‘rvest’) this is likely because it was not installed. To install the rvest package, run the following command once inside R (and then delete that line of code): install.packages(&quot;rvest&quot;). Next, we tell R what page we want to scrape by providing the webpage’s URL in quotations to the function read_html: page &lt;- read_html(&quot;https://en.wikipedia.org/wiki/Canada&quot;) Then we send the page object to the html_nodes function. We also provide that function with the CSS selectors we obtained from the selectorgadget tool. These should be surrounded by quotations. The html_nodes function select nodes from the HTML document using CSS selectors. nodes are the HTML tag pairs as well as the content between the tags. For our CSS selector td:nth-child(5) and example node that would be selected would be: &lt;td style=&quot;text-align:left;background:#f0f0f0;&quot;&gt;&lt;a href=&quot;/wiki/London,_Ontario&quot; title=&quot;London, Ontario&quot;&gt;London&lt;/a&gt;&lt;/td&gt; population_nodes &lt;- html_nodes(page, &quot;td:nth-child(5) , td:nth-child(7) , .infobox:nth-child(122) td:nth-child(1) , .infobox td:nth-child(3)&quot;) head(population_nodes) ## {xml_nodeset (6)} ## [1] &lt;td style=&quot;text-align:right;&quot;&gt;5,928,040&lt;/td&gt; ## [2] &lt;td style=&quot;text-align:left;background:#f0f0f0;&quot;&gt;&lt;a href=&quot;/wiki/Londo ... ## [3] &lt;td style=&quot;text-align:right;&quot;&gt;494,069\\n&lt;/td&gt; ## [4] &lt;td style=&quot;text-align:right;&quot;&gt;4,098,927&lt;/td&gt; ## [5] &lt;td style=&quot;text-align:left;background:#f0f0f0;&quot;&gt;\\n&lt;a href=&quot;/wiki/St. ... ## [6] &lt;td style=&quot;text-align:right;&quot;&gt;406,074\\n&lt;/td&gt; Next we extract the meaningful data from the HTML nodes using the html_text function. For our example, this functions only required argument is the an html_nodes object, which we named rent_nodes. In the case of this example node: &lt;td style=&quot;text-align:left;background:#f0f0f0;&quot;&gt;&lt;a href=&quot;/wiki/London,_Ontario&quot; title=&quot;London, Ontario&quot;&gt;London&lt;/a&gt;&lt;/td&gt;, the html_text function would return London. population_text &lt;- html_text(population_nodes) head(population_text) ## [1] &quot;5,928,040&quot; &quot;London&quot; ## [3] &quot;494,069\\n&quot; &quot;4,098,927&quot; ## [5] &quot;St. Catharines–Niagara&quot; &quot;406,074\\n&quot; Are we done? Not quite… If you look at the data closely you see that the data is not in an optimal format for data analysis. Both the city names and population are encoded as characters in a single vector instead of being in a data frame with one character column for city and one numeric column for population (think of how you would organize the data in a spreadsheet). Additionally, the populations contain commas (not useful for programmatically dealing with numbers), and some even contain a line break character at the end (\\n). Next chapter we will learn more about data wrangling using R so that we can easily clean up this data with a few lines of code. 2.9 Additional readings/resources Data import chapter from R for Data Science by Garrett Grolemund &amp; Hadley Wickham "]
]
