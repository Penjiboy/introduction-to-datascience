[
["classification.html", "Chapter 5 Classification 5.1 Overview 5.2 Learning objectives 5.3 Classification 5.4 Wisconsin Breast Cancer Example:", " Chapter 5 Classification 5.1 Overview This chapter serves as an introduction to classification using K-nearest neighbours (k-nn) in the case where we have two quantitative variables that we want to use to predict the class of a third, categorical variable. 5.2 Learning objectives Recognize situations where a simple classifier would be appropriate for making predictions. Explain the k-nearest neighbour classification algorithm. Interpret the output of a classifier. Compute, by hand, the length of the ordinary, straight (Euclidian) distance line between points on a graph when there are two explanatory variables/predictors. Describe what a training data set is and how it is used in classification. In a dataset with two explanatory variables/predictors, perform k-nearest neighbour classification in R using caret::train(method = &quot;knn&quot;, ...) to predict the class of a single new observation. 5.3 Classification In many situations, we want to learn how to make predictions based on our experience from past examples. For instance, a doctor wants to diagnose a patient as either diseased or healthy based on some observed characteristics, an email provider would like to assign a given email as “spam” or “non-spam”, or an online store wants to predict if an order is fraudulent (or not). These are all examples of classification tasks. Classification is the problem of predicting a qualitative or categorical class/label for an observation (set of data collected from an object, such as a person or an email). It involves assigning an observation to a class (e.g. disease or healthy) on the basis of how similar they are to other observations that have already been classified. These already classified observations that we use as a basis to predict classes for new, unclassfied observations is called a training set. We call them a “training set” because we use these observations to train, or teach, our classifier so that we can use it to make predictions on new data that we have not seen previously. There are many possible classifier methods that we could use to predict a qualitative or categorical class/label for an observation. These classification methods can perform binary classification, where only two classes are involved (e.g. disease or healthy patient), as well as multiclass classification, which involves assigning an object to one of several classes (e.g., private, public, or not for-profit organization). Here we will focus on a simple, and widely used method of classification called K-nearest neighbors, but other examples include decision trees, support vector machines and logistic regression. 5.4 Wisconsin Breast Cancer Example: Let’s start by looking at some Breast Cancer data, which was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg. Each row in the data set represents an observation of a tumour whose diagnosis class is known (benign/non-cancerous or malignant/cancerous) and for each, we have several response variables/predictors (texture, perimeter etc.). We’d like to find which response variables/predictors are most useful for diagnosing benign or malignant tumours, and develop a way to classify future patient tumours when we do not know the label/class (benign/non-cancerous or malignant/cancerous). Being able to do this is important because benign tumours are not normally dangerous, the cells stay in the same place and the tumour stops growing before it gets very large. In malignant tumours, the cells invade the surrounding tissue and spread into nearby organs where they can cause serious damage. (https://www.worldwidecancerresearch.org/who-we-are/cancer-basics/) 5.4.1 Data Exploration As usual, we start by loading the necessary libraries for our analysis. We have learned about the tidyverse before. Today we’ll also be loading a new library, forcats that allows us to easily manipulate factors in R. Factors are a special categorical type of varibale in R that are very helpful when doing statistics and machine learning with categorical variables. library(tidyverse) library(forcats) The data file we need to read in is a plain vanilla csv with headers, and thus we can use the read_csv function with no additional arguments: cancer &lt;- read_csv(&quot;data/clean-wdbc.data.csv&quot;) head(cancer) ## # A tibble: 6 x 12 ## ID Class Radius Texture Perimeter Area Smoothness Compactness ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 8.42e5 M 1.89 -1.36 2.30 2.00 1.31 2.61 ## 2 8.43e5 M 1.80 -0.369 1.53 1.89 -0.375 -0.430 ## 3 8.43e7 M 1.51 -0.0240 1.35 1.46 0.527 1.08 ## 4 8.43e7 M -0.281 0.134 -0.250 -0.550 3.39 3.89 ## 5 8.44e7 M 1.30 -1.47 1.34 1.22 0.220 -0.313 ## 6 8.44e5 M -0.165 -0.314 -0.115 -0.244 2.05 1.72 ## # ... with 4 more variables: Concavity &lt;dbl&gt;, Concave_points &lt;dbl&gt;, ## # Symmetry &lt;dbl&gt;, Fractal_dimension &lt;dbl&gt; 5.4.1.1 Variable descriptions Breast tumours can be diagnosed by performing a biopsy, a process where tissue is removed from the body to discover the presence of a disease. Traditionally these procedures were quite invasive, but now fine needle asipiration is a type of biopsy that uses a thin needle to examine a small amount of tissue from the tumour. With this method, 10 different variables are typically measured of cell nuclei from a digital image of a fine needle aspirate (FNA) of a breast mass. Source: https://www.semanticscholar.org/paper/Breast-Cancer-Diagnosis-and-Prognosis-Via-Linear-P-Mangasarian-Street/3721bb14b16e866115c906336e9d70db096c05b9/figure/0 A magnified image of a malignant breast Fine Needle Aspiration image. White lines denote the boundary of the cell nuclei. ID number Class - diagnosis (M = malignant, B = benign) radius (mean of distances from center to points on the perimeter) texture (standard deviation of gray-scale values) perimeter area smoothness (local variation in radius lengths) compactness (\\(perimeter^2 / area - 1.0\\)) concavity (severity of concave portions of the contour) concave points (number of concave portions of the contour) symmetry fractal dimension (\\(&quot;coastline\\: approximation&quot; - 1\\)) The “worst” or largest (mean of the three largest values) values of these variable were computed for each image. As part of the data preparation, the data have been scaled (we will discuss what this means and why we do it in the next chapter). Below we use glimpse to preview the data frame. This function is similar to head, but can be easier to read when we have a lot of columns: glimpse(cancer) ## Observations: 569 ## Variables: 12 ## $ ID &lt;int&gt; 842302, 842517, 84300903, 84348301, 84358402... ## $ Class &lt;chr&gt; &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;,... ## $ Radius &lt;dbl&gt; 1.8850310, 1.8043398, 1.5105411, -0.2812170,... ## $ Texture &lt;dbl&gt; -1.35809849, -0.36887865, -0.02395331, 0.133... ## $ Perimeter &lt;dbl&gt; 2.30157548, 1.53377643, 1.34629062, -0.24971... ## $ Area &lt;dbl&gt; 1.999478159, 1.888827020, 1.455004298, -0.54... ## $ Smoothness &lt;dbl&gt; 1.306536657, -0.375281748, 0.526943750, 3.39... ## $ Compactness &lt;dbl&gt; 2.61436466, -0.43006581, 1.08198014, 3.88997... ## $ Concavity &lt;dbl&gt; 2.10767182, -0.14661996, 0.85422232, 1.98783... ## $ Concave_points &lt;dbl&gt; 2.29405760, 1.08612862, 1.95328166, 2.173873... ## $ Symmetry &lt;dbl&gt; 2.7482041, -0.2436753, 1.1512420, 6.0407261,... ## $ Fractal_dimension &lt;dbl&gt; 1.93531174, 0.28094279, 0.20121416, 4.930671... We can see from the summary of the data above that Class is of type character. We are going to be working with “Class” as a categorical statistical variable so we will convert it to factor using the function as.factor. cancer &lt;- cancer %&gt;% mutate(Class = as.factor(Class)) Factors have what are called “levels”, which you can think of as categories. We can ask for the levels from the Class column by using the levels function. This function should return the name of each category in that column. Given that we only have 2 different values in our Class column, “B” and “M”, we only expect to get two names back. If we had 4 difference values in the column, we would expect to get 4 back. Note the use of unlist to between select and levels. This is because select outputs a data frame (even though we only select a single column), and levels expects a vector. cancer %&gt;% select(Class) %&gt;% unlist() %&gt;% # turns a data frame into a vector levels() ## [1] &quot;B&quot; &quot;M&quot; In our dataset, we have 357 (63%) benign and 212 (37%) malignant observations. cancer %&gt;% group_by(Class) %&gt;% tally() ## # A tibble: 2 x 2 ## Class n ## &lt;fct&gt; &lt;int&gt; ## 1 B 357 ## 2 M 212 Let’s draw a scatter plot to visualize the relationship between the perimeter and concavity variables. cbPalette &lt;- c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;) # colour palette p &lt;- cancer %&gt;% ggplot(aes(x=Perimeter, y=Concavity, color = Class)) + geom_point() + scale_x_continuous(name = &quot;Perimeter&quot;) + scale_y_continuous(name = &quot;Concavity&quot;) + scale_color_manual(values=c(cbPalette[3], cbPalette[2])) p Suppose we have a new observation that is not in the data set with perimeter 1 and concavity 1, would you classify that observation as benign or malignant? What about a new observation with perimeter -1 and concavity -0.5? What about 0 and 1? We can see that most of the benign observations are clustered in the lower-left corner and most of the malignant observations are above and to the right of that cluster, though not all the observations follow this pattern. We want to find a way to program a computer to automatically detect patterns. 5.4.2 K-Nearest Neighbour Classifier To classify a new observation as benign or malignant, we find some observations in the training set that are “nearest” to our new observation, and then use that diagnosis (benign or malignant) to make a prediction. Suppose we have a new observation, with perimeter of 2 and concavity of 4 (labelled in red on the scatterplot), whose “Class” is unknown. We see that the nearest point to this new observation is located at the coordinates (2.3, 3.2). The idea here is that if a point is close to one another in the scatterplot then the perimeter and concavity values are similar so we may expect that they would have the same diagnosis. Suppose we have another new observation with perimeter 0.38 and concavity of 1.8. Looking at the scatterplot below, how would you classify this red observation? The nearest neighbour to this new point is a benign observation at (0.2, 1.8). We can look at a few points, say \\(k = 3\\), that are closest to the new red observation to predict its class rather than just looking at one. Among those 3 closest points, we look at their class and use the majority class as our prediction for the new observation. We see that the diagnoses of 2 of the 3 nearest neighbours to our new observation are malignant so we take majority vote and classify our new red observation as malignant. ## # A tibble: 3 x 4 ## ID Perimeter Concavity Class ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 9113239 0.230 1.84 B ## 2 8511133 0.531 1.72 M ## 3 899667 0.361 1.99 M Here we chose the \\(k=3\\) nearest observations, but there is nothing special about \\(k=3\\). We could have used \\(k=4, 5\\) or more, though we may want to choose an odd number to avoid ties. We will discuss more about choosing \\(k\\) in the next section. 5.4.2.1 Distance Between Points When There are Two Attributes How do we decide which points are “nearest” to our new observation? We can compute the distance between any pair of points using the following formula: \\[Distance = \\sqrt{(x_a -x_b)^2 + (y_a - y_b)^2}\\] Suppose we want to classify a new observation with perimeter of -1 and concavity of 4.2. Let’s calculate the distances between our new point and each of the observations in the training set to find the \\(k=5\\) observations in the training data that are nearest to our new point. ID Perimeter Concavity Distance Class 859471 -1.24 4.7 \\(\\sqrt{-1 - (-1.24))^2 + (4.2 - 4.7)^2}=\\) 0.55 B 84501001 -0.29 3.99 \\(\\sqrt{(-1 - (-0.29))^2 + (4.2 - 3.99)^2} =\\) 0.74 M 8710441 -1.08 2.63 \\(\\sqrt{(-1 - (-1.08))^2 + (4.2 - 2.63)^2} =\\) 1.57 B 9013838 -0.46 2.72 \\(\\sqrt{(-1 - (-0.46))^2 + (4.2 - 2.72)^2} =\\) 1.57 M 925622 0.64 4.3 \\(\\sqrt{(-1 - 0.64)^2 + (4.2 - 4.3)^2} =\\) 1.64 M From the table, we see that 3 of the 5 nearest neighbours to our new observation are malignant so classify our new observation as malignant. 5.4.2.1.1 Summary: In order to classify a new observation using a k-nearest neighbor classifier, we have to do the follow steps: Step 1: Compute the distance between the new observation and each observation in our training set. Step 2: Sort the data table in ascending order according to the distances. Step 3: Choose the top \\(k\\) rows of the sorted table. Step 4: Classify the new observation based on majority vote. 5.4.3 K-Nearest Neighbours in R We will implement the k-nearest neighbour algorithm in R. Here we will make use of the caret (Classification And REgression Training) package in R, which contains a set of tools to help the process of making predictive models. We can use the function names(getModelInfo()) to see a full list of the algorithms caret has to offer. library(caret) ## Loading required package: lattice ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## lift Let’s suppose we have a new observation with perimeter 0 and concavity 0.5, but its diagnosis is unknown. Suppose we want to use its perimeter and concavity attributes to predict the class of this observation. Let’s pick out our 2 desired attributes and store it as a new dataset. trainingDat &lt;- cancer %&gt;% select(&quot;Perimeter&quot;, &quot;Concavity&quot;) glimpse(trainingDat) ## Observations: 569 ## Variables: 2 ## $ Perimeter &lt;dbl&gt; 2.30157548, 1.53377643, 1.34629062, -0.24971958, 1.3... ## $ Concavity &lt;dbl&gt; 2.10767182, -0.14661996, 0.85422232, 1.98783917, 0.6... We will store the class labels in a vector. labels &lt;- cancer$Class glimpse(labels) ## Factor w/ 2 levels &quot;B&quot;,&quot;M&quot;: 2 2 2 2 2 2 2 2 2 2 ... We will use the function train(), where x is an object where observations are in rows and the attributes are in columns (e.g. simple matrix, dataframe) and y is a numeric or factor vector containing the outcome for each row. The argument tuneGrid should be a dataframe with possible “tuning values”. For now, just know that this is where we will specify our \\(k\\) and we will use \\(k =7\\) (we will discuss how to choose \\(k\\) in a later section). We will use “knn” as our method. k &lt;- data.frame(k = 7) model_knn &lt;- train(x = data.frame(trainingDat), y = labels, method=&#39;knn&#39;, tuneGrid = k) Now we can predict the label of the new observation. new_obs &lt;- data.frame(Perimeter = 0, Concavity = 0.5) predict(object=model_knn, new_obs) ## [1] M ## Levels: B M Our model classifies this new observation as malignant. How do we know how well our model did? In later sections, we will discuss ways to evaluate our model. 5.4.4 Multiple Attributes So far we have seen how to build a classifier based on only two attributes, but we can use k-nearest neighbours classifier in higher dimensional space. Let’s make a scatterplot with 3 variables instead of 2: ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout Each attribute can give us new information to help create our classifier. The distance formula for 3-dimensions is \\[Distance = \\sqrt{(x_a -x_b)^2 + (y_a - y_b)^2 + (z_a - z_b)^2}\\] We can generalize for n-dimensions by summing up the squares of the differences between each individual coordinate taking the square root of the sum Data source: W.N. Street, W.H. Wolberg and O.L. Mangasarian Nuclear feature extraction for breast tumor diagnosis. IS&amp;T/SPIE 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905, pages 861-870, San Jose, CA, 1993. "]
]
