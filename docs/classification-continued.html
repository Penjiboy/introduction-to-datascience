<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Classification continued | Introduction to Data Science</title>
  <meta name="description" content="This is an open source textbook for teaching introductory data science." />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Classification continued | Introduction to Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is an open source textbook for teaching introductory data science." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Classification continued | Introduction to Data Science" />
  
  <meta name="twitter:description" content="This is an open source textbook for teaching introductory data science." />
  

<meta name="author" content="Tiffany-Anne Timbers" />
<meta name="author" content="Melissa Lee" />
<meta name="author" content="Trevor Campbell" />


<meta name="date" content="2019-08-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classification.html">
<link rel="next" href="regression1.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.46.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.46.1/plotly-latest.min.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Data Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction to Data Science</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#chapter-learning-objectives"><i class="fa fa-check"></i><b>1.1</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#jupyter-notebooks"><i class="fa fa-check"></i><b>1.2</b> Jupyter notebooks</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#loading-a-tabular-dataset"><i class="fa fa-check"></i><b>1.3</b> Loading a tabular dataset</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#assigning-value-to-an-object"><i class="fa fa-check"></i><b>1.4</b> Assigning value to an object</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#subsetting-data-frames-with-select-filter"><i class="fa fa-check"></i><b>1.5</b> Subsetting data frames with <code>select</code> &amp; <code>filter</code></a><ul>
<li class="chapter" data-level="1.5.1" data-path="index.html"><a href="index.html#using-select-to-subset-multiple-columns"><i class="fa fa-check"></i><b>1.5.1</b> Using <code>select</code> to subset multiple columns</a></li>
<li class="chapter" data-level="1.5.2" data-path="index.html"><a href="index.html#using-select-to-subset-a-range-of-columns"><i class="fa fa-check"></i><b>1.5.2</b> Using <code>select</code> to subset a range of columns</a></li>
<li class="chapter" data-level="1.5.3" data-path="index.html"><a href="index.html#using-filter-to-subset-a-single-column"><i class="fa fa-check"></i><b>1.5.3</b> Using <code>filter</code> to subset a single column</a></li>
<li class="chapter" data-level="1.5.4" data-path="index.html"><a href="index.html#using-filter-to-get-rows-with-values-above-a-threshold"><i class="fa fa-check"></i><b>1.5.4</b> Using <code>filter</code> to get rows with values above a threshold</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#combining-functions-using-the-pipe-operator"><i class="fa fa-check"></i><b>1.6</b> Combining functions using the pipe operator: %&gt;%</a><ul>
<li class="chapter" data-level="1.6.1" data-path="index.html"><a href="index.html#using-to-combine-filter-and-select"><i class="fa fa-check"></i><b>1.6.1</b> Using %&gt;% to combine <code>filter</code> and <code>select</code></a></li>
<li class="chapter" data-level="1.6.2" data-path="index.html"><a href="index.html#using-with-more-than-two-functions"><i class="fa fa-check"></i><b>1.6.2</b> Using %&gt;% with more than two functions</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#creating-visualizations-in-r"><i class="fa fa-check"></i><b>1.7</b> Creating Visualizations in R</a><ul>
<li class="chapter" data-level="1.7.1" data-path="index.html"><a href="index.html#using-ggplot-to-create-a-scatter-plot"><i class="fa fa-check"></i><b>1.7.1</b> Using <code>ggplot</code> to create a scatter plot</a></li>
<li class="chapter" data-level="1.7.2" data-path="index.html"><a href="index.html#using-ggplot-to-create-a-scatter-plot-1"><i class="fa fa-check"></i><b>1.7.2</b> Using <code>ggplot</code> to create a scatter plot</a></li>
<li class="chapter" data-level="1.7.3" data-path="index.html"><a href="index.html#formatting-ggplot-objects"><i class="fa fa-check"></i><b>1.7.3</b> Formatting ggplot objects</a></li>
<li class="chapter" data-level="1.7.4" data-path="index.html"><a href="index.html#coloring-points-by-group"><i class="fa fa-check"></i><b>1.7.4</b> Coloring points by group</a></li>
<li class="chapter" data-level="1.7.5" data-path="index.html"><a href="index.html#whats-next"><i class="fa fa-check"></i><b>1.7.5</b> What’s next?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="reading.html"><a href="reading.html"><i class="fa fa-check"></i><b>2</b> Reading in data locally and from the web</a><ul>
<li class="chapter" data-level="2.1" data-path="reading.html"><a href="reading.html#overview"><i class="fa fa-check"></i><b>2.1</b> Overview</a></li>
<li class="chapter" data-level="2.2" data-path="reading.html"><a href="reading.html#chapter-learning-objectives-1"><i class="fa fa-check"></i><b>2.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="2.3" data-path="reading.html"><a href="reading.html#absolute-and-relative-file-paths"><i class="fa fa-check"></i><b>2.3</b> Absolute and relative file paths</a></li>
<li class="chapter" data-level="2.4" data-path="reading.html"><a href="reading.html#reading-tabular-data-into-r"><i class="fa fa-check"></i><b>2.4</b> Reading tabular data into R</a><ul>
<li class="chapter" data-level="2.4.1" data-path="reading.html"><a href="reading.html#read_delim-as-a-more-flexible-method-to-get-tabular-data-into-r"><i class="fa fa-check"></i><b>2.4.1</b> <code>read_delim</code> as a more flexible method to get tabular data into R</a></li>
<li class="chapter" data-level="2.4.2" data-path="reading.html"><a href="reading.html#reading-tabular-data-directly-from-a-url"><i class="fa fa-check"></i><b>2.4.2</b> Reading tabular data directly from a URL</a></li>
<li class="chapter" data-level="2.4.3" data-path="reading.html"><a href="reading.html#previewing-a-data-file-before-reading-it-into-r"><i class="fa fa-check"></i><b>2.4.3</b> Previewing a data file before reading it into R</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="reading.html"><a href="reading.html#scraping-data-off-the-web-using-r"><i class="fa fa-check"></i><b>2.5</b> Scraping data off the web using R</a><ul>
<li class="chapter" data-level="2.5.1" data-path="reading.html"><a href="reading.html#html-and-css-selectors"><i class="fa fa-check"></i><b>2.5.1</b> HTML and CSS selectors</a></li>
<li class="chapter" data-level="2.5.2" data-path="reading.html"><a href="reading.html#are-you-allowed-to-scrape-that-website"><i class="fa fa-check"></i><b>2.5.2</b> Are you allowed to scrape that website?</a></li>
<li class="chapter" data-level="2.5.3" data-path="reading.html"><a href="reading.html#using-rvest"><i class="fa fa-check"></i><b>2.5.3</b> Using <code>rvest</code></a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="reading.html"><a href="reading.html#additional-readingsresources"><i class="fa fa-check"></i><b>2.6</b> Additional readings/resources</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="wrangling.html"><a href="wrangling.html"><i class="fa fa-check"></i><b>3</b> Cleaning and wrangling data</a><ul>
<li class="chapter" data-level="3.1" data-path="wrangling.html"><a href="wrangling.html#overview-1"><i class="fa fa-check"></i><b>3.1</b> Overview</a></li>
<li class="chapter" data-level="3.2" data-path="wrangling.html"><a href="wrangling.html#chapter-learning-objectives-2"><i class="fa fa-check"></i><b>3.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="3.3" data-path="wrangling.html"><a href="wrangling.html#vectors-and-data-frames"><i class="fa fa-check"></i><b>3.3</b> Vectors and Data frames</a><ul>
<li class="chapter" data-level="3.3.1" data-path="wrangling.html"><a href="wrangling.html#what-is-a-data-frame"><i class="fa fa-check"></i><b>3.3.1</b> What is a data frame?</a></li>
<li class="chapter" data-level="3.3.2" data-path="wrangling.html"><a href="wrangling.html#what-is-a-vector"><i class="fa fa-check"></i><b>3.3.2</b> What is a vector?</a></li>
<li class="chapter" data-level="3.3.3" data-path="wrangling.html"><a href="wrangling.html#how-are-vectors-different-from-a-list"><i class="fa fa-check"></i><b>3.3.3</b> How are vectors different from a list?</a></li>
<li class="chapter" data-level="3.3.4" data-path="wrangling.html"><a href="wrangling.html#what-does-this-have-to-do-with-data-frames"><i class="fa fa-check"></i><b>3.3.4</b> What does this have to do with data frames?</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="wrangling.html"><a href="wrangling.html#the-dplyr-functions"><i class="fa fa-check"></i><b>3.4</b> The <code>dplyr</code> functions</a></li>
<li class="chapter" data-level="3.5" data-path="wrangling.html"><a href="wrangling.html#tidy-data"><i class="fa fa-check"></i><b>3.5</b> Tidy Data</a><ul>
<li class="chapter" data-level="3.5.1" data-path="wrangling.html"><a href="wrangling.html#what-is-tidy-data"><i class="fa fa-check"></i><b>3.5.1</b> What is tidy data?</a></li>
<li class="chapter" data-level="3.5.2" data-path="wrangling.html"><a href="wrangling.html#why-is-tidy-data-important-in-r"><i class="fa fa-check"></i><b>3.5.2</b> Why is tidy data important in R?</a></li>
<li class="chapter" data-level="3.5.3" data-path="wrangling.html"><a href="wrangling.html#going-from-wide-to-long-or-tidy-using-gather"><i class="fa fa-check"></i><b>3.5.3</b> Going from wide to long (or tidy!) using <code>gather</code></a></li>
<li class="chapter" data-level="3.5.4" data-path="wrangling.html"><a href="wrangling.html#using-separate-to-deal-with-multiple-delimiters"><i class="fa fa-check"></i><b>3.5.4</b> Using separate to deal with multiple delimiters</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="wrangling.html"><a href="wrangling.html#using-purrrs-map-functions-to-iterate"><i class="fa fa-check"></i><b>3.6</b> Using <code>purrr</code>’s <code>map*</code> functions to iterate</a><ul>
<li class="chapter" data-level="3.6.1" data-path="wrangling.html"><a href="wrangling.html#a-bit-more-about-the-map-functions"><i class="fa fa-check"></i><b>3.6.1</b> A bit more about the <code>map*</code> functions</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="wrangling.html"><a href="wrangling.html#additional-readingsresources-1"><i class="fa fa-check"></i><b>3.7</b> Additional readings/resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="viz.html"><a href="viz.html"><i class="fa fa-check"></i><b>4</b> Effective data visualization</a><ul>
<li class="chapter" data-level="4.1" data-path="viz.html"><a href="viz.html#overview-2"><i class="fa fa-check"></i><b>4.1</b> Overview</a></li>
<li class="chapter" data-level="4.2" data-path="viz.html"><a href="viz.html#chapter-learning-objectives-3"><i class="fa fa-check"></i><b>4.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="4.3" data-path="viz.html"><a href="viz.html#ggplot2-for-data-visualization-in-r"><i class="fa fa-check"></i><b>4.3</b> <code>ggplot2</code> for data visualization in R</a></li>
<li class="chapter" data-level="4.4" data-path="viz.html"><a href="viz.html#making-effective-vizualizations"><i class="fa fa-check"></i><b>4.4</b> Making effective vizualizations</a><ul>
<li class="chapter" data-level="4.4.1" data-path="viz.html"><a href="viz.html#some-guiding-principles-for-making-effectice-visualizations"><i class="fa fa-check"></i><b>4.4.1</b> Some guiding principles for making effectice visualizations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="GitHub.html"><a href="GitHub.html"><i class="fa fa-check"></i><b>5</b> Version control with GitHub</a><ul>
<li class="chapter" data-level="5.1" data-path="GitHub.html"><a href="GitHub.html#overview-3"><i class="fa fa-check"></i><b>5.1</b> Overview</a></li>
<li class="chapter" data-level="5.2" data-path="GitHub.html"><a href="GitHub.html#videos-to-learn-about-version-control-with-github-and-git"><i class="fa fa-check"></i><b>5.2</b> Videos to learn about version control with GitHub and Git</a><ul>
<li class="chapter" data-level="5.2.1" data-path="GitHub.html"><a href="GitHub.html#creating-a-github-repository"><i class="fa fa-check"></i><b>5.2.1</b> Creating a GitHub repository</a></li>
<li class="chapter" data-level="5.2.2" data-path="GitHub.html"><a href="GitHub.html#exploring-a-github-repository"><i class="fa fa-check"></i><b>5.2.2</b> Exploring a GitHub repository</a></li>
<li class="chapter" data-level="5.2.3" data-path="GitHub.html"><a href="GitHub.html#directly-editing-files-on-github"><i class="fa fa-check"></i><b>5.2.3</b> Directly editing files on GitHub</a></li>
<li class="chapter" data-level="5.2.4" data-path="GitHub.html"><a href="GitHub.html#logging-changes-and-pushing-them-to-github"><i class="fa fa-check"></i><b>5.2.4</b> Logging changes and pushing them to GitHub</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="GitHub.html"><a href="GitHub.html#git-command-cheatsheet"><i class="fa fa-check"></i><b>5.3</b> Git command cheatsheet</a><ul>
<li class="chapter" data-level="5.3.1" data-path="GitHub.html"><a href="GitHub.html#getting-a-repository-from-github-onto-the-server-for-the-first-time"><i class="fa fa-check"></i><b>5.3.1</b> Getting a repository from GitHub onto the server for the first time</a></li>
<li class="chapter" data-level="5.3.2" data-path="GitHub.html"><a href="GitHub.html#logging-changes"><i class="fa fa-check"></i><b>5.3.2</b> Logging changes</a></li>
<li class="chapter" data-level="5.3.3" data-path="GitHub.html"><a href="GitHub.html#sending-your-changes-back-to-github"><i class="fa fa-check"></i><b>5.3.3</b> Sending your changes back to GitHub</a></li>
<li class="chapter" data-level="5.3.4" data-path="GitHub.html"><a href="GitHub.html#getting-changes"><i class="fa fa-check"></i><b>5.3.4</b> Getting changes</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="GitHub.html"><a href="GitHub.html#terminal-cheatsheet"><i class="fa fa-check"></i><b>5.4</b> Terminal cheatsheet</a><ul>
<li class="chapter" data-level="5.4.1" data-path="GitHub.html"><a href="GitHub.html#see-where-you-are"><i class="fa fa-check"></i><b>5.4.1</b> See where you are:</a></li>
<li class="chapter" data-level="5.4.2" data-path="GitHub.html"><a href="GitHub.html#see-what-is-inside-the-directory-where-you-are"><i class="fa fa-check"></i><b>5.4.2</b> See what is inside the directory where you are:</a></li>
<li class="chapter" data-level="5.4.3" data-path="GitHub.html"><a href="GitHub.html#move-to-a-different-directory"><i class="fa fa-check"></i><b>5.4.3</b> Move to a different directory</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>6</b> Classification</a><ul>
<li class="chapter" data-level="6.1" data-path="classification.html"><a href="classification.html#overview-4"><i class="fa fa-check"></i><b>6.1</b> Overview</a></li>
<li class="chapter" data-level="6.2" data-path="classification.html"><a href="classification.html#learning-objectives"><i class="fa fa-check"></i><b>6.2</b> Learning objectives</a></li>
<li class="chapter" data-level="6.3" data-path="classification.html"><a href="classification.html#classification-1"><i class="fa fa-check"></i><b>6.3</b> Classification</a></li>
<li class="chapter" data-level="6.4" data-path="classification.html"><a href="classification.html#wisconsin-breast-cancer-example"><i class="fa fa-check"></i><b>6.4</b> Wisconsin Breast Cancer Example:</a><ul>
<li class="chapter" data-level="6.4.1" data-path="classification.html"><a href="classification.html#data-exploration"><i class="fa fa-check"></i><b>6.4.1</b> Data Exploration</a></li>
<li class="chapter" data-level="6.4.2" data-path="classification.html"><a href="classification.html#k-nearest-neighbour-classifier"><i class="fa fa-check"></i><b>6.4.2</b> K-Nearest Neighbour Classifier</a></li>
<li class="chapter" data-level="6.4.3" data-path="classification.html"><a href="classification.html#k-nearest-neighbours-in-r"><i class="fa fa-check"></i><b>6.4.3</b> K-Nearest Neighbours in R</a></li>
<li class="chapter" data-level="6.4.4" data-path="classification.html"><a href="classification.html#more-than-two-explanatory-variablespredictors"><i class="fa fa-check"></i><b>6.4.4</b> More than two explanatory variables/predictors</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="classification.html"><a href="classification.html#additional-readingsresources-2"><i class="fa fa-check"></i><b>6.5</b> Additional readings/resources</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification-continued.html"><a href="classification-continued.html"><i class="fa fa-check"></i><b>7</b> Classification continued</a><ul>
<li class="chapter" data-level="7.1" data-path="classification-continued.html"><a href="classification-continued.html#overview-5"><i class="fa fa-check"></i><b>7.1</b> Overview</a></li>
<li class="chapter" data-level="7.2" data-path="classification-continued.html"><a href="classification-continued.html#learning-objectives-1"><i class="fa fa-check"></i><b>7.2</b> Learning objectives</a></li>
<li class="chapter" data-level="7.3" data-path="classification-continued.html"><a href="classification-continued.html#assessing-how-good-your-classifier-is"><i class="fa fa-check"></i><b>7.3</b> Assessing how good your classifier is</a><ul>
<li class="chapter" data-level="7.3.1" data-path="classification-continued.html"><a href="classification-continued.html#assessing-your-classifier-in-r"><i class="fa fa-check"></i><b>7.3.1</b> Assessing your classifier in R</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="classification-continued.html"><a href="classification-continued.html#cross-validation-for-assessing-classifier-quality"><i class="fa fa-check"></i><b>7.4</b> Cross-validation for assessing classifier quality</a></li>
<li class="chapter" data-level="7.5" data-path="classification-continued.html"><a href="classification-continued.html#choosing-the-number-of-neighbours-for-k-nn-classification"><i class="fa fa-check"></i><b>7.5</b> Choosing the number of neighbours for k-nn classification</a></li>
<li class="chapter" data-level="7.6" data-path="classification-continued.html"><a href="classification-continued.html#other-ways-to-increase-accuracy"><i class="fa fa-check"></i><b>7.6</b> Other ways to increase accuracy</a></li>
<li class="chapter" data-level="7.7" data-path="classification-continued.html"><a href="classification-continued.html#test-data-set"><i class="fa fa-check"></i><b>7.7</b> Test data set</a></li>
<li class="chapter" data-level="7.8" data-path="classification-continued.html"><a href="classification-continued.html#scaling-your-data"><i class="fa fa-check"></i><b>7.8</b> Scaling your data</a></li>
<li class="chapter" data-level="7.9" data-path="classification-continued.html"><a href="classification-continued.html#strengths-and-limitations-of-k-nn-classification"><i class="fa fa-check"></i><b>7.9</b> Strengths and limitations of k-nn classification</a><ul>
<li class="chapter" data-level="7.9.1" data-path="classification-continued.html"><a href="classification-continued.html#strengths-of-k-nn-classification"><i class="fa fa-check"></i><b>7.9.1</b> Strengths of k-nn classification</a></li>
<li class="chapter" data-level="7.9.2" data-path="classification-continued.html"><a href="classification-continued.html#limitations-of-k-nn-classification"><i class="fa fa-check"></i><b>7.9.2</b> Limitations of k-nn classification</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="classification-continued.html"><a href="classification-continued.html#additional-readingsresources-3"><i class="fa fa-check"></i><b>7.10</b> Additional readings/resources</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regression1.html"><a href="regression1.html"><i class="fa fa-check"></i><b>8</b> Introduction to regression through K-nearest neighbours</a><ul>
<li class="chapter" data-level="8.1" data-path="regression1.html"><a href="regression1.html#overview-6"><i class="fa fa-check"></i><b>8.1</b> Overview</a></li>
<li class="chapter" data-level="8.2" data-path="regression1.html"><a href="regression1.html#learning-objectives-2"><i class="fa fa-check"></i><b>8.2</b> Learning objectives</a></li>
<li class="chapter" data-level="8.3" data-path="regression1.html"><a href="regression1.html#regression"><i class="fa fa-check"></i><b>8.3</b> Regression</a></li>
<li class="chapter" data-level="8.4" data-path="regression1.html"><a href="regression1.html#sacremento-real-estate-example"><i class="fa fa-check"></i><b>8.4</b> Sacremento real estate example</a></li>
<li class="chapter" data-level="8.5" data-path="regression1.html"><a href="regression1.html#k-nearest-neighbours-regression"><i class="fa fa-check"></i><b>8.5</b> K-nearest neighbours regression</a></li>
<li class="chapter" data-level="8.6" data-path="regression1.html"><a href="regression1.html#assessing-a-knn-regression-model"><i class="fa fa-check"></i><b>8.6</b> Assessing a knn regression model</a></li>
<li class="chapter" data-level="8.7" data-path="regression1.html"><a href="regression1.html#how-do-different-ks-affect-k-nn-regression-predictions"><i class="fa fa-check"></i><b>8.7</b> How do different k’s affect k-nn regression predictions</a></li>
<li class="chapter" data-level="8.8" data-path="regression1.html"><a href="regression1.html#assessing-model-goodness-with-the-test-set"><i class="fa fa-check"></i><b>8.8</b> Assessing model goodness with the test set</a></li>
<li class="chapter" data-level="8.9" data-path="regression1.html"><a href="regression1.html#strengths-and-limitations-of-k-nn-regression"><i class="fa fa-check"></i><b>8.9</b> Strengths and limitations of k-nn regression</a><ul>
<li class="chapter" data-level="8.9.1" data-path="regression1.html"><a href="regression1.html#strengths-of-k-nn-regression"><i class="fa fa-check"></i><b>8.9.1</b> Strengths of k-nn regression</a></li>
<li class="chapter" data-level="8.9.2" data-path="regression1.html"><a href="regression1.html#limitations-of-k-nn-regression"><i class="fa fa-check"></i><b>8.9.2</b> Limitations of k-nn regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regression2.html"><a href="regression2.html"><i class="fa fa-check"></i><b>9</b> Regression, continued</a><ul>
<li class="chapter" data-level="9.1" data-path="regression2.html"><a href="regression2.html#overview-7"><i class="fa fa-check"></i><b>9.1</b> Overview</a></li>
<li class="chapter" data-level="9.2" data-path="regression2.html"><a href="regression2.html#learning-objectives-3"><i class="fa fa-check"></i><b>9.2</b> Learning objectives</a></li>
<li class="chapter" data-level="9.3" data-path="regression2.html"><a href="regression2.html#rmse-versus-rmpse"><i class="fa fa-check"></i><b>9.3</b> <span class="math inline">\(RMSE\)</span> versus <span class="math inline">\(RMPSE\)</span></a></li>
<li class="chapter" data-level="9.4" data-path="regression2.html"><a href="regression2.html#linear-regression"><i class="fa fa-check"></i><b>9.4</b> Linear regression</a></li>
<li class="chapter" data-level="9.5" data-path="regression2.html"><a href="regression2.html#linear-regression-in-r-using-caret"><i class="fa fa-check"></i><b>9.5</b> Linear regression in R using <code>caret</code></a></li>
<li class="chapter" data-level="9.6" data-path="regression2.html"><a href="regression2.html#comparing-linear-and-k-nn-regression"><i class="fa fa-check"></i><b>9.6</b> Comparing linear and k-nn regression</a></li>
<li class="chapter" data-level="9.7" data-path="regression2.html"><a href="regression2.html#additional-readingsresources-4"><i class="fa fa-check"></i><b>9.7</b> Additional readings/resources</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="regression3.html"><a href="regression3.html"><i class="fa fa-check"></i><b>10</b> Regression, continued some more…</a><ul>
<li class="chapter" data-level="10.1" data-path="regression3.html"><a href="regression3.html#overview-8"><i class="fa fa-check"></i><b>10.1</b> Overview</a></li>
<li class="chapter" data-level="10.2" data-path="regression3.html"><a href="regression3.html#learning-objectives-4"><i class="fa fa-check"></i><b>10.2</b> Learning objectives</a></li>
<li class="chapter" data-level="10.3" data-path="regression3.html"><a href="regression3.html#multivariate-k-nn-regression"><i class="fa fa-check"></i><b>10.3</b> Multivariate k-nn regression</a></li>
<li class="chapter" data-level="10.4" data-path="regression3.html"><a href="regression3.html#multivariate-linear-regression"><i class="fa fa-check"></i><b>10.4</b> Multivariate linear regression</a></li>
<li class="chapter" data-level="10.5" data-path="regression3.html"><a href="regression3.html#the-other-side-of-regression"><i class="fa fa-check"></i><b>10.5</b> The other side of regression</a></li>
<li class="chapter" data-level="10.6" data-path="regression3.html"><a href="regression3.html#additional-readingsresources-5"><i class="fa fa-check"></i><b>10.6</b> Additional readings/resources</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>11</b> Clustering</a><ul>
<li class="chapter" data-level="11.1" data-path="clustering.html"><a href="clustering.html#overview-9"><i class="fa fa-check"></i><b>11.1</b> Overview</a></li>
<li class="chapter" data-level="11.2" data-path="clustering.html"><a href="clustering.html#learning-objectives-5"><i class="fa fa-check"></i><b>11.2</b> Learning objectives</a></li>
<li class="chapter" data-level="11.3" data-path="clustering.html"><a href="clustering.html#clustering-1"><i class="fa fa-check"></i><b>11.3</b> Clustering</a><ul>
<li class="chapter" data-level="11.3.1" data-path="clustering.html"><a href="clustering.html#a-toy-example"><i class="fa fa-check"></i><b>11.3.1</b> A toy example</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="clustering.html"><a href="clustering.html#k-means-clustering-algorithm"><i class="fa fa-check"></i><b>11.4</b> K-means clustering algorithm</a></li>
<li class="chapter" data-level="11.5" data-path="clustering.html"><a href="clustering.html#k-means-clustering-in-r"><i class="fa fa-check"></i><b>11.5</b> K-means clustering in R</a></li>
<li class="chapter" data-level="11.6" data-path="clustering.html"><a href="clustering.html#choosing-k-for-k-means-clustering"><i class="fa fa-check"></i><b>11.6</b> Choosing K for K-means clustering</a></li>
<li class="chapter" data-level="11.7" data-path="clustering.html"><a href="clustering.html#additional-readings"><i class="fa fa-check"></i><b>11.7</b> Additional readings:</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification_continued" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Classification continued</h1>
<div id="overview-5" class="section level2">
<h2><span class="header-section-number">7.1</span> Overview</h2>
<p>Metrics for classification accuracy; cross-validation to choose the number of neighbours; scaling of variables and other practical considerations.</p>
</div>
<div id="learning-objectives-1" class="section level2">
<h2><span class="header-section-number">7.2</span> Learning objectives</h2>
<p>By the end of the chapter, students will be able to:</p>
<ul>
<li>Describe what a validation data set is and how it is used in classification.</li>
<li>Using R, evaluate classification accuracy using a validation data set and appropriate metrics.</li>
<li>Using R, execute cross-validation in R to choose the number of neighbours.</li>
<li>Identify when it is necessary to scale variables before classification and do this using R</li>
<li>In a dataset with &gt; 2 attributes, perform k-nearest neighbour classification in R using <code>caret::train(method = "knn", ...)</code> to predict the class of a test dataset.</li>
<li>Describe advantages and disadvantages of the k-nearest neighbour classification algorithm.</li>
</ul>
</div>
<div id="assessing-how-good-your-classifier-is" class="section level2">
<h2><span class="header-section-number">7.3</span> Assessing how good your classifier is</h2>
<p>Sometimes our classifier might make the wrong prediction. A classifier does not need to be right 100% of the time to be useful, though we don’t want the classifier to make too many wrong predictions. How do we measure how “good” our classifier is?</p>
<p>One way to assess our classifier’s performance can be done by splitting our data into a <strong>training</strong> set and a <strong>validation</strong> set. When we split the data, we make the assumption that there is no order to our originally collected data set. However, if we think that there might be some order to the original data set, then we can randomly shuffle the data before splitting it into a training and validation set.</p>
<p><img src="img/training_validation.jpeg" width="600" /></p>
<p>The <strong>training set</strong> is used to build the classifer. Then we can give the observations from the <strong>validation set</strong> (without the labels/classes) to our classifier and predict the labels/classes as if these were new observations that we didn’t have the labels/classes for. Then we can see how well our predictions match the true labels/classes for the observations in the <strong>validation set</strong>. If our predictions match the true labels/classes for the observations in the <strong>validation set</strong> very well then we have some confidence that our classifier might also do a good job of predicting the class labels for new observations that we do not have the class labels for.</p>
<p>How exactly can we assess how well our predictions match the true labels/classes for the observations in the <strong>validation set</strong>? One way we can do this is to calculate the prediction <strong>accuracy</strong>. This is essentially the proportion of time the classifier was correct. To calculate this we divide the number of correct predictions by the number of predictions made. Other measures for how well our classifier did include precision and recall (which will not be discussed here, but are discussed in other more advanced courses on this topic).</p>
<p>We try to illustrate this below:</p>
<p><img src="img/ML-paradigm.png" width="800" /></p>
<div id="assessing-your-classifier-in-r" class="section level3">
<h3><span class="header-section-number">7.3.1</span> Assessing your classifier in R</h3>
<p>We can use the <code>caret</code> package in R to not only perform k-nn classification, but also to assess how well our classification worked. Let’s start by loading the necessary libraries, data (we’ll continue exploring the breast cancer data set from last chapter) and making a quick scatter plot of tumour cell concavity versus smoothness, labelling the points be diagnosis class.</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb119-1" title="1"><span class="co"># load libraries</span></a>
<a class="sourceLine" id="cb119-2" title="2"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb119-3" title="3"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb119-4" title="4"></a>
<a class="sourceLine" id="cb119-5" title="5"><span class="co">#load data</span></a>
<a class="sourceLine" id="cb119-6" title="6">cancer &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/clean-wdbc.data.csv&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb119-7" title="7"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Class =</span> <span class="kw">as.factor</span>(Class)) <span class="co"># because we will be doing statistical analysis on a categorical variable</span></a>
<a class="sourceLine" id="cb119-8" title="8"></a>
<a class="sourceLine" id="cb119-9" title="9"><span class="co"># colour palette</span></a>
<a class="sourceLine" id="cb119-10" title="10">cbPalette &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;#56B4E9&quot;</span>, <span class="st">&quot;#E69F00&quot;</span>,<span class="st">&quot;#009E73&quot;</span>, <span class="st">&quot;#F0E442&quot;</span>, <span class="st">&quot;#0072B2&quot;</span>, <span class="st">&quot;#D55E00&quot;</span>, <span class="st">&quot;#CC79A7&quot;</span>, <span class="st">&quot;#999999&quot;</span>) </a>
<a class="sourceLine" id="cb119-11" title="11"></a>
<a class="sourceLine" id="cb119-12" title="12"><span class="co"># create scatter plot of tumour cell concavity versus smoothness, </span></a>
<a class="sourceLine" id="cb119-13" title="13"><span class="co"># labelling the points be diagnosis class</span></a>
<a class="sourceLine" id="cb119-14" title="14">perim_concav &lt;-<span class="st"> </span>cancer <span class="op">%&gt;%</span><span class="st">  </span></a>
<a class="sourceLine" id="cb119-15" title="15"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Smoothness, <span class="dt">y =</span> Concavity, <span class="dt">color =</span> Class)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb119-16" title="16"><span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.5</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb119-17" title="17"><span class="st">    </span><span class="kw">labs</span>(<span class="dt">color =</span> <span class="st">&quot;Diagnosis&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb119-18" title="18"><span class="st">    </span><span class="kw">scale_color_manual</span>(<span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;Benign&quot;</span>, <span class="st">&quot;Malignant&quot;</span>), <span class="dt">values =</span> cbPalette)</a>
<a class="sourceLine" id="cb119-19" title="19">perim_concav</a></code></pre></div>
<p><img src="_main_files/figure-html/06-precode-1.png" width="480" /></p>
<div id="splitting-into-training-and-validation-sets" class="section level4">
<h4><span class="header-section-number">7.3.1.1</span> Splitting into training and validation sets</h4>
<p>Next, lets split our data into a training and a validation set using <code>caret</code>’s <code>createDataPartition</code> function. When using this function to split a data set into a training and validation set it takes 3 arguments:</p>
<ol style="list-style-type: decimal">
<li><code>y</code> (the class labels, must be a vector),</li>
<li><code>p</code> (the proportion of the data you would like in the training data set), and</li>
<li><code>list = FALSE</code> (says we want the data back as a matrix instead of a list).</li>
</ol>
<p>The <code>createDataPartition</code> function returns the row numbers for the training set.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb120-1" title="1"><span class="kw">set.seed</span>(<span class="dv">1234</span>) <span class="co"># makes the random selection of rows reproducible</span></a>
<a class="sourceLine" id="cb120-2" title="2">set_rows &lt;-<span class="st"> </span>cancer <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb120-3" title="3"><span class="st">  </span><span class="kw">select</span>(Class) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb120-4" title="4"><span class="st">  </span><span class="kw">unlist</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># converts Class from a tibble to a vector</span></a>
<a class="sourceLine" id="cb120-5" title="5"><span class="st">  </span><span class="kw">createDataPartition</span>(<span class="dt">p =</span> <span class="fl">0.75</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb120-6" title="6"><span class="kw">head</span>(set_rows)</a></code></pre></div>
<pre><code>##      Resample1
## [1,]         1
## [2,]         3
## [3,]         4
## [4,]         6
## [5,]        10
## [6,]        11</code></pre>
<p><em>You will also see in the code above that we use the <code>set.seed</code> function. This is because <code>createDataPartition</code> uses random sampling to choose which rows will be in the training set, and if we use <code>set.seed</code> to specify where the random number generator starts for this process then we can make our analysis reproducible (always get the same random set of observations in the training set). We should always set a seed before any function that uses a random process. We’ll point out where as we work through this code.</em></p>
<p>Now that we have the row numbers for the training set, we can use the <code>slice</code> function to get the rows from the original data set (here <code>cancer</code>) to create the training set and the validation sets.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb122-1" title="1">training_set &lt;-<span class="st"> </span>cancer <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(set_rows)</a>
<a class="sourceLine" id="cb122-2" title="2">validation_set &lt;-<span class="st"> </span>cancer <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="op">-</span>set_rows)</a>
<a class="sourceLine" id="cb122-3" title="3"><span class="kw">glimpse</span>(training_set)</a></code></pre></div>
<pre><code>## Observations: 427
## Variables: 12
## $ ID                &lt;dbl&gt; 842302, 84300903, 84348301, 843786, 84501001...
## $ Class             &lt;fct&gt; M, M, M, M, M, M, M, M, M, M, M, M, M, B, M,...
## $ Radius            &lt;dbl&gt; 1.88503100, 1.51054113, -0.28121702, -0.1653...
## $ Texture           &lt;dbl&gt; -1.35809849, -0.02395331, 0.13386631, -0.313...
## $ Perimeter         &lt;dbl&gt; 2.30157548, 1.34629062, -0.24971958, -0.1149...
## $ Area              &lt;dbl&gt; 1.999478159, 1.455004298, -0.549537693, -0.2...
## $ Smoothness        &lt;dbl&gt; 1.30653666, 0.52694375, 3.39129072, 2.046711...
## $ Compactness       &lt;dbl&gt; 2.61436466, 1.08198014, 3.88997467, 1.720102...
## $ Concavity         &lt;dbl&gt; 2.10767182, 0.85422232, 1.98783917, 1.262132...
## $ Concave_points    &lt;dbl&gt; 2.29405760, 1.95328166, 2.17387323, 0.905091...
## $ Symmetry          &lt;dbl&gt; 2.7482041, 1.1512420, 6.0407261, 1.7525273, ...
## $ Fractal_dimension &lt;dbl&gt; 1.93531174, 0.20121416, 4.93067187, 2.239830...</code></pre>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb124-1" title="1"><span class="kw">glimpse</span>(validation_set)</a></code></pre></div>
<pre><code>## Observations: 142
## Variables: 12
## $ ID                &lt;dbl&gt; 842517, 84358402, 844359, 84458202, 844981, ...
## $ Class             &lt;fct&gt; M, M, M, M, M, M, B, B, M, M, M, M, M, M, B,...
## $ Radius            &lt;dbl&gt; 1.8043398, 1.2974336, 1.3677798, 0.1636190, ...
## $ Texture           &lt;dbl&gt; -0.368878647, -1.465480907, 0.322599039, 0.4...
## $ Perimeter         &lt;dbl&gt; 1.533776431, 1.337362721, 1.367122374, 0.099...
## $ Area              &lt;dbl&gt; 1.88882702, 1.21965108, 1.27409847, 0.028834...
## $ Smoothness        &lt;dbl&gt; -0.375281748, 0.220362270, 0.518184279, 1.44...
## $ Compactness       &lt;dbl&gt; -0.43006581, -0.31311900, 0.02119633, 0.7241...
## $ Concavity         &lt;dbl&gt; -0.146619958, 0.612639700, 0.509104292, -0.0...
## $ Concave_points    &lt;dbl&gt; 1.0861286, 0.7286181, 1.1956637, 0.6236470, ...
## $ Symmetry          &lt;dbl&gt; -0.2436753, -0.8675896, 0.2622449, 0.4772206...
## $ Fractal_dimension &lt;dbl&gt; 0.28094279, -0.39675052, -0.01471753, 1.7249...</code></pre>
<p>We can see from <code>glimpse</code> in the code above that the training set contains 427 observations, while the validation set contains 142 observations. This corresponds to the training set having 75% of the observations from the original data set and the validation set having the other 25% of the observations. We specified this when we provided the argument <code>p = 0.75</code> to <code>createDataPartition</code>.</p>
</div>
<div id="creating-the-k-nn-classifier" class="section level4">
<h4><span class="header-section-number">7.3.1.2</span> Creating the k-nn classifier</h4>
<p>Now that we have split our original data set into a training and validation set, we can create our k-nn classifier using the training set. We explained how to do this last chapter, so here we just do it! For the time being we will just choose a single <span class="math inline">\(k\)</span> of 3, and use the predictors concavity and smoothness.</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb126-1" title="1">X_train &lt;-<span class="st"> </span>training_set <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb126-2" title="2"><span class="st">  </span><span class="kw">select</span>(Concavity, Smoothness) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb126-3" title="3"><span class="st">  </span><span class="kw">data.frame</span>()</a>
<a class="sourceLine" id="cb126-4" title="4">Y_train &lt;-<span class="st"> </span>training_set <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb126-5" title="5"><span class="st">  </span><span class="kw">select</span>(Class) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb126-6" title="6"><span class="st">  </span><span class="kw">unlist</span>()</a>
<a class="sourceLine" id="cb126-7" title="7">k =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb126-8" title="8"></a>
<a class="sourceLine" id="cb126-9" title="9"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb126-10" title="10">model_knn &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> X_train, <span class="dt">y =</span> Y_train, <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="dt">tuneGrid =</span> k)</a>
<a class="sourceLine" id="cb126-11" title="11">model_knn</a></code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 427 samples
##   2 predictor
##   2 classes: &#39;B&#39;, &#39;M&#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 427, 427, 427, 427, 427, 427, ... 
## Resampling results:
## 
##   Accuracy  Kappa    
##   0.804416  0.5842458
## 
## Tuning parameter &#39;k&#39; was held constant at a value of 3</code></pre>
<p><em>We set the seed above calling the train function because if there were a tie, the winning class is randomly chosen. In k-nn classification where <span class="math inline">\(k\)</span> = 3 and there are only 2 classes this will never happen, however, if our <span class="math inline">\(k\)</span> was divisible by 2, or if we had more than two possible class labels we could find ourselves in a situtation where we do have a tie.</em></p>
</div>
<div id="predict-class-labels-for-the-validation-set" class="section level4">
<h4><span class="header-section-number">7.3.1.3</span> Predict class labels for the validation set</h4>
<p>Now that we have a k-nn classifier object, we can use it to predict the class labels for our validation set:</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb128-1" title="1">X_validation &lt;-<span class="st"> </span>validation_set <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb128-2" title="2"><span class="st">  </span><span class="kw">select</span>(Concavity, Smoothness) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb128-3" title="3"><span class="st">  </span><span class="kw">data.frame</span>()</a>
<a class="sourceLine" id="cb128-4" title="4">Y_validation_predicted &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object =</span> model_knn, X_validation)</a>
<a class="sourceLine" id="cb128-5" title="5"><span class="kw">head</span>(Y_validation_predicted)</a></code></pre></div>
<pre><code>## [1] B M M M M M
## Levels: B M</code></pre>
</div>
<div id="assessing-our-classifiers-accuracy" class="section level4">
<h4><span class="header-section-number">7.3.1.4</span> Assessing our classifier’s accuracy</h4>
<p>Finally we can assess our classifier’s accuracy. To do this we need to create a vector containing the class labels for the validation set. Next we use the function <code>confusionMatrix</code> to get the statistics about the quality of our model, this includes the statistic we are interested: accuracy. <code>confusionMatrix</code> takes two arguments:</p>
<ol style="list-style-type: decimal">
<li><code>data</code> (the predicted class labels for the validation set), and</li>
<li><code>reference</code> (the original/measured class labels for the validation set).</li>
</ol>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb130-1" title="1">Y_validation &lt;-<span class="st"> </span>validation_set <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb130-2" title="2"><span class="st">  </span><span class="kw">select</span>(Class) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb130-3" title="3"><span class="st">  </span><span class="kw">unlist</span>()</a>
<a class="sourceLine" id="cb130-4" title="4"></a>
<a class="sourceLine" id="cb130-5" title="5">model_quality &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="dt">data =</span> Y_validation_predicted, <span class="dt">reference =</span> Y_validation)</a>
<a class="sourceLine" id="cb130-6" title="6">model_quality</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  B  M
##          B 79 18
##          M 10 35
##                                           
##                Accuracy : 0.8028          
##                  95% CI : (0.7278, 0.8648)
##     No Information Rate : 0.6268          
##     P-Value [Acc &gt; NIR] : 4.43e-06        
##                                           
##                   Kappa : 0.5653          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.1859          
##                                           
##             Sensitivity : 0.8876          
##             Specificity : 0.6604          
##          Pos Pred Value : 0.8144          
##          Neg Pred Value : 0.7778          
##              Prevalence : 0.6268          
##          Detection Rate : 0.5563          
##    Detection Prevalence : 0.6831          
##       Balanced Accuracy : 0.7740          
##                                           
##        &#39;Positive&#39; Class : B               
## </code></pre>
<p>A lot of information is output from <code>confusionMatrix</code>, but what we are interested in at this point is accuracy (found on the 6th line of printed output). That single value can be obtained from the <code>confusionMatrix</code> object using base/built-in R subsetting:</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb132-1" title="1">model_quality<span class="op">$</span>overall[<span class="dv">1</span>]</a></code></pre></div>
<pre><code>##  Accuracy 
## 0.8028169</code></pre>
<p>From a value of accuracy of 0.7957746, we can say that our k-nn classifier predicted the correct class label ~ 80% of the time.</p>
</div>
</div>
</div>
<div id="cross-validation-for-assessing-classifier-quality" class="section level2">
<h2><span class="header-section-number">7.4</span> Cross-validation for assessing classifier quality</h2>
<p>Is that the best estimate of accuracy that we can get? What would happen if we again shuffled the observations in our training and validation sets, would we get the same accuracy? Let’s do and experiment and see. By changing the <code>set.seed</code> value, we can get a different shuffle of the data when we create our training and validation data sets.</p>
<p>Using <code>set.seed(4321)</code></p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb134-1" title="1"><span class="kw">set.seed</span>(<span class="dv">4321</span>) <span class="co"># makes the random selection of rows reproducible</span></a>
<a class="sourceLine" id="cb134-2" title="2">set_rows &lt;-<span class="st"> </span>cancer <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb134-3" title="3"><span class="st">  </span><span class="kw">select</span>(Class) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb134-4" title="4"><span class="st">  </span><span class="kw">unlist</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># converts Class from a tibble to a vector</span></a>
<a class="sourceLine" id="cb134-5" title="5"><span class="st">  </span><span class="kw">createDataPartition</span>(<span class="dt">p =</span> <span class="fl">0.75</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb134-6" title="6">training_set &lt;-<span class="st"> </span>cancer <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(set_rows)</a>
<a class="sourceLine" id="cb134-7" title="7">validation_set &lt;-<span class="st"> </span>cancer <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="op">-</span>set_rows)</a>
<a class="sourceLine" id="cb134-8" title="8">X_train &lt;-<span class="st"> </span>training_set <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb134-9" title="9"><span class="st">  </span><span class="kw">select</span>(Concavity, Smoothness) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb134-10" title="10"><span class="st">  </span><span class="kw">data.frame</span>()</a>
<a class="sourceLine" id="cb134-11" title="11">Y_train &lt;-<span class="st"> </span>training_set <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb134-12" title="12"><span class="st">  </span><span class="kw">select</span>(Class) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb134-13" title="13"><span class="st">  </span><span class="kw">unlist</span>()</a>
<a class="sourceLine" id="cb134-14" title="14">k =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb134-15" title="15">model_knn &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> X_train, <span class="dt">y =</span> Y_train, <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="dt">tuneGrid =</span> k)</a>
<a class="sourceLine" id="cb134-16" title="16">X_validation &lt;-<span class="st"> </span>validation_set <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb134-17" title="17"><span class="st">  </span><span class="kw">select</span>(Concavity, Smoothness) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb134-18" title="18"><span class="st">  </span><span class="kw">data.frame</span>()</a>
<a class="sourceLine" id="cb134-19" title="19">Y_validation_predicted &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object =</span> model_knn, X_validation)</a>
<a class="sourceLine" id="cb134-20" title="20">Y_validation &lt;-<span class="st"> </span>validation_set <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb134-21" title="21"><span class="st">  </span><span class="kw">select</span>(Class) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb134-22" title="22"><span class="st">  </span><span class="kw">unlist</span>()</a>
<a class="sourceLine" id="cb134-23" title="23">model_quality &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="dt">data =</span> Y_validation_predicted, <span class="dt">reference =</span> Y_validation)</a>
<a class="sourceLine" id="cb134-24" title="24">model_quality<span class="op">$</span>overall[<span class="dv">1</span>]</a></code></pre></div>
<pre><code>##  Accuracy 
## 0.8450704</code></pre>
<p>Using <code>set.seed(8765)</code></p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb136-1" title="1"><span class="kw">set.seed</span>(<span class="dv">8765</span>) <span class="co"># makes the random selection of rows reproducible</span></a>
<a class="sourceLine" id="cb136-2" title="2">set_rows &lt;-<span class="st"> </span>cancer <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb136-3" title="3"><span class="st">  </span><span class="kw">select</span>(Class) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb136-4" title="4"><span class="st">  </span><span class="kw">unlist</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># converts Class from a tibble to a vector</span></a>
<a class="sourceLine" id="cb136-5" title="5"><span class="st">  </span><span class="kw">createDataPartition</span>(<span class="dt">p =</span> <span class="fl">0.75</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb136-6" title="6">training_set &lt;-<span class="st"> </span>cancer <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(set_rows)</a>
<a class="sourceLine" id="cb136-7" title="7">validation_set &lt;-<span class="st"> </span>cancer <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="op">-</span>set_rows)</a>
<a class="sourceLine" id="cb136-8" title="8">X_train &lt;-<span class="st"> </span>training_set <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb136-9" title="9"><span class="st">  </span><span class="kw">select</span>(Concavity, Smoothness) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb136-10" title="10"><span class="st">  </span><span class="kw">data.frame</span>()</a>
<a class="sourceLine" id="cb136-11" title="11">Y_train &lt;-<span class="st"> </span>training_set <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb136-12" title="12"><span class="st">  </span><span class="kw">select</span>(Class) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb136-13" title="13"><span class="st">  </span><span class="kw">unlist</span>()</a>
<a class="sourceLine" id="cb136-14" title="14">k =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb136-15" title="15">model_knn &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> X_train, <span class="dt">y =</span> Y_train, <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="dt">tuneGrid =</span> k)</a>
<a class="sourceLine" id="cb136-16" title="16">X_validation &lt;-<span class="st"> </span>validation_set <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb136-17" title="17"><span class="st">  </span><span class="kw">select</span>(Concavity, Smoothness) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb136-18" title="18"><span class="st">  </span><span class="kw">data.frame</span>()</a>
<a class="sourceLine" id="cb136-19" title="19">Y_validation_predicted &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object =</span> model_knn, X_validation)</a>
<a class="sourceLine" id="cb136-20" title="20">Y_validation &lt;-<span class="st"> </span>validation_set <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb136-21" title="21"><span class="st">  </span><span class="kw">select</span>(Class) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb136-22" title="22"><span class="st">  </span><span class="kw">unlist</span>()</a>
<a class="sourceLine" id="cb136-23" title="23">model_quality &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="dt">data =</span> Y_validation_predicted, <span class="dt">reference =</span> Y_validation)</a>
<a class="sourceLine" id="cb136-24" title="24">model_quality<span class="op">$</span>overall[<span class="dv">1</span>]</a></code></pre></div>
<pre><code>##  Accuracy 
## 0.8450704</code></pre>
<p>When we have 3 different shuffles of the data, we get 3 different values for accuracy! 0.7957746, 0.8661972 and 0.7676056! Which one is correct? Sadly, there is no easy answer to that question. The best we can do is to do this many times and take the average of the accuracies. Typically this is done is a more structured way so that each observation in the data set is used in a validation set only a single time. The name for this strategy is called cross-validation and we illustrate it below:</p>
<p><img src="img/cv.png" width="800" /></p>
<p>In the picture above, 5 different folds/partitions of the data set are shown, and consequently we call this 5-fold cross-validation. To do 5-fold cross-validation in R with <code>caret</code>, we use another function called <code>trainControl</code>. This function passes additional information to the <code>train</code> function we use to create our classifier. The arguments we pass <code>trainControl</code> are:</p>
<ol style="list-style-type: decimal">
<li><code>method</code> (method used for assessing classifier quality, here we specify <code>"cv"</code> for cross-validation)</li>
<li><code>number</code> (how many folds/partitions of the data set we want to use for cross validation)</li>
</ol>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb138-1" title="1">train_control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>)</a></code></pre></div>
<p>Then when we create our classifier we add an additional argument to <code>train</code>, called <code>trControl</code> where we give it the name of the object we created with the <code>trainControl</code> function. Additionally, we do not need to specify a training and a validation set because we are telling <code>train</code> that we are doing cross validation (it will take care creating the folds, calculating the accuracy for each fold and averaging the accuracies for us).</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb139-1" title="1">X_cancer &lt;-<span class="st"> </span>cancer <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb139-2" title="2"><span class="st">  </span><span class="kw">select</span>(Concavity, Smoothness) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb139-3" title="3"><span class="st">  </span><span class="kw">data.frame</span>()</a>
<a class="sourceLine" id="cb139-4" title="4">Y_cancer &lt;-<span class="st"> </span>cancer <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb139-5" title="5"><span class="st">  </span><span class="kw">select</span>(Class) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb139-6" title="6"><span class="st">  </span><span class="kw">unlist</span>()</a>
<a class="sourceLine" id="cb139-7" title="7">k =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb139-8" title="8"></a>
<a class="sourceLine" id="cb139-9" title="9"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb139-10" title="10">knn_model_cv_5fold &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> X_cancer, <span class="dt">y =</span> Y_cancer, <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="dt">tuneGrid =</span> k, <span class="dt">trControl =</span> train_control)</a>
<a class="sourceLine" id="cb139-11" title="11">knn_model_cv_5fold</a></code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 569 samples
##   2 predictor
##   2 classes: &#39;B&#39;, &#39;M&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 455, 456, 454, 456, 455 
## Resampling results:
## 
##   Accuracy   Kappa    
##   0.8349113  0.6492669
## 
## Tuning parameter &#39;k&#39; was held constant at a value of 3</code></pre>
<p><em>This time we set the seed when we call <code>train</code> not only because of the potential for ties, but also because we are doing cross-validation. Cross-validation uses a random process to select which observations are included in which folds.</em></p>
<p>We can choose any number of folds, typically the more the better. However we are limited by computational power. The more folds we choose, the more computation it takes, and hence the more time it takes to run the analysis. So for each time you do cross-validation you need to consider the size of the data, and the speed of the algorithm (here k-nn) and the speed of your computer. In practice this is a trial and error process. Here we show what happens when we do 10 folds:</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb141-1" title="1">train_control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb141-2" title="2"></a>
<a class="sourceLine" id="cb141-3" title="3"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb141-4" title="4">knn_model_cv_10fold &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> X_cancer, <span class="dt">y =</span> Y_cancer, <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="dt">tuneGrid =</span> k, <span class="dt">trControl =</span> train_control)</a>
<a class="sourceLine" id="cb141-5" title="5">knn_model_cv_10fold</a></code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 569 samples
##   2 predictor
##   2 classes: &#39;B&#39;, &#39;M&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 512, 513, 512, 512, 512, 512, ... 
## Resampling results:
## 
##   Accuracy   Kappa    
##   0.8399944  0.6590657
## 
## Tuning parameter &#39;k&#39; was held constant at a value of 3</code></pre>
</div>
<div id="choosing-the-number-of-neighbours-for-k-nn-classification" class="section level2">
<h2><span class="header-section-number">7.5</span> Choosing the number of neighbours for k-nn classification</h2>
<p>From 5- and 10-fold cross-validate we estimate that the prediction accuracy of our classifier to be ~ 83%. This could be not too bad of an accuracy, however what accuracy you aim for always depends on the downstream application of your analysis. Here, we are trying to predict a very important outcome, tumour cell diagnosis class. And the class label we assign to a real patient may have life or death consequences. Hence, we’d like to do better for this application than 83%. To do this we can use cross-validation in an even bigger way, we can choose a range of possible <span class="math inline">\(k\)</span>’s and perform cross-validation to calculate the accuracy for each <span class="math inline">\(k\)</span>, and then choose the smallest <span class="math inline">\(k\)</span> which gives us the best cross-validation accuracy. To do this, we will create a vector of values for <span class="math inline">\(k\)</span> instead of providing just 1.</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb143-1" title="1">train_control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb143-2" title="2">k =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">11</span>, <span class="dv">13</span>, <span class="dv">15</span>, <span class="dv">17</span>))</a>
<a class="sourceLine" id="cb143-3" title="3"></a>
<a class="sourceLine" id="cb143-4" title="4"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb143-5" title="5">knn_model_cv_10fold &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> X_cancer, <span class="dt">y =</span> Y_cancer, <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="dt">tuneGrid =</span> k, <span class="dt">trControl =</span> train_control)</a>
<a class="sourceLine" id="cb143-6" title="6">knn_model_cv_10fold</a></code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 569 samples
##   2 predictor
##   2 classes: &#39;B&#39;, &#39;M&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 512, 513, 512, 512, 512, 512, ... 
## Resampling results across tuning parameters:
## 
##   k   Accuracy   Kappa    
##    1  0.8046258  0.5846168
##    3  0.8399944  0.6590657
##    5  0.8398097  0.6584483
##    7  0.8398421  0.6586905
##    9  0.8504602  0.6803442
##   11  0.8627096  0.7053704
##   13  0.8573859  0.6958556
##   15  0.8433195  0.6663557
##   17  0.8433508  0.6671632
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was k = 11.</code></pre>
<p>Then to help us choose <span class="math inline">\(k\)</span> it is very useful to visualize the accuracies as we increase <span class="math inline">\(k\)</span>. This will help us choose the smallest <span class="math inline">\(k\)</span> with the biggest accuracy. We can access the results from the cross-validation by accessing the<code>results</code> attribute of the <code>train</code> object (our classifier).</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb145-1" title="1">accuracies &lt;-<span class="st"> </span>knn_model_cv_10fold<span class="op">$</span>results</a>
<a class="sourceLine" id="cb145-2" title="2">accuracies </a></code></pre></div>
<pre><code>##    k  Accuracy     Kappa AccuracySD    KappaSD
## 1  1 0.8046258 0.5846168 0.06665286 0.13868551
## 2  3 0.8399944 0.6590657 0.03869435 0.08621080
## 3  5 0.8398097 0.6584483 0.04150248 0.08499962
## 4  7 0.8398421 0.6586905 0.03766683 0.07633179
## 5  9 0.8504602 0.6803442 0.03528440 0.07609122
## 6 11 0.8627096 0.7053704 0.04312713 0.09343721
## 7 13 0.8573859 0.6958556 0.04824717 0.10298219
## 8 15 0.8433195 0.6663557 0.05234251 0.11017154
## 9 17 0.8433508 0.6671632 0.04514788 0.09477311</code></pre>
<p>Now we can plot accuracy versus k:</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb147-1" title="1">accuracy_vs_k &lt;-<span class="st"> </span><span class="kw">ggplot</span>(accuracies, <span class="kw">aes</span>(<span class="dt">x =</span> k, <span class="dt">y =</span> Accuracy)) <span class="op">+</span></a>
<a class="sourceLine" id="cb147-2" title="2"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb147-3" title="3"><span class="st">  </span><span class="kw">geom_line</span>()</a>
<a class="sourceLine" id="cb147-4" title="4">accuracy_vs_k</a></code></pre></div>
<p><img src="_main_files/figure-html/06-find-k-1.png" width="480" /></p>
<p>Based off of the visualization above we typically would choose <span class="math inline">\(k\)</span> to be ~ 11, given that at this value of <span class="math inline">\(k\)</span> our accuracy is a high as it can be with much larger values of <span class="math inline">\(k\)</span>. As you can see there is no exact or perfect answer here, what we are looking for is a value for <span class="math inline">\(k\)</span> where we get a roughly optimal increase of accuracy but at the same time we want to keep <span class="math inline">\(k\)</span> small.</p>
<p>Why do we want to keep <span class="math inline">\(k\)</span> small? Well this is because if we keep increasing <span class="math inline">\(k\)</span> our accuracy actually starts to decrease! Take a look as the plot below as we vary <span class="math inline">\(k\)</span> from 1 to almost the number of observations in the data set:</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb148-1" title="1">train_control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb148-2" title="2">k_lots =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">1</span>, <span class="dt">to =</span> <span class="dv">499</span>, <span class="dt">by =</span> <span class="dv">10</span>))</a>
<a class="sourceLine" id="cb148-3" title="3"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb148-4" title="4">knn_model_cv_10fold_lots &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> X_cancer, <span class="dt">y =</span> Y_cancer, <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="dt">tuneGrid =</span> k_lots, <span class="dt">trControl =</span> train_control)</a>
<a class="sourceLine" id="cb148-5" title="5">accuracies_lots &lt;-<span class="st"> </span>knn_model_cv_10fold_lots<span class="op">$</span>results</a>
<a class="sourceLine" id="cb148-6" title="6">accuracy_vs_k_lots &lt;-<span class="st"> </span><span class="kw">ggplot</span>(accuracies_lots, <span class="kw">aes</span>(<span class="dt">x =</span> k, <span class="dt">y =</span> Accuracy)) <span class="op">+</span></a>
<a class="sourceLine" id="cb148-7" title="7"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb148-8" title="8"><span class="st">  </span><span class="kw">geom_line</span>()</a>
<a class="sourceLine" id="cb148-9" title="9">accuracy_vs_k_lots</a></code></pre></div>
<p><img src="_main_files/figure-html/06-lots-of-ks-1.png" width="480" /></p>
</div>
<div id="other-ways-to-increase-accuracy" class="section level2">
<h2><span class="header-section-number">7.6</span> Other ways to increase accuracy</h2>
<p>By using cross-validation to choose <span class="math inline">\(k\)</span> we were able to slightly increase our accuracy, but can we still do better? Perhaps. We can start to explore this by taking a look at what is called the training accuracy. Training accuracy is our accuracy if we asked our classifier to make predictions on the training data and then we assessed how well the predictions matched up to the true labels we have for our training data. If they don’t match up really well (training accuracy is low), our classification model might be too simple and adding more information (e.g., additional predictors/explanatory variables) could potentially help. The situation where the training accuracy is low is often called underfitting, or high bias.</p>
<p>The training error can be obtained from using the classifier object returned from <code>train</code> (when you don’t perform cross-validation) to predict on the training data. Then passing the predictions on the training data and the true observed labels into <code>confusionMatrix</code>.</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb149-1" title="1">k =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="dv">11</span>)</a>
<a class="sourceLine" id="cb149-2" title="2"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb149-3" title="3">knn_model &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> X_cancer, <span class="dt">y =</span> Y_cancer, <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="dt">tuneGrid =</span> k)</a>
<a class="sourceLine" id="cb149-4" title="4">training_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(knn_model, X_cancer)</a>
<a class="sourceLine" id="cb149-5" title="5">results &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(training_pred, Y_cancer)</a>
<a class="sourceLine" id="cb149-6" title="6">results</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   B   M
##          B 322  36
##          M  35 176
##                                           
##                Accuracy : 0.8752          
##                  95% CI : (0.8452, 0.9012)
##     No Information Rate : 0.6274          
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.7329          
##                                           
##  Mcnemar&#39;s Test P-Value : 1               
##                                           
##             Sensitivity : 0.9020          
##             Specificity : 0.8302          
##          Pos Pred Value : 0.8994          
##          Neg Pred Value : 0.8341          
##              Prevalence : 0.6274          
##          Detection Rate : 0.5659          
##    Detection Prevalence : 0.6292          
##       Balanced Accuracy : 0.8661          
##                                           
##        &#39;Positive&#39; Class : B               
## </code></pre>
<p>From the complex output, we can see the training accuracy, here 0.8752. Again we can use base/built-in subsetting syntax to directly get the value:</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb151-1" title="1">results<span class="op">$</span>overall[<span class="dv">1</span>]</a></code></pre></div>
<pre><code>##  Accuracy 
## 0.8752197</code></pre>
<p>Here we see that our training accuracy is high, 0.8752197, but there is still room for improvement! (If it were 1.0 there wouldn’t be and we would have a different problem). So let’s see if adding additional information (predictors/explanatory variables) might help our model training and consequently (and more importantly) validation accuracy.</p>
<p><em>Note - when adding more information to the model, <span class="math inline">\(k\)</span> = 11 may no longer be the “best” <span class="math inline">\(k\)</span>. So we will want to also choose <span class="math inline">\(k\)</span> again.</em></p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb153-1" title="1"><span class="co"># set-up training data</span></a>
<a class="sourceLine" id="cb153-2" title="2">X_cancer_all &lt;-<span class="st"> </span>cancer <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb153-3" title="3"><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>Class, <span class="op">-</span>ID) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb153-4" title="4"><span class="st">  </span><span class="kw">data.frame</span>()</a>
<a class="sourceLine" id="cb153-5" title="5">Y_cancer_all &lt;-<span class="st"> </span>cancer <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb153-6" title="6"><span class="st">  </span><span class="kw">select</span>(Class) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb153-7" title="7"><span class="st">  </span><span class="kw">unlist</span>()</a>
<a class="sourceLine" id="cb153-8" title="8"></a>
<a class="sourceLine" id="cb153-9" title="9"><span class="co"># set-up classifier specifications</span></a>
<a class="sourceLine" id="cb153-10" title="10">train_control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb153-11" title="11">k =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">1</span>, <span class="dt">to =</span> <span class="dv">29</span>, <span class="dt">by =</span> <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb153-12" title="12"></a>
<a class="sourceLine" id="cb153-13" title="13"><span class="co"># create classifier</span></a>
<a class="sourceLine" id="cb153-14" title="14"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb153-15" title="15">knn_model_all &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> X_cancer_all, <span class="dt">y =</span> Y_cancer_all, <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="dt">tuneGrid =</span> k, <span class="dt">trControl =</span> train_control)</a>
<a class="sourceLine" id="cb153-16" title="16"></a>
<a class="sourceLine" id="cb153-17" title="17"><span class="co"># assess training accuracy</span></a>
<a class="sourceLine" id="cb153-18" title="18">training_pred_all &lt;-<span class="st"> </span><span class="kw">predict</span>(knn_model_all, X_cancer_all)</a>
<a class="sourceLine" id="cb153-19" title="19">results_all &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(training_pred_all, Y_cancer_all)</a>
<a class="sourceLine" id="cb153-20" title="20">results_all<span class="op">$</span>overall[<span class="dv">1</span>]</a></code></pre></div>
<pre><code>##  Accuracy 
## 0.9683656</code></pre>
<p>We can see that by including more information (making our classifier more complex by adding additional predictors/explanatory variables) we increased the training accuracy. What about the cross-validation accuracy? And what <span class="math inline">\(k\)</span> should we choose?</p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb155-1" title="1">accuracies_all &lt;-<span class="st"> </span>knn_model_all</a>
<a class="sourceLine" id="cb155-2" title="2">accuracy_vs_k_all &lt;-<span class="st"> </span><span class="kw">ggplot</span>(accuracies_all, <span class="kw">aes</span>(<span class="dt">x =</span> k, <span class="dt">y =</span> Accuracy)) <span class="op">+</span></a>
<a class="sourceLine" id="cb155-3" title="3"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb155-4" title="4"><span class="st">  </span><span class="kw">geom_line</span>()</a>
<a class="sourceLine" id="cb155-5" title="5">accuracy_vs_k_all</a></code></pre></div>
<p><img src="_main_files/figure-html/06-more-info-1.png" width="480" /></p>
<p>From the plot above, it seems as though now with more information in our model, we should choose a <span class="math inline">\(k\)</span> of 7. Additionally, with this extra information our validation accuracy has also increased with <span class="math inline">\(k = 7\)</span>.</p>
</div>
<div id="test-data-set" class="section level2">
<h2><span class="header-section-number">7.7</span> Test data set</h2>
<p>In addition to a training and validation sets, in practice we really split our data set into 3 different sets:</p>
<ol style="list-style-type: decimal">
<li>training</li>
<li>validation</li>
<li>testing</li>
</ol>
<p>What is the testing set and what purpose does this third set serve? Typically create the testing set at the very beginning of our analysis, leave it a the locked box so that it plays no role while we a fiddling with things (usually through cross validation) like choosing <span class="math inline">\(k\)</span>, or the number of predictors/explanatory variables to put in the classifier. After we have settled on all the settings (e.g., <span class="math inline">\(k\)</span> and number of predictors) for our classifier and we have no plans to EVER change them again we re-train the classifier on the entire training set (i.e., don’t do cross validation) with those settings and then predict on testing set observations. We then take those predictions and compare them to the true labels of the test set and come up with a test accuracy measure. This typically looks something like this:</p>
<p><img src="img/testing.png" width="400"/></p>
<p>source: <a href="https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6" class="uri">https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6</a></p>
<p>Why do we have this super special test set? This is so we do not violate the golden rule of statistical/machine learning: YOU CANNOT USE THE TEST DATA TO BUILD THE MODEL!!! If you do, this is analagous to a student cheating on a midterm.</p>
</div>
<div id="scaling-your-data" class="section level2">
<h2><span class="header-section-number">7.8</span> Scaling your data</h2>
<p>For a knn classifier, the scale of the variables matter. Since the knn classifier predicts classes by identifying observations that are nearest to it, any variables that are on a large scale will have a much larger effect than variables on a small scale. (Even though they might not actually be more important!) For example, suppose your dataset has two attributes: salary (in dollars) and years of education. For the knn classifier, a difference of $1000 is huge compared to a difference of 10 years of education. For our conceptual understanding and answering of the problem, its the opposite! 10 years of education is huge compared to a difference of $1000 in yearly salary!</p>
<p>So to prevent an overpowering influence of salary on the distance function we need to standardize our predictors/explanatory variables so that our variables will be on a comparable scale. We can do this with the <code>scale()</code> function in R.</p>
<p>To illustrate the difference between scaled and non-scaled data, and the effect it can have on k-nn, we will read in the original, un-scaled Wisconsin breast cancer data set (we have been using a scaled version of the data set in the textbook and worksheets up until now).</p>
<p>Here we get the data from the UCI Machine learning repository, and then get the equivalent columns that we have been working with in the scaled version of the data set (“worst” measures). We also give the data column names (the data set comes with none).</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb156-1" title="1">unscaled_cancer &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data&quot;</span>, </a>
<a class="sourceLine" id="cb156-2" title="2">                            <span class="dt">col_names =</span> <span class="ot">FALSE</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb156-3" title="3"><span class="st">  </span><span class="kw">select</span>(X1, X2, X11<span class="op">:</span>X20)</a></code></pre></div>
<pre><code>## Parsed with column specification:
## cols(
##   .default = col_double(),
##   X2 = col_character()
## )</code></pre>
<pre><code>## See spec(...) for full column specifications.</code></pre>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb159-1" title="1"><span class="kw">colnames</span>(unscaled_cancer) &lt;-<span class="st"> </span><span class="kw">colnames</span>(cancer)</a>
<a class="sourceLine" id="cb159-2" title="2"><span class="kw">head</span>(unscaled_cancer)</a></code></pre></div>
<pre><code>## # A tibble: 6 x 12
##       ID Class Radius Texture Perimeter  Area Smoothness Compactness
##    &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;
## 1 8.42e5 M      0.242  0.0787     1.10  0.905       8.59       153. 
## 2 8.43e5 M      0.181  0.0567     0.544 0.734       3.40        74.1
## 3 8.43e7 M      0.207  0.0600     0.746 0.787       4.58        94.0
## 4 8.43e7 M      0.260  0.0974     0.496 1.16        3.44        27.2
## 5 8.44e7 M      0.181  0.0588     0.757 0.781       5.44        94.4
## 6 8.44e5 M      0.209  0.0761     0.334 0.890       2.22        27.2
## # ... with 4 more variables: Concavity &lt;dbl&gt;, Concave_points &lt;dbl&gt;,
## #   Symmetry &lt;dbl&gt;, Fractal_dimension &lt;dbl&gt;</code></pre>
<p>Looking at the unscaled data above, you can see that the difference between the values for smoothness measurements are much larger than those for area. Let’s make a scatter plot for these two predictors below (colouring by diagnosis) and then overlay that with a new observation we would like to classify as either benign or malignant with a red dot. We’ll do this for the unscaled and scaled data sets, and put them side-by-side:</p>
<p><img src="_main_files/figure-html/06-scaling-2-1.png" width="960" /></p>
<p>In the plot with the unscaled data above, its very clear that k-nn would classify the red dot (new observation) as malignant. However, once we scale the data, the diagnosis class labelling becomes less clear and appears it would depend upon the choice of <span class="math inline">\(k\)</span>. Hopefully this graphic drives home the message that scaling the data changes things in an important way when we are using algorithms like k-nn that use distance between points as a part of their decision making process.</p>
<p>How do we scale data in R? The code below demonstrates how we can use the scale function in R to take the <code>unscaled_cancer</code> data and scale it:</p>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb161-1" title="1">scaled_cancer &lt;-<span class="st"> </span>unscaled_cancer <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb161-2" title="2"><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span><span class="kw">c</span>(ID,Class)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb161-3" title="3"><span class="st">  </span><span class="kw">scale</span>(<span class="dt">center =</span> <span class="ot">FALSE</span>) </a>
<a class="sourceLine" id="cb161-4" title="4">scaled_cancer &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">ID =</span> unscaled_cancer<span class="op">$</span>ID, <span class="dt">Class =</span> unscaled_cancer<span class="op">$</span>Class, scaled_cancer)</a>
<a class="sourceLine" id="cb161-5" title="5"><span class="kw">head</span>(scaled_cancer)</a></code></pre></div>
<pre><code>##         ID Class      Area Smoothness
## 1   842302     M 0.6731696  2.4446014
## 2   842517     M 0.5457187  0.9671389
## 3 84300903     M 0.5851288  1.3049828
## 4 84348301     M 0.8595869  0.9805160
## 5 84358402     M 0.5809648  1.5477637
## 6   843786     M 0.6619414  0.6310026</code></pre>
<p>Scaling your data should be a part of the pre-processing you do before you start doing an analysis where distance between points plays a role. And as we learned in the last two chapters, k-nn classification is one of these.</p>
<blockquote>
<p>What about centering the data? I’ve heard that is also a data pre-processing step? Yes, it can be helpful but it depends on the algorithm. For k-nn classification centering doesn’t help, nor does it hurt. So you can do it if you would like but you do not have to.</p>
</blockquote>
</div>
<div id="strengths-and-limitations-of-k-nn-classification" class="section level2">
<h2><span class="header-section-number">7.9</span> Strengths and limitations of k-nn classification</h2>
<div id="strengths-of-k-nn-classification" class="section level3">
<h3><span class="header-section-number">7.9.1</span> Strengths of k-nn classification</h3>
<ol style="list-style-type: decimal">
<li>Simple and easy to understand</li>
<li>No assumptions about what the data must look like</li>
<li>Works easily for binary (two-class) and multi-class (&gt; 2 classes) classification problems</li>
</ol>
</div>
<div id="limitations-of-k-nn-classification" class="section level3">
<h3><span class="header-section-number">7.9.2</span> Limitations of k-nn classification</h3>
<ol style="list-style-type: decimal">
<li>As data gets bigger and bigger, k-nn gets slower and slower, quite quickly</li>
<li>Does not perform well with a large number of predictors</li>
<li>Does not perform well when classes are imbalanced (when many more observations are in one of the classes compared to the others)</li>
</ol>
</div>
</div>
<div id="additional-readingsresources-3" class="section level2">
<h2><span class="header-section-number">7.10</span> Additional readings/resources</h2>
<ul>
<li><a href="https://topepo.github.io/caret/index.html">The <code>caret</code> Package</a></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/06-classification_continued.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
