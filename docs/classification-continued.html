<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Classification II: Evaluation &amp; tuning | Introduction to Data Science</title>
  <meta name="description" content="This is an open source textbook for teaching introductory data science." />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Classification II: Evaluation &amp; tuning | Introduction to Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is an open source textbook for teaching introductory data science." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Classification II: Evaluation &amp; tuning | Introduction to Data Science" />
  
  <meta name="twitter:description" content="This is an open source textbook for teaching introductory data science." />
  

<meta name="author" content="Tiffany-Anne Timbers" />
<meta name="author" content="Melissa Lee" />
<meta name="author" content="Trevor Campbell" />


<meta name="date" content="2019-11-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classification.html">
<link rel="next" href="regression1.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.46.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.46.1/plotly-latest.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Data Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction to Data Science</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#chapter-learning-objectives"><i class="fa fa-check"></i><b>1.1</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#jupyter-notebooks"><i class="fa fa-check"></i><b>1.2</b> Jupyter notebooks</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#loading-a-spreadsheet-like-dataset"><i class="fa fa-check"></i><b>1.3</b> Loading a spreadsheet-like dataset</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#assigning-value-to-a-data-frame"><i class="fa fa-check"></i><b>1.4</b> Assigning value to a data frame</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#creating-subsets-of-data-frames-with-select-filter"><i class="fa fa-check"></i><b>1.5</b> Creating subsets of data frames with <code>select</code> &amp; <code>filter</code></a><ul>
<li class="chapter" data-level="1.5.1" data-path="index.html"><a href="index.html#using-select-to-extract-multiple-columns"><i class="fa fa-check"></i><b>1.5.1</b> Using <code>select</code> to extract multiple columns</a></li>
<li class="chapter" data-level="1.5.2" data-path="index.html"><a href="index.html#using-select-to-extract-a-range-of-columns"><i class="fa fa-check"></i><b>1.5.2</b> Using <code>select</code> to extract a range of columns</a></li>
<li class="chapter" data-level="1.5.3" data-path="index.html"><a href="index.html#using-filter-to-extract-a-single-row"><i class="fa fa-check"></i><b>1.5.3</b> Using <code>filter</code> to extract a single row</a></li>
<li class="chapter" data-level="1.5.4" data-path="index.html"><a href="index.html#using-filter-to-extract-rows-with-values-above-a-threshold"><i class="fa fa-check"></i><b>1.5.4</b> Using <code>filter</code> to extract rows with values above a threshold</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#exploring-data-with-visualizations"><i class="fa fa-check"></i><b>1.6</b> Exploring data with visualizations</a><ul>
<li class="chapter" data-level="1.6.1" data-path="index.html"><a href="index.html#using-ggplot-to-create-a-scatter-plot"><i class="fa fa-check"></i><b>1.6.1</b> Using <code>ggplot</code> to create a scatter plot</a></li>
<li class="chapter" data-level="1.6.2" data-path="index.html"><a href="index.html#using-ggplot-to-create-a-scatter-plot-1"><i class="fa fa-check"></i><b>1.6.2</b> Using <code>ggplot</code> to create a scatter plot</a></li>
<li class="chapter" data-level="1.6.3" data-path="index.html"><a href="index.html#formatting-ggplot-objects"><i class="fa fa-check"></i><b>1.6.3</b> Formatting ggplot objects</a></li>
<li class="chapter" data-level="1.6.4" data-path="index.html"><a href="index.html#coloring-points-by-group"><i class="fa fa-check"></i><b>1.6.4</b> Coloring points by group</a></li>
<li class="chapter" data-level="1.6.5" data-path="index.html"><a href="index.html#putting-it-all-together"><i class="fa fa-check"></i><b>1.6.5</b> Putting it all together</a></li>
<li class="chapter" data-level="1.6.6" data-path="index.html"><a href="index.html#whats-next"><i class="fa fa-check"></i><b>1.6.6</b> What’s next?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="reading.html"><a href="reading.html"><i class="fa fa-check"></i><b>2</b> Reading in data locally and from the web</a><ul>
<li class="chapter" data-level="2.1" data-path="reading.html"><a href="reading.html#overview"><i class="fa fa-check"></i><b>2.1</b> Overview</a></li>
<li class="chapter" data-level="2.2" data-path="reading.html"><a href="reading.html#chapter-learning-objectives-1"><i class="fa fa-check"></i><b>2.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="2.3" data-path="reading.html"><a href="reading.html#absolute-and-relative-file-paths"><i class="fa fa-check"></i><b>2.3</b> Absolute and relative file paths</a></li>
<li class="chapter" data-level="2.4" data-path="reading.html"><a href="reading.html#reading-tabular-data-from-a-plain-text-file-into-r"><i class="fa fa-check"></i><b>2.4</b> Reading tabular data from a plain text file into R</a><ul>
<li class="chapter" data-level="2.4.1" data-path="reading.html"><a href="reading.html#skipping-rows-when-reading-in-data"><i class="fa fa-check"></i><b>2.4.1</b> Skipping rows when reading in data</a></li>
<li class="chapter" data-level="2.4.2" data-path="reading.html"><a href="reading.html#read_delim-as-a-more-flexible-method-to-get-tabular-data-into-r"><i class="fa fa-check"></i><b>2.4.2</b> <code>read_delim</code> as a more flexible method to get tabular data into R</a></li>
<li class="chapter" data-level="2.4.3" data-path="reading.html"><a href="reading.html#reading-tabular-data-directly-from-a-url"><i class="fa fa-check"></i><b>2.4.3</b> Reading tabular data directly from a URL</a></li>
<li class="chapter" data-level="2.4.4" data-path="reading.html"><a href="reading.html#previewing-a-data-file-before-reading-it-into-r"><i class="fa fa-check"></i><b>2.4.4</b> Previewing a data file before reading it into R</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="reading.html"><a href="reading.html#reading-data-from-an-microsoft-excel-file"><i class="fa fa-check"></i><b>2.5</b> Reading data from an Microsoft Excel file</a></li>
<li class="chapter" data-level="2.6" data-path="reading.html"><a href="reading.html#reading-data-from-a-database"><i class="fa fa-check"></i><b>2.6</b> Reading data from a database</a><ul>
<li class="chapter" data-level="2.6.1" data-path="reading.html"><a href="reading.html#reading-data-from-a-sqlite-database"><i class="fa fa-check"></i><b>2.6.1</b> Reading data from a SQLite database</a></li>
<li class="chapter" data-level="2.6.2" data-path="reading.html"><a href="reading.html#reading-data-from-a-postgresql-database"><i class="fa fa-check"></i><b>2.6.2</b> Reading data from a PostgreSQL database</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="reading.html"><a href="reading.html#writing-data-from-r-to-a-.csv-file"><i class="fa fa-check"></i><b>2.7</b> Writing data from R to a <code>.csv</code> file</a></li>
<li class="chapter" data-level="2.8" data-path="reading.html"><a href="reading.html#scraping-data-off-the-web-using-r"><i class="fa fa-check"></i><b>2.8</b> Scraping data off the web using R</a><ul>
<li class="chapter" data-level="2.8.1" data-path="reading.html"><a href="reading.html#html-and-css-selectors"><i class="fa fa-check"></i><b>2.8.1</b> HTML and CSS selectors</a></li>
<li class="chapter" data-level="2.8.2" data-path="reading.html"><a href="reading.html#are-you-allowed-to-scrape-that-website"><i class="fa fa-check"></i><b>2.8.2</b> Are you allowed to scrape that website?</a></li>
<li class="chapter" data-level="2.8.3" data-path="reading.html"><a href="reading.html#using-rvest"><i class="fa fa-check"></i><b>2.8.3</b> Using <code>rvest</code></a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="reading.html"><a href="reading.html#additional-readingsresources"><i class="fa fa-check"></i><b>2.9</b> Additional readings/resources</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="wrangling.html"><a href="wrangling.html"><i class="fa fa-check"></i><b>3</b> Cleaning and wrangling data</a><ul>
<li class="chapter" data-level="3.1" data-path="wrangling.html"><a href="wrangling.html#overview-1"><i class="fa fa-check"></i><b>3.1</b> Overview</a></li>
<li class="chapter" data-level="3.2" data-path="wrangling.html"><a href="wrangling.html#chapter-learning-objectives-2"><i class="fa fa-check"></i><b>3.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="3.3" data-path="wrangling.html"><a href="wrangling.html#vectors-and-data-frames"><i class="fa fa-check"></i><b>3.3</b> Vectors and Data frames</a><ul>
<li class="chapter" data-level="3.3.1" data-path="wrangling.html"><a href="wrangling.html#what-is-a-data-frame"><i class="fa fa-check"></i><b>3.3.1</b> What is a data frame?</a></li>
<li class="chapter" data-level="3.3.2" data-path="wrangling.html"><a href="wrangling.html#what-is-a-vector"><i class="fa fa-check"></i><b>3.3.2</b> What is a vector?</a></li>
<li class="chapter" data-level="3.3.3" data-path="wrangling.html"><a href="wrangling.html#how-are-vectors-different-from-a-list"><i class="fa fa-check"></i><b>3.3.3</b> How are vectors different from a list?</a></li>
<li class="chapter" data-level="3.3.4" data-path="wrangling.html"><a href="wrangling.html#what-does-this-have-to-do-with-data-frames"><i class="fa fa-check"></i><b>3.3.4</b> What does this have to do with data frames?</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="wrangling.html"><a href="wrangling.html#tidy-data"><i class="fa fa-check"></i><b>3.4</b> Tidy Data</a><ul>
<li class="chapter" data-level="3.4.1" data-path="wrangling.html"><a href="wrangling.html#what-is-tidy-data"><i class="fa fa-check"></i><b>3.4.1</b> What is tidy data?</a></li>
<li class="chapter" data-level="3.4.2" data-path="wrangling.html"><a href="wrangling.html#why-is-tidy-data-important-in-r"><i class="fa fa-check"></i><b>3.4.2</b> Why is tidy data important in R?</a></li>
<li class="chapter" data-level="3.4.3" data-path="wrangling.html"><a href="wrangling.html#going-from-wide-to-long-or-tidy-using-gather"><i class="fa fa-check"></i><b>3.4.3</b> Going from wide to long (or tidy!) using <code>gather</code></a></li>
<li class="chapter" data-level="3.4.4" data-path="wrangling.html"><a href="wrangling.html#using-separate-to-deal-with-multiple-delimiters"><i class="fa fa-check"></i><b>3.4.4</b> Using separate to deal with multiple delimiters</a></li>
<li class="chapter" data-level="3.4.5" data-path="wrangling.html"><a href="wrangling.html#notes-on-defining-tidy-data"><i class="fa fa-check"></i><b>3.4.5</b> Notes on defining tidy data</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="wrangling.html"><a href="wrangling.html#combining-functions-using-the-pipe-operator"><i class="fa fa-check"></i><b>3.5</b> Combining functions using the pipe operator, <code>%&gt;%</code>:</a><ul>
<li class="chapter" data-level="3.5.1" data-path="wrangling.html"><a href="wrangling.html#using-to-combine-filter-and-select"><i class="fa fa-check"></i><b>3.5.1</b> Using <code>%&gt;%</code> to combine <code>filter</code> and <code>select</code></a></li>
<li class="chapter" data-level="3.5.2" data-path="wrangling.html"><a href="wrangling.html#using-with-more-than-two-functions"><i class="fa fa-check"></i><b>3.5.2</b> Using <code>%&gt;%</code> with more than two functions</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="wrangling.html"><a href="wrangling.html#iterating-over-data-with-group_by-summarize"><i class="fa fa-check"></i><b>3.6</b> Iterating over data with <code>group_by</code> + <code>summarize</code></a><ul>
<li class="chapter" data-level="3.6.1" data-path="wrangling.html"><a href="wrangling.html#calculating-summary-statistics"><i class="fa fa-check"></i><b>3.6.1</b> Calculating summary statistics:</a></li>
<li class="chapter" data-level="3.6.2" data-path="wrangling.html"><a href="wrangling.html#calculating-group-summary-statistics"><i class="fa fa-check"></i><b>3.6.2</b> Calculating group summary statistics:</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="wrangling.html"><a href="wrangling.html#additional-reading-on-the-dplyr-functions"><i class="fa fa-check"></i><b>3.7</b> Additional reading on the <code>dplyr</code> functions</a></li>
<li class="chapter" data-level="3.8" data-path="wrangling.html"><a href="wrangling.html#using-purrrs-map-functions-to-iterate"><i class="fa fa-check"></i><b>3.8</b> Using <code>purrr</code>’s <code>map*</code> functions to iterate</a><ul>
<li class="chapter" data-level="3.8.1" data-path="wrangling.html"><a href="wrangling.html#a-bit-more-about-the-map_-functions"><i class="fa fa-check"></i><b>3.8.1</b> A bit more about the <code>map_*</code> functions</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="wrangling.html"><a href="wrangling.html#additional-readingsresources-1"><i class="fa fa-check"></i><b>3.9</b> Additional readings/resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="viz.html"><a href="viz.html"><i class="fa fa-check"></i><b>4</b> Effective data visualization</a><ul>
<li class="chapter" data-level="4.1" data-path="viz.html"><a href="viz.html#overview-2"><i class="fa fa-check"></i><b>4.1</b> Overview</a></li>
<li class="chapter" data-level="4.2" data-path="viz.html"><a href="viz.html#chapter-learning-objectives-3"><i class="fa fa-check"></i><b>4.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="4.3" data-path="viz.html"><a href="viz.html#choosing-the-visualization"><i class="fa fa-check"></i><b>4.3</b> Choosing the visualization</a></li>
<li class="chapter" data-level="4.4" data-path="viz.html"><a href="viz.html#refining-the-visualization"><i class="fa fa-check"></i><b>4.4</b> Refining the visualization</a></li>
<li class="chapter" data-level="4.5" data-path="viz.html"><a href="viz.html#creating-visualizations-with-ggplot2"><i class="fa fa-check"></i><b>4.5</b> Creating visualizations with <code>ggplot2</code></a><ul>
<li class="chapter" data-level="4.5.1" data-path="viz.html"><a href="viz.html#the-mauna-loa-co2-data-set"><i class="fa fa-check"></i><b>4.5.1</b> The Mauna Loa CO2 data set</a></li>
<li class="chapter" data-level="4.5.2" data-path="viz.html"><a href="viz.html#the-island-landmass-data-set"><i class="fa fa-check"></i><b>4.5.2</b> The island landmass data set</a></li>
<li class="chapter" data-level="4.5.3" data-path="viz.html"><a href="viz.html#the-old-faithful-eruption-waiting-time-data-set"><i class="fa fa-check"></i><b>4.5.3</b> The Old Faithful eruption / waiting time data set</a></li>
<li class="chapter" data-level="4.5.4" data-path="viz.html"><a href="viz.html#the-michelson-speed-of-light-data-set"><i class="fa fa-check"></i><b>4.5.4</b> The Michelson speed of light data set</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="viz.html"><a href="viz.html#explaining-the-visualization"><i class="fa fa-check"></i><b>4.6</b> Explaining the visualization</a></li>
<li class="chapter" data-level="4.7" data-path="viz.html"><a href="viz.html#saving-the-visualization"><i class="fa fa-check"></i><b>4.7</b> Saving the visualization</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="GitHub.html"><a href="GitHub.html"><i class="fa fa-check"></i><b>5</b> Version control with GitHub</a><ul>
<li class="chapter" data-level="5.1" data-path="GitHub.html"><a href="GitHub.html#overview-3"><i class="fa fa-check"></i><b>5.1</b> Overview</a></li>
<li class="chapter" data-level="5.2" data-path="GitHub.html"><a href="GitHub.html#videos-to-learn-about-version-control-with-github-and-git"><i class="fa fa-check"></i><b>5.2</b> Videos to learn about version control with GitHub and Git</a><ul>
<li class="chapter" data-level="5.2.1" data-path="GitHub.html"><a href="GitHub.html#creating-a-github-repository"><i class="fa fa-check"></i><b>5.2.1</b> Creating a GitHub repository</a></li>
<li class="chapter" data-level="5.2.2" data-path="GitHub.html"><a href="GitHub.html#exploring-a-github-repository"><i class="fa fa-check"></i><b>5.2.2</b> Exploring a GitHub repository</a></li>
<li class="chapter" data-level="5.2.3" data-path="GitHub.html"><a href="GitHub.html#directly-editing-files-on-github"><i class="fa fa-check"></i><b>5.2.3</b> Directly editing files on GitHub</a></li>
<li class="chapter" data-level="5.2.4" data-path="GitHub.html"><a href="GitHub.html#logging-changes-and-pushing-them-to-github"><i class="fa fa-check"></i><b>5.2.4</b> Logging changes and pushing them to GitHub</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="GitHub.html"><a href="GitHub.html#git-command-cheatsheet"><i class="fa fa-check"></i><b>5.3</b> Git command cheatsheet</a><ul>
<li class="chapter" data-level="5.3.1" data-path="GitHub.html"><a href="GitHub.html#getting-a-repository-from-github-onto-the-server-for-the-first-time"><i class="fa fa-check"></i><b>5.3.1</b> Getting a repository from GitHub onto the server for the first time</a></li>
<li class="chapter" data-level="5.3.2" data-path="GitHub.html"><a href="GitHub.html#logging-changes"><i class="fa fa-check"></i><b>5.3.2</b> Logging changes</a></li>
<li class="chapter" data-level="5.3.3" data-path="GitHub.html"><a href="GitHub.html#sending-your-changes-back-to-github"><i class="fa fa-check"></i><b>5.3.3</b> Sending your changes back to GitHub</a></li>
<li class="chapter" data-level="5.3.4" data-path="GitHub.html"><a href="GitHub.html#getting-changes"><i class="fa fa-check"></i><b>5.3.4</b> Getting changes</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="GitHub.html"><a href="GitHub.html#terminal-cheatsheet"><i class="fa fa-check"></i><b>5.4</b> Terminal cheatsheet</a><ul>
<li class="chapter" data-level="5.4.1" data-path="GitHub.html"><a href="GitHub.html#see-where-you-are"><i class="fa fa-check"></i><b>5.4.1</b> See where you are:</a></li>
<li class="chapter" data-level="5.4.2" data-path="GitHub.html"><a href="GitHub.html#see-what-is-inside-the-directory-where-you-are"><i class="fa fa-check"></i><b>5.4.2</b> See what is inside the directory where you are:</a></li>
<li class="chapter" data-level="5.4.3" data-path="GitHub.html"><a href="GitHub.html#move-to-a-different-directory"><i class="fa fa-check"></i><b>5.4.3</b> Move to a different directory</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>6</b> Classification I: Training &amp; predicting</a><ul>
<li class="chapter" data-level="6.1" data-path="classification.html"><a href="classification.html#overview-4"><i class="fa fa-check"></i><b>6.1</b> Overview</a></li>
<li class="chapter" data-level="6.2" data-path="classification.html"><a href="classification.html#chapter-learning-objectives-4"><i class="fa fa-check"></i><b>6.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="6.3" data-path="classification.html"><a href="classification.html#the-classification-problem"><i class="fa fa-check"></i><b>6.3</b> The classification problem</a></li>
<li class="chapter" data-level="6.4" data-path="classification.html"><a href="classification.html#exploring-a-labelled-data-set"><i class="fa fa-check"></i><b>6.4</b> Exploring a labelled data set</a></li>
<li class="chapter" data-level="6.5" data-path="classification.html"><a href="classification.html#classification-with-k-nearest-neighbours"><i class="fa fa-check"></i><b>6.5</b> Classification with K-nearest neighbours</a></li>
<li class="chapter" data-level="6.6" data-path="classification.html"><a href="classification.html#k-nearest-neighbours-in-r"><i class="fa fa-check"></i><b>6.6</b> K-nearest neighbours in R</a></li>
<li class="chapter" data-level="6.7" data-path="classification.html"><a href="classification.html#data-preprocessing"><i class="fa fa-check"></i><b>6.7</b> Data preprocessing</a><ul>
<li class="chapter" data-level="6.7.1" data-path="classification.html"><a href="classification.html#shifting-and-scaling"><i class="fa fa-check"></i><b>6.7.1</b> Shifting and scaling</a></li>
<li class="chapter" data-level="6.7.2" data-path="classification.html"><a href="classification.html#balancing"><i class="fa fa-check"></i><b>6.7.2</b> Balancing</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification-continued.html"><a href="classification-continued.html"><i class="fa fa-check"></i><b>7</b> Classification II: Evaluation &amp; tuning</a><ul>
<li class="chapter" data-level="7.1" data-path="classification-continued.html"><a href="classification-continued.html#overview-5"><i class="fa fa-check"></i><b>7.1</b> Overview</a></li>
<li class="chapter" data-level="7.2" data-path="classification-continued.html"><a href="classification-continued.html#chapter-learning-objectives-5"><i class="fa fa-check"></i><b>7.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="7.3" data-path="classification-continued.html"><a href="classification-continued.html#evaluating-accuracy"><i class="fa fa-check"></i><b>7.3</b> Evaluating accuracy</a></li>
<li class="chapter" data-level="7.4" data-path="classification-continued.html"><a href="classification-continued.html#tuning-the-classifier"><i class="fa fa-check"></i><b>7.4</b> Tuning the classifier</a><ul>
<li class="chapter" data-level="7.4.1" data-path="classification-continued.html"><a href="classification-continued.html#cross-validation"><i class="fa fa-check"></i><b>7.4.1</b> Cross-validation</a></li>
<li class="chapter" data-level="7.4.2" data-path="classification-continued.html"><a href="classification-continued.html#parameter-value-selection"><i class="fa fa-check"></i><b>7.4.2</b> Parameter value selection</a></li>
<li class="chapter" data-level="7.4.3" data-path="classification-continued.html"><a href="classification-continued.html#underoverfitting"><i class="fa fa-check"></i><b>7.4.3</b> Under/overfitting</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="classification-continued.html"><a href="classification-continued.html#splitting-data"><i class="fa fa-check"></i><b>7.5</b> Splitting data</a></li>
<li class="chapter" data-level="7.6" data-path="classification-continued.html"><a href="classification-continued.html#summary"><i class="fa fa-check"></i><b>7.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regression1.html"><a href="regression1.html"><i class="fa fa-check"></i><b>8</b> Regression I: K-nearest neighbours</a><ul>
<li class="chapter" data-level="8.1" data-path="regression1.html"><a href="regression1.html#overview-6"><i class="fa fa-check"></i><b>8.1</b> Overview</a></li>
<li class="chapter" data-level="8.2" data-path="regression1.html"><a href="regression1.html#chapter-learning-objectives-6"><i class="fa fa-check"></i><b>8.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="8.3" data-path="regression1.html"><a href="regression1.html#regression"><i class="fa fa-check"></i><b>8.3</b> Regression</a></li>
<li class="chapter" data-level="8.4" data-path="regression1.html"><a href="regression1.html#sacremento-real-estate-example"><i class="fa fa-check"></i><b>8.4</b> Sacremento real estate example</a></li>
<li class="chapter" data-level="8.5" data-path="regression1.html"><a href="regression1.html#k-nearest-neighbours-regression"><i class="fa fa-check"></i><b>8.5</b> K-nearest neighbours regression</a></li>
<li class="chapter" data-level="8.6" data-path="regression1.html"><a href="regression1.html#assessing-a-nearest-neighbours-regression-model"><i class="fa fa-check"></i><b>8.6</b> Assessing a nearest neighbours regression model</a><ul>
<li class="chapter" data-level="8.6.1" data-path="regression1.html"><a href="regression1.html#rmspe-versus-rmse"><i class="fa fa-check"></i><b>8.6.1</b> RMSPE versus RMSE</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="regression1.html"><a href="regression1.html#how-do-different-ks-affect-k-nn-regression-predictions"><i class="fa fa-check"></i><b>8.7</b> How do different k’s affect k-nn regression predictions</a></li>
<li class="chapter" data-level="8.8" data-path="regression1.html"><a href="regression1.html#assessing-how-well-the-model-predicts-on-unseen-data-with-the-test-set"><i class="fa fa-check"></i><b>8.8</b> Assessing how well the model predicts on unseen data with the test set</a></li>
<li class="chapter" data-level="8.9" data-path="regression1.html"><a href="regression1.html#strengths-and-limitations-of-k-nn-regression"><i class="fa fa-check"></i><b>8.9</b> Strengths and limitations of k-nn regression</a></li>
<li class="chapter" data-level="8.10" data-path="regression1.html"><a href="regression1.html#multivariate-k-nn-regression"><i class="fa fa-check"></i><b>8.10</b> Multivariate k-nn regression</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regression2.html"><a href="regression2.html"><i class="fa fa-check"></i><b>9</b> Regression II: Linear regression</a><ul>
<li class="chapter" data-level="9.1" data-path="regression2.html"><a href="regression2.html#overview-7"><i class="fa fa-check"></i><b>9.1</b> Overview</a></li>
<li class="chapter" data-level="9.2" data-path="regression2.html"><a href="regression2.html#chapter-learning-objectives-7"><i class="fa fa-check"></i><b>9.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="9.3" data-path="regression2.html"><a href="regression2.html#simple-linear-regression"><i class="fa fa-check"></i><b>9.3</b> Simple linear regression</a></li>
<li class="chapter" data-level="9.4" data-path="regression2.html"><a href="regression2.html#linear-regression-in-r-using-caret"><i class="fa fa-check"></i><b>9.4</b> Linear regression in R using <code>caret</code></a></li>
<li class="chapter" data-level="9.5" data-path="regression2.html"><a href="regression2.html#comparing-simple-linear-and-k-nn-regression"><i class="fa fa-check"></i><b>9.5</b> Comparing simple linear and k-nn regression</a></li>
<li class="chapter" data-level="9.6" data-path="regression2.html"><a href="regression2.html#multivariate-linear-regression"><i class="fa fa-check"></i><b>9.6</b> Multivariate linear regression</a></li>
<li class="chapter" data-level="9.7" data-path="regression2.html"><a href="regression2.html#the-other-side-of-regression"><i class="fa fa-check"></i><b>9.7</b> The other side of regression</a></li>
<li class="chapter" data-level="9.8" data-path="regression2.html"><a href="regression2.html#additional-readingsresources-2"><i class="fa fa-check"></i><b>9.8</b> Additional readings/resources</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>10</b> Clustering</a><ul>
<li class="chapter" data-level="10.1" data-path="clustering.html"><a href="clustering.html#overview-8"><i class="fa fa-check"></i><b>10.1</b> Overview</a></li>
<li class="chapter" data-level="10.2" data-path="clustering.html"><a href="clustering.html#chapter-learning-objectives-8"><i class="fa fa-check"></i><b>10.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="10.3" data-path="clustering.html"><a href="clustering.html#clustering"><i class="fa fa-check"></i><b>10.3</b> Clustering</a></li>
<li class="chapter" data-level="10.4" data-path="clustering.html"><a href="clustering.html#k-means"><i class="fa fa-check"></i><b>10.4</b> K-means</a><ul>
<li class="chapter" data-level="10.4.1" data-path="clustering.html"><a href="clustering.html#measuring-cluster-quality"><i class="fa fa-check"></i><b>10.4.1</b> Measuring cluster quality</a></li>
<li class="chapter" data-level="10.4.2" data-path="clustering.html"><a href="clustering.html#the-clustering-algorithm"><i class="fa fa-check"></i><b>10.4.2</b> The clustering algorithm</a></li>
<li class="chapter" data-level="10.4.3" data-path="clustering.html"><a href="clustering.html#random-restarts"><i class="fa fa-check"></i><b>10.4.3</b> Random restarts</a></li>
<li class="chapter" data-level="10.4.4" data-path="clustering.html"><a href="clustering.html#choosing-k"><i class="fa fa-check"></i><b>10.4.4</b> Choosing K</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="clustering.html"><a href="clustering.html#k-means-in-r"><i class="fa fa-check"></i><b>10.5</b> K-means in R</a></li>
<li class="chapter" data-level="10.6" data-path="clustering.html"><a href="clustering.html#additional-readings"><i class="fa fa-check"></i><b>10.6</b> Additional readings:</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification_continued" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Classification II: Evaluation &amp; tuning</h1>
<div id="overview-5" class="section level2">
<h2><span class="header-section-number">7.1</span> Overview</h2>
<p>This chapter continues the introduction to predictive modelling through classification. While the previous chapter covered training and data preprocessing, this chapter focuses on how to split data, how to evaluate prediction accuracy, and how to choose model parameters to maximize performance.</p>
</div>
<div id="chapter-learning-objectives-5" class="section level2">
<h2><span class="header-section-number">7.2</span> Chapter learning objectives</h2>
<p>By the end of the chapter, students will be able to:</p>
<ul>
<li>Describe what training, validation, and test data sets are and how they are used in classification</li>
<li>Split data into training, validation, and test data sets</li>
<li>Evaluate classification accuracy in R using a validation data set and appropriate metrics</li>
<li>Execute cross-validation in R to choose the number of neighbours in a K-nearest neighbour classifier</li>
<li>Describe advantages and disadvantages of the K-nearest neighbour classification algorithm</li>
</ul>
</div>
<div id="evaluating-accuracy" class="section level2">
<h2><span class="header-section-number">7.3</span> Evaluating accuracy</h2>
<p>Sometimes our classifier might make the wrong prediction. A classifier does not need to be right 100% of the time to be useful, though we don’t want the classifier to make too many wrong predictions. How do we measure how “good” our classifier is? Let’s revisit the Wisconsin breast cancer example and think about how our classifier will be used in practice. A biopsy will be performed on a <em>new</em> patient’s tumour, the resulting image will be analyzed, and the classifier will be asked to decide whether the tumour is benign or malignant. The key word here is <em>new</em>: our classifier is “good” if it provides accurate predictions on data <em>not seen during training</em>. But then how can we evaluate our classifier without having to visit the hospital to collect more tumour images?</p>
<p>The trick is to split up the data set into a <strong>training set</strong> and <strong>test set</strong>, and only show the classifier the <strong>training set</strong> when building the classifier. Then to evaluate the accuracy of the classifier, we can use it to predict the labels (which we know) in the <strong>test set</strong>. If our predictions match the true labels for the observations in the <strong>test set</strong> very well, then we have some confidence that our classifier might also do a good job of predicting the class labels for new observations that we do not have the class labels for.</p>
<blockquote>
<p>Note: if there were a golden rule of machine learning, it might be this: <em>you cannot use the test data to build the model!</em> If you do, the model gets to “see” the test data in advance, making it look more accurate than it really is. Imagine how bad it would be to overestimate your classifier’s accuracy when predicting whether a patient’s tumour is malignant or benign!</p>
</blockquote>
<center>
<img src="img/training_test.jpeg" width="600" />
</center>
<p>How exactly can we assess how well our predictions match the true labels for the observations in the test set? One way we can do this is to calculate the <strong>prediction accuracy</strong>. This is the fraction of examples for which the classifier made the correct prediction. To calculate this we divide the number of correct predictions by the number of predictions made. Other measures for how well our classifier performed include <em>precision</em> and <em>recall</em>; these will not be discussed here, but you will encounter them in other more advanced courses on this topic. This process is illustrated below:</p>
<center>
<img src="img/ML-paradigm-test.png" width="800" />
</center>
<p>In R, we can use the <code>caret</code> package not only to perform K-nearest neighbour classification, but also to assess how well our classification worked. Let’s start by loading the necessary libraries, reading in the breast cancer data from the previous chapter, and making a quick scatter plot visualization of tumour cell concavity versus smoothness coloured by diagnosis.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load libraries</span>
<span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(caret)

<span class="co">#load data</span>
cancer &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/clean-wdbc.data.csv&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Class =</span> <span class="kw">as.factor</span>(Class)) <span class="co"># because we will be doing statistical analysis on a categorical variable</span>

<span class="co"># colour palette</span>
cbPalette &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;#56B4E9&quot;</span>, <span class="st">&quot;#E69F00&quot;</span>,<span class="st">&quot;#009E73&quot;</span>, <span class="st">&quot;#F0E442&quot;</span>, <span class="st">&quot;#0072B2&quot;</span>, <span class="st">&quot;#D55E00&quot;</span>, <span class="st">&quot;#CC79A7&quot;</span>, <span class="st">&quot;#999999&quot;</span>) 

<span class="co"># create scatter plot of tumour cell concavity versus smoothness, </span>
<span class="co"># labelling the points be diagnosis class</span>
perim_concav &lt;-<span class="st"> </span>cancer <span class="op">%&gt;%</span><span class="st">  </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Smoothness, <span class="dt">y =</span> Concavity, <span class="dt">color =</span> Class)) <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.5</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">color =</span> <span class="st">&quot;Diagnosis&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">scale_color_manual</span>(<span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;Benign&quot;</span>, <span class="st">&quot;Malignant&quot;</span>), <span class="dt">values =</span> cbPalette)
perim_concav</code></pre></div>
<p><img src="_main_files/figure-html/06-precode-1.png" width="480" /></p>
<p><strong>1. Create the train / test split</strong></p>
<p>Once we have decided on a predictive question to answer and done some preliminary exploration, the very next thing to do is to split the data into the training and test sets. Typically, the training set is between 50 - 100% of the data, while the test set is the remaining 0 - 50%; the intuition is that you want to trade off between training an accurate model (by using a larger training data set) and getting an accurate evaluation of its performance (by using a larger test data set). Here, we will use 75% of the data for training, and 25% for testing. To do this we will use the <code>createDataPartition</code> function from the <code>caret</code> package, specifying values for 3 arguments:</p>
<ol style="list-style-type: decimal">
<li><code>y</code>: the class labels. These must be a vector.</li>
<li><code>p</code>: the proportion (between 0 and 1) of the data you would like in the training data set.</li>
<li><code>list = FALSE</code>: this states that we want the training and test sets in the form of a matrix, not a list.</li>
</ol>
<p>The <code>createDataPartition</code> function returns the row numbers for the training set.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>) <span class="co"># makes the random selection of rows reproducible</span>
set_rows &lt;-<span class="st"> </span>cancer <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(Class) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">unlist</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># converts Class from a tibble to a vector</span>
<span class="st">  </span><span class="kw">createDataPartition</span>(<span class="dt">p =</span> <span class="fl">0.75</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)
<span class="kw">head</span>(set_rows)</code></pre></div>
<pre><code>##      Resample1
## [1,]         1
## [2,]         5
## [3,]         6
## [4,]         9
## [5,]        10
## [6,]        11</code></pre>
<blockquote>
<p>Note: You will see in the code above that we use the <code>set.seed</code> function again, as discussed in the previous chapter. In this case it is because <code>createDataPartition</code> uses random sampling to choose which rows will be in the training set. Since we want our code to be reproducible and generate the same train/test split each time it is run, we use <code>set.seed</code>.</p>
</blockquote>
<p>Now that we have the row numbers for the training set, we can use the <code>slice</code> function to get the rows from the original data set (here <code>cancer</code>) to create the training and test data sets.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">training_set &lt;-<span class="st"> </span>cancer <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(set_rows)
test_set &lt;-<span class="st"> </span>cancer <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="op">-</span>set_rows)
<span class="kw">glimpse</span>(training_set)</code></pre></div>
<pre><code>## Observations: 427
## Variables: 12
## $ ID                &lt;dbl&gt; 842302, 84358402, 843786, 844981, 84501001, 84…
## $ Class             &lt;fct&gt; M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M…
## $ Radius            &lt;dbl&gt; 1.8850310, 1.2974336, -0.1653528, -0.1612147, …
## $ Texture           &lt;dbl&gt; -1.3580985, -1.4654809, -0.3135604, 0.8220900,…
## $ Perimeter         &lt;dbl&gt; 2.30157548, 1.33736272, -0.11490835, -0.031581…
## $ Area              &lt;dbl&gt; 1.999478159, 1.219651081, -0.244105421, -0.248…
## $ Smoothness        &lt;dbl&gt; 1.30653666, 0.22036227, 2.04671194, 1.66129523…
## $ Compactness       &lt;dbl&gt; 2.6143647, -0.3131190, 1.7201029, 1.8167112, 5…
## $ Concavity         &lt;dbl&gt; 2.10767182, 0.61263970, 1.26213265, 1.27890922…
## $ Concave_points    &lt;dbl&gt; 2.29405760, 0.72861815, 0.90509140, 1.39039284…
## $ Symmetry          &lt;dbl&gt; 2.7482041, -0.8675896, 1.7525273, 2.3877562, 2…
## $ Fractal_dimension &lt;dbl&gt; 1.93531174, -0.39675052, 2.23983079, 1.2875166…</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">glimpse</span>(test_set)</code></pre></div>
<pre><code>## Observations: 142
## Variables: 12
## $ ID                &lt;dbl&gt; 842517, 84300903, 84348301, 844359, 84458202, …
## $ Class             &lt;fct&gt; M, M, M, M, M, M, M, B, B, B, M, M, M, M, B, B…
## $ Radius            &lt;dbl&gt; 1.80433981, 1.51054113, -0.28121702, 1.3677798…
## $ Texture           &lt;dbl&gt; -0.36887865, -0.02395331, 0.13386631, 0.322599…
## $ Perimeter         &lt;dbl&gt; 1.53377643, 1.34629062, -0.24971958, 1.3671223…
## $ Area              &lt;dbl&gt; 1.88882702, 1.45500430, -0.54953769, 1.2740984…
## $ Smoothness        &lt;dbl&gt; -0.375281748, 0.526943750, 3.391290721, 0.5181…
## $ Compactness       &lt;dbl&gt; -0.43006581, 1.08198014, 3.88997467, 0.0211963…
## $ Concavity         &lt;dbl&gt; -0.14661996, 0.85422232, 1.98783917, 0.5091042…
## $ Concave_points    &lt;dbl&gt; 1.0861286, 1.9532817, 2.1738732, 1.1956637, 0.…
## $ Symmetry          &lt;dbl&gt; -0.24367526, 1.15124203, 6.04072615, 0.2622449…
## $ Fractal_dimension &lt;dbl&gt; 0.28094279, 0.20121416, 4.93067187, -0.0147175…</code></pre>
<p>We can see from <code>glimpse</code> in the code above that the training set contains 427 observations, while the test set contains 142 observations. This corresponds to a train / test split of 75% / 25%, as desired.</p>
<p><strong>2. Train the classifier</strong></p>
<p>Now that we have split our original data set into training and test sets, we can create our K-nearest neighbour classifier with only the training set using the technique we learned in the previous chapter. For now, we will just choose the number <span class="math inline">\(K\)</span> of neighbours to be 3, and use concavity and smoothness as the predictors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X_train &lt;-<span class="st"> </span>training_set <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(Concavity, Smoothness) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">data.frame</span>()
Y_train &lt;-<span class="st"> </span>training_set <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(Class) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">unlist</span>()
k =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="dv">3</span>)

<span class="kw">set.seed</span>(<span class="dv">1</span>)
model_knn &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> X_train, <span class="dt">y =</span> Y_train, <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="dt">tuneGrid =</span> k)
model_knn</code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 427 samples
##   2 predictor
##   2 classes: &#39;B&#39;, &#39;M&#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 427, 427, 427, 427, 427, 427, ... 
## Resampling results:
## 
##   Accuracy   Kappa    
##   0.8098792  0.5916618
## 
## Tuning parameter &#39;k&#39; was held constant at a value of 3</code></pre>
<blockquote>
<p>Note: Here again you see the <code>set.seed</code> function. In the K-nearest neighbour implementation in <code>caret</code>, when there is a tie for the majority neighbour class, the winner is randomly selected. Although there is no chance of a tie when <span class="math inline">\(K\)</span> is odd (here <span class="math inline">\(K=3\)</span>), it is possible that the code may be changed in the future to have an even value of <span class="math inline">\(K\)</span>. Thus, to prevent potential issues with reproducibility, we have set the seed. Note that in your own code, you only have to set the seed once at the beginning of your analysis.</p>
</blockquote>
<p><strong>3. Predict the labels in the test set</strong></p>
<p>Now that we have a K-nearest neighbour classifier object, we can use it to predict the class labels for our test set:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X_test &lt;-<span class="st"> </span>test_set <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(Concavity, Smoothness) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">data.frame</span>()
Y_test_predicted &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object =</span> model_knn, X_test)
<span class="kw">head</span>(Y_test_predicted)</code></pre></div>
<pre><code>## [1] B M M M M M
## Levels: B M</code></pre>
<p><strong>4. Compute the accuracy</strong></p>
<p>Finally we can assess our classifier’s accuracy. To do this we need to create a vector containing the class labels for the test set. Next we use the function <code>confusionMatrix</code> to get the statistics about the quality of our model, this includes the statistic we are interested: accuracy. <code>confusionMatrix</code> takes two arguments:</p>
<ol style="list-style-type: decimal">
<li><code>data</code> (the predicted class labels for the test set), and</li>
<li><code>reference</code> (the original/measured class labels for the test set).</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Y_test &lt;-<span class="st"> </span>test_set <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(Class) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">unlist</span>()

model_quality &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="dt">data =</span> Y_test_predicted, <span class="dt">reference =</span> Y_test)
model_quality</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  B  M
##          B 72  8
##          M 17 45
##                                           
##                Accuracy : 0.8239          
##                  95% CI : (0.7512, 0.8827)
##     No Information Rate : 0.6268          
##     P-Value [Acc &gt; NIR] : 2.437e-07       
##                                           
##                   Kappa : 0.6362          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.1096          
##                                           
##             Sensitivity : 0.8090          
##             Specificity : 0.8491          
##          Pos Pred Value : 0.9000          
##          Neg Pred Value : 0.7258          
##              Prevalence : 0.6268          
##          Detection Rate : 0.5070          
##    Detection Prevalence : 0.5634          
##       Balanced Accuracy : 0.8290          
##                                           
##        &#39;Positive&#39; Class : B               
## </code></pre>
<p>A lot of information is output from <code>confusionMatrix</code>, but what we are interested in at this point is accuracy (found on the 6th line of printed output). That single value can be obtained from the <code>confusionMatrix</code> object using base/built-in R subsetting:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model_quality<span class="op">$</span>overall[<span class="dv">1</span>]</code></pre></div>
<pre><code>##  Accuracy 
## 0.8239437</code></pre>
<p>From a value of accuracy of around 0.824, we can say that our K-nearest neighbour classifier predicted the correct class label on roughly 82% of the examples.</p>
</div>
<div id="tuning-the-classifier" class="section level2">
<h2><span class="header-section-number">7.4</span> Tuning the classifier</h2>
<p>The vast majority of predictive models in statistics and machine learning have <em>parameters</em> that you have to pick. For example, in the K-nearest neighbour classification algorithm we have been using in the past two chapters, we have had to pick the number of neighbours <span class="math inline">\(K\)</span> for the class vote. Is it possible to make this selection, i.e., <em>tune</em> the model, in a principled way? Ideally what we want is to somehow maximize the performance of our classifier on data <em>it hasn’t seen yet</em>. So we will play the same trick we did before when evaluating our classifier: we’ll split our <strong>overall training data set</strong> further into two subsets, called the <strong>training set</strong> and <strong>validation set</strong>. We will use the newly-named <strong>training set</strong> for building the classifier, and the <strong>validation set</strong> for evaluating it! Then we will try different values of the parameter <span class="math inline">\(K\)</span> and pick the one that yields the highest accuracy.</p>
<blockquote>
<p><strong>Remember:</strong> <em>don’t touch the test set during the tuning process. Tuning is a part of model training!</em></p>
</blockquote>
<div id="cross-validation" class="section level3">
<h3><span class="header-section-number">7.4.1</span> Cross-validation</h3>
<p>There is an important detail to mention about the process of tuning: we can, if we want to, split our overall training data up in multiple different ways, train and evaluate a classifier for each split, and then choose the parameter based on <em>all</em> of the different results. If we just split our overall training data <em>once</em>, our best parameter choice will depend strongly on whatever data was lucky enough to end up in the validation set. Perhaps using multiple different train / validation splits, we’ll get a better estimate of accuracy, which will lead to a better choice of the number of neighbours <span class="math inline">\(K\)</span> for the overall set of training data.</p>
<blockquote>
<p><strong>Note:</strong> you might be wondering why we can’t we use the multiple splits to test our final classifier after tuning is done. This is simply because at the end of the day, we will produce a single classifier using our overall training data. If we do multiple train / test splits, we will end up with multiple classifiers, each with their own accuracy evaluated on different test data!</p>
</blockquote>
<p>Let’s investigate this idea in R! In particular, we will use different seed values in the <code>set.seed</code> function to generate five different train / validation splits of our overall training data, train five different K-nearest neighbour models, and evaluate their accuracy. First we’ll rename our overall training data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X_train_total &lt;-<span class="st"> </span>X_train
Y_train_total &lt;-<span class="st"> </span>Y_train</code></pre></div>
<p>Then we’ll try 5 different random train / validation splits of the data and record the accuracy:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">accuracies &lt;-<span class="st"> </span><span class="kw">c</span>()
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>){
    <span class="kw">set.seed</span>(i) <span class="co"># makes the random selection of rows reproducible</span>
    <span class="co"># create the 75 / 25 train/validation split</span>
    set_rows &lt;-<span class="st"> </span>Y_train_total <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">      </span><span class="kw">createDataPartition</span>(<span class="dt">p =</span> <span class="fl">0.75</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)
    
    <span class="co">#split the X and Y data into train/validation</span>
    X_train &lt;-<span class="st"> </span>X_train_total <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(set_rows)
    Y_train &lt;-<span class="st"> </span>Y_train_total[set_rows]
    X_validation &lt;-<span class="st"> </span>X_train_total <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="op">-</span>set_rows)
    Y_validation &lt;-<span class="st"> </span>Y_train_total[<span class="op">-</span>set_rows]
    
    <span class="co">#train the KNN model with K=3, and predict the validation labels</span>
    k =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="dv">3</span>)
    model_knn &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> X_train, <span class="dt">y =</span> Y_train, <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="dt">tuneGrid =</span> k)
    Y_validation_predicted &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object =</span> model_knn, X_validation)
    
    <span class="co">#compute the accuracy</span>
    model_quality &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="dt">data =</span> Y_validation_predicted, <span class="dt">reference =</span> Y_validation)
    accuracies &lt;-<span class="st"> </span><span class="kw">append</span>(accuracies, model_quality<span class="op">$</span>overall[<span class="dv">1</span>])
}
accuracies</code></pre></div>
<pre><code>##  Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
## 0.8301887 0.8773585 0.8396226 0.8207547 0.8867925</code></pre>
<p>With five different shuffles of the data, we get five different values for accuracy: 0.83, 0.88, 0.84, 0.82, and 0.89! None of these is necessarily “more correct” than any other; they’re just five estimates of the true, underlying accuracy of our classifier built using our overall training data. We can combine the estimates by taking their average (here 0.851) to try to get a single assessment of our classifier’s accuracy; this has the effect of reducing the influence of any one (un)lucky validation set on the estimate.</p>
<p>In practice, we don’t use random splits, but rather use a more structured splitting procedure so that each observation in the data set is used in a validation set only a single time. The name for this strategy is called <strong>cross-validation</strong>. In <strong>cross-validation</strong>, we split our <strong>overall training data</strong> into <span class="math inline">\(C\)</span> evenly-sized chunks, and then iteratively use <span class="math inline">\(1\)</span> chunk as the <strong>validation set</strong> and combine the remaining <span class="math inline">\(C-1\)</span> chunks as the <strong>training set</strong>:</p>
<p><img src="img/cv.png" width="800" /></p>
<p>In the picture above, <span class="math inline">\(C=5\)</span> different chunks of the data set are used, resulting in 5 different choices for the <strong>validation set</strong>; we call this <em>5-fold</em> cross-validation. To do 5-fold cross-validation in R with <code>caret</code>, we use another function called <code>trainControl</code>. This function passes additional information to the <code>train</code> function we use to create our classifier. The arguments we pass <code>trainControl</code> are:</p>
<ol style="list-style-type: decimal">
<li><code>method=&quot;cv&quot;</code>: specifies to use cross-validation for assessing quality</li>
<li><code>number</code>: how many chunks to split the data into for cross validation</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>)</code></pre></div>
<p>Then, when we create our classifier, we add an additional argument to <code>train</code> called <code>trControl</code>, which we set to the <code>train_control</code> object we just created. One benefit of using <code>caret</code> for this is that we do not need to manually do any of the work of cross-validation ourselves; the <code>train</code> function will handle creating the chunks, doing multiple rounds of training and evaluation, and averaging the results for us.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1234</span>)
k =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="dv">3</span>)
knn_model_cv_5fold &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> X_train_total, <span class="dt">y =</span> Y_train_total, <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="dt">tuneGrid =</span> k, <span class="dt">trControl =</span> train_control)
knn_model_cv_5fold</code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 427 samples
##   2 predictor
##   2 classes: &#39;B&#39;, &#39;M&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 342, 341, 341, 341, 343 
## Resampling results:
## 
##   Accuracy   Kappa    
##   0.8219868  0.6175085
## 
## Tuning parameter &#39;k&#39; was held constant at a value of 3</code></pre>
<blockquote>
<p><strong>Note:</strong> we set the seed when we call <code>train</code> not only because of the potential for ties, but also because we are doing cross-validation. Cross-validation uses a random process to select how to partition the training data.</p>
</blockquote>
<p>We can choose any number of folds, and typically the more we use the better our accuracy estimate will be. However, we are limited by computational power: the more folds we choose, the more computation it takes, and hence the more time it takes to run the analysis. So when you do cross-validation, you need to consider the size of the data, and the speed of the algorithm (e.g., K-nearest neighbour) and the speed of your computer. In practice, this is a trial and error process, but typically <span class="math inline">\(C\)</span> is chosen to be either 5 or 10. Here we show what happens when we split the data into 10 chunks and do 10-fold cross-validation:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>)

<span class="kw">set.seed</span>(<span class="dv">1234</span>)
knn_model_cv_10fold &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> X_train_total, <span class="dt">y =</span> Y_train_total, <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="dt">tuneGrid =</span> k, <span class="dt">trControl =</span> train_control)
knn_model_cv_10fold</code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 427 samples
##   2 predictor
##   2 classes: &#39;B&#39;, &#39;M&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 385, 384, 384, 384, 384, 384, ... 
## Resampling results:
## 
##   Accuracy  Kappa    
##   0.843057  0.6607231
## 
## Tuning parameter &#39;k&#39; was held constant at a value of 3</code></pre>
</div>
<div id="parameter-value-selection" class="section level3">
<h3><span class="header-section-number">7.4.2</span> Parameter value selection</h3>
<p>Using 5- and 10-fold cross-validation, we have estimated that the prediction accuracy of our classifier is somewhere around 85%. Whether 85% is good or not depends entirely on the downstream application of the data analysis. In the present situation, we are trying to predict a tumour diagnosis, with expensive, damaging chemo/radiation therapy or patient death as potential consequences of misprediction. Hence, we’d like to do better than 85% for this application. In order to improve our classifier, we have one choice of parameter: the number of neighbours, <span class="math inline">\(K\)</span>. Since cross-validation helps us evaluate the accuracy of our classifier, we can use cross-validation to calculate an accuracy for each value of <span class="math inline">\(K\)</span> in a reasonable range, and then pick the value of <span class="math inline">\(K\)</span> that gives us the best accuracy. In R, we can accomplish this <em>tuning</em> by passing a vector of values for <span class="math inline">\(K\)</span> to the <code>tuneGrid</code> argument of <code>train</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>)
k =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">11</span>, <span class="dv">13</span>, <span class="dv">15</span>, <span class="dv">17</span>))

<span class="kw">set.seed</span>(<span class="dv">1234</span>)
knn_model_cv_10fold &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> X_train_total, <span class="dt">y =</span> Y_train_total, <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="dt">tuneGrid =</span> k, <span class="dt">trControl =</span> train_control)
knn_model_cv_10fold</code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 427 samples
##   2 predictor
##   2 classes: &#39;B&#39;, &#39;M&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 385, 384, 384, 384, 384, 384, ... 
## Resampling results across tuning parameters:
## 
##   k   Accuracy   Kappa    
##    1  0.7967696  0.5658038
##    3  0.8430570  0.6607231
##    5  0.8408449  0.6596402
##    7  0.8503160  0.6801461
##    9  0.8503160  0.6819992
##   11  0.8572928  0.6981143
##   13  0.8549118  0.6906446
##   15  0.8502606  0.6808579
##   17  0.8525862  0.6866224
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was k = 11.</code></pre>
<p>Although <code>caret</code> provides a selection of <span class="math inline">\(K=11\)</span> for us by maximizing the accuracy estimate, it is helpful regardless to visualize the accuracy as we increase <span class="math inline">\(K\)</span>. We can access the results from the cross-validation via the<code>results</code> attribute of the <code>train</code> object (our classifier).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">accuracies &lt;-<span class="st"> </span>knn_model_cv_10fold<span class="op">$</span>results
accuracies </code></pre></div>
<pre><code>##    k  Accuracy     Kappa AccuracySD    KappaSD
## 1  1 0.7967696 0.5658038 0.06864062 0.14354647
## 2  3 0.8430570 0.6607231 0.04435446 0.09509448
## 3  5 0.8408449 0.6596402 0.04051072 0.08085130
## 4  7 0.8503160 0.6801461 0.04084149 0.08685086
## 5  9 0.8503160 0.6819992 0.03615899 0.07298776
## 6 11 0.8572928 0.6981143 0.04931848 0.09851785
## 7 13 0.8549118 0.6906446 0.04184867 0.08469489
## 8 15 0.8502606 0.6808579 0.02878506 0.05850151
## 9 17 0.8525862 0.6866224 0.03428950 0.06804069</code></pre>
<p>Now we can plot accuracy versus <span class="math inline">\(K\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">accuracy_vs_k &lt;-<span class="st"> </span><span class="kw">ggplot</span>(accuracies, <span class="kw">aes</span>(<span class="dt">x =</span> k, <span class="dt">y =</span> Accuracy)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>()
accuracy_vs_k</code></pre></div>
<p><img src="_main_files/figure-html/06-find-k-1.png" width="480" /></p>
<p>Based off of the visualization above, we might agree with <code>caret</code>’s choice of <span class="math inline">\(K=11\)</span>. But as you can see, there is no exact or perfect answer here; any choice between <span class="math inline">\(K=8\)</span> and <span class="math inline">\(K = 15\)</span> would be reasonably justified. Remember: the values you see on this plot are <em>estimates</em> of the true accuracy of our classifier. Although the <span class="math inline">\(K=11\)</span> value is higher than the others on this plot, that doesn’t mean the classifier is actually more accurate with this parameter value! Generally, when selecting <span class="math inline">\(K\)</span> (and other parameters for other predictive models), we are looking for a value where:</p>
<ul>
<li>we get roughly optimal accuracy, so that our model will likely be accurate</li>
<li>changing the value to a nearby one (e.g. from <span class="math inline">\(K=11\)</span> to 10 or 12) doesn’t decrease accuracy too much, so that our choice is reliable in the presence of uncertainty</li>
<li>the cost of training the model is not prohibitive (e.g., in our situation, if <span class="math inline">\(K\)</span> is too large, predicting becomes expensive!)</li>
</ul>
</div>
<div id="underoverfitting" class="section level3">
<h3><span class="header-section-number">7.4.3</span> Under/overfitting</h3>
<p>To build a bit more intuition, what happens if we keep increasing the number of neighbours <span class="math inline">\(K\)</span>? In fact, the accuracy actually starts to decrease! Take a look as the plot below as we vary <span class="math inline">\(K\)</span> from 1 to almost the number of observations in the data set:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>)
k_lots =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">1</span>, <span class="dt">to =</span> <span class="dv">385</span>, <span class="dt">by =</span> <span class="dv">10</span>))
<span class="kw">set.seed</span>(<span class="dv">1234</span>)
knn_model_cv_10fold_lots &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> X_train_total, <span class="dt">y =</span> Y_train_total, <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="dt">tuneGrid =</span> k_lots, <span class="dt">trControl =</span> train_control)
accuracies_lots &lt;-<span class="st"> </span>knn_model_cv_10fold_lots<span class="op">$</span>results
accuracy_vs_k_lots &lt;-<span class="st"> </span><span class="kw">ggplot</span>(accuracies_lots, <span class="kw">aes</span>(<span class="dt">x =</span> k, <span class="dt">y =</span> Accuracy)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>()
accuracy_vs_k_lots</code></pre></div>
<p><img src="_main_files/figure-html/06-lots-of-ks-1.png" width="480" /></p>
<p><strong>Underfitting:</strong> What is actually happening to our classifier that causes this? As we increase the number of neighbours, more and more of the training observations (and those that are farther and farther away from the point) get a “say” in what the class of a new observation is. This causes a sort of “averaging effect” to take place, making the boundary between where our classifier would predict a tumour to be malignant versus benign to smooth out and become <em>simpler.</em> If you take this to the extreme, setting <span class="math inline">\(K\)</span> to the total training data set size, then the classifier will always predict the same label regardless of what the new observation looks like. In general, if the model <em>isn’t influenced enough</em> by the training data, it is said to <strong>underfit</strong> the data.</p>
<p><strong>Overfitting:</strong> In contrast, when we decrease the number of neighbours, each individual data point has a stronger and stronger vote regarding nearby points. Since the data themselves are noisy, this causes a more “jagged” boundary corresponding to a <em>less simple</em> model. If you take this case to the extreme, setting <span class="math inline">\(K = 1\)</span>, then the classifier is essentially just matching each new observation to its closest neighbour in the training data set. This is just as problematic as the large <span class="math inline">\(K\)</span> case, because the classifier becomes unreliable on new data: if we had a different training set, the predictions would be completely different. In general, if the model <em>is influenced too much</em> by the training data, it is said to <strong>overfit</strong> the data.</p>
<p>You can see this effect in the plots below as we vary the number of neighbours <span class="math inline">\(K\)</span> in (1, 7, 20, 200):</p>
<center>
<img src="_main_files/figure-html/06-decision-grid-K-1.png" width="960" />
</center>
<!--By using cross-validation to choose $k$ we were able to slightly increase our accuracy, but can we still do better? Perhaps. We can start 
to explore this by taking a look at what is called the training accuracy. Training accuracy is our accuracy if we asked our classifier 
to make predictions on the training data and then we assessed how well the predictions matched up to the true labels we have for our 
training data. If they don't match up really well (training accuracy is low), our classification model might be too simple and adding more 
information (e.g., additional predictors/explanatory variables) could potentially help. The situation where the training accuracy is low 
is often called underfitting, or high bias.

The training error can be obtained from using the classifier object returned from `train` (when you don't perform cross-validation) to predict 
on the training data. Then passing the predictions on the training data and the true observed labels into `confusionMatrix`.


```r
k = data.frame(k = 11)
set.seed(1234)
knn_model <- train(x = X_train_total, y = Y_train_total, method = "knn", tuneGrid = k)
training_pred <- predict(knn_model, X_train_total)
results <- confusionMatrix(training_pred, Y_train_total)
results
```

```
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   B   M
##          B 238  25
##          M  30 134
##                                           
##                Accuracy : 0.8712          
##                  95% CI : (0.8357, 0.9015)
##     No Information Rate : 0.6276          
##     P-Value [Acc > NIR] : <2e-16          
##                                           
##                   Kappa : 0.7262          
##                                           
##  Mcnemar's Test P-Value : 0.5896          
##                                           
##             Sensitivity : 0.8881          
##             Specificity : 0.8428          
##          Pos Pred Value : 0.9049          
##          Neg Pred Value : 0.8171          
##              Prevalence : 0.6276          
##          Detection Rate : 0.5574          
##    Detection Prevalence : 0.6159          
##       Balanced Accuracy : 0.8654          
##                                           
##        'Positive' Class : B               
## 
```

From the complex output, we can see the training accuracy, here 0.8752. Again we can use base/built-in subsetting syntax to directly get the value:


```r
results$overall[1]
```

```
##  Accuracy 
## 0.8711944
```

Here we see that our training accuracy is high, 0.8752197, but there is still room for improvement! (If it were 1.0 there wouldn't be and we would have a different problem). So let's see if adding additional information (predictors/explanatory variables) might help our model training and consequently (and more importantly) validation accuracy.

*Note - when adding more information to the model, $k$ = 11 may no longer be the "best" $k$. So we will want to also choose $k$ again.*


```r
# set-up training data
X_cancer_all <- cancer %>% 
  select(-Class, -ID) %>% 
  data.frame()
Y_cancer_all <- cancer %>% 
  select(Class) %>% 
  unlist()

# set-up classifier specifications
train_control <- trainControl(method="cv", number = 10)
k = data.frame(k = seq(from = 1, to = 29, by = 2))

# create classifier
set.seed(1234)
knn_model_all <- train(x = X_cancer_all, y = Y_cancer_all, method = "knn", tuneGrid = k, trControl = train_control)

# assess training accuracy
training_pred_all <- predict(knn_model_all, X_cancer_all)
results_all <- confusionMatrix(training_pred_all, Y_cancer_all)
results_all$overall[1]
```

```
##  Accuracy 
## 0.9718805
```

We can see that by including more information (making our classifier more complex by adding additional predictors/explanatory variables) we increased the training accuracy. What about the cross-validation accuracy? And what $k$ should we choose?


```r
accuracies_all <- knn_model_all
accuracy_vs_k_all <- ggplot(accuracies_all, aes(x = k, y = Accuracy)) +
  geom_point() +
  geom_line()
accuracy_vs_k_all
```

<img src="_main_files/figure-html/06-more-info-1.png" width="480" />

From the plot above, it seems as though now with more information in our model, we should choose a $k$ of 7. Additionally, with this extra information our validation accuracy has also increased with $k = 7$.
-->
</div>
</div>
<div id="splitting-data" class="section level2">
<h2><span class="header-section-number">7.5</span> Splitting data</h2>
<p><strong>Shuffling:</strong> When we split the data into train, test, and validation sets, we make the assumption that there is no order to our originally collected data set. However, if we think that there might be some order to the original data set, then we can randomly shuffle the data before splitting it. The <code>caret</code> package’s <code>createDataPartition</code> function does this for us.</p>
<p><strong>Stratification:</strong> If the data are imbalanced, we also need to be extra careful about splitting the data to ensure that enough of each class ends up in each of the train, validation, and test partitions. Luckily, the <code>createDataPartition</code> does this for us as well.</p>
</div>
<div id="summary" class="section level2">
<h2><span class="header-section-number">7.6</span> Summary</h2>
<p>Classification algorithms use one or more quantitative variables to predict the value of a third, categorical variable. The K-nearest neighbour algorithm in particular does this by first finding the K points in the training data nearest to the new observation, and then returning the majority class vote from those training observations. We can evaluate a classifier by splitting the data randomly into a training and test data set, using the training set to build the classifier, and using the test set to estimate its accuracy. To tune the classifier (e.g., select the K in K-nearest neighbours), we maximize accuracy estimates from cross-validation.</p>
<center>
<figure class="image">
<img src="img/testing.png" width="600"/>
<figcaption>
A typical 10-fold cross-validation data set split. <br>Source: <a href="https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6" class="uri">https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6</a>
</figcaption>
</figure>
</center>
<p>The overall workflow for performing K-nearest neighbour classification in <code>caret</code> is as follows:</p>
<ol style="list-style-type: decimal">
<li>Use the <code>createDataPartition</code> function to split the data into a training and test set. Put the test set aside for now.</li>
<li>Use the <code>trainControl</code> function to specify whether to use cross-validation and how many folds</li>
<li>Use the <code>train</code> function to train and evaluate the classifier for different values of <span class="math inline">\(K\)</span></li>
<li>Retrain the classifier on <em>all</em> the training data, using the best parameter from the previous step</li>
<li>Evaluate the estimated accuracy of the classifier on the test set</li>
</ol>
<p><strong>Strengths:</strong></p>
<ol style="list-style-type: decimal">
<li>Simple and easy to understand</li>
<li>No assumptions about what the data must look like</li>
<li>Works easily for binary (two-class) and multi-class (&gt; 2 classes) classification problems</li>
</ol>
<p><strong>Weaknesses:</strong></p>
<ol style="list-style-type: decimal">
<li>As data gets bigger and bigger, K-nearest neighbour gets slower and slower, quite quickly</li>
<li>Does not perform well with a large number of predictors</li>
<li>Does not perform well when classes are imbalanced (when many more observations are in one of the classes compared to the others)</li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/06-classification_continued.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
