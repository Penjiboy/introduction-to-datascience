# Regression, continued {#regression2}

## Overview 
Continued explortion of k-nn regression in higher dimensions. We will also begin to compare k-nn to linear models in the context of regression.

## Learning objectives 
By the end of the chapter, students will be able to:

* In the context of k-nn regression, compare and contrast goodness of fit and prediction properties (namely MSE vs MSPE).
* In a dataset with 2 variables, perform simple ordinary least squares regression in R using `lm()` to predict the values for a test dataset.
* Compare and contrast predictions obtained from k-nearest neighbour regression to those obtained using simple ordinary least squares regression from the same dataset.

## $RMSE$ versus $RMPSE$
The output we get from `caret` for our model error labels it `RMSE` for root mean square error. We showed the equation for calculating this above. We have used the more general notation of $RMSE$ in this textbook as well, however, in other texts, papers and courses you may come across $RMPSE$ as well. What is $RMPSE$? $RMPSE$ stands for root mean prediction square error. The same formula is used to calculate $RMPSE$ and $RMSE$, however these separate terms exists to specify what the error is being calculated on. $RMPSE$ is specifically referring to the error when predicting on future data (e.g., the validation set or the testing set), not the training set. So if you do choose to use the term $RMSE$ as opposed to $RMPSE$, ensure that you provide the correct context about what error you are reporting (r.g., training, validation or test).
