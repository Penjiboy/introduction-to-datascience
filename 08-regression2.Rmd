# Regression, continued {#regression2}

## Overview 
Continued explortion of k-nn regression in higher dimensions. We will also begin to compare k-nn to linear models in the context of regression.

## Learning objectives 
By the end of the chapter, students will be able to:

* In the context of k-nn regression, compare and contrast goodness of fit and prediction properties (namely RMSE vs RMSPE).
* In a dataset with 2 variables, perform simple ordinary least squares regression in R using `caret`'s `train` with `method = "lm"` to predict the values for a test dataset.
* Compare and contrast predictions obtained from k-nearest neighbour regression to those obtained using simple ordinary least squares regression from the same dataset.

## $RMSE$ versus $RMPSE$
The error output we have been getting from `caret` to assess how well our k-nn regression models predict is labelled as `RMSE` for root mean square error. The equation for calculating $RMSE$ is shown in [the previous chapter](regression1.html#assessing-a-knn-regression-model). So far, we have called this error $RMSE$ in this textbook as well, however, in certain contexts the correct term becomes $RMPSE$, which stands for root mean square prediction error. The same formula is used to calculate $RMPSE$ and $RMSE$, however these separate terms exists to specify what the error is being calculated on. $RMPSE$ is specifically referring to the error when predicting on future data (e.g., the validation set(s), or the testing set), not the training set. Whereas, $RMSE$ is specifically referring to the error when predicting on the training data set and is an attempt to get at model goodness of fit on the data used to fit the model. From this point on in the course, we will be using the terms $RMPSE$ and $RMSE$ in their appropriate contexts.

## Linear regression
k-nn is not the only the other type of regression, there are many and one common and quite useful type of regression is called linear regression. Linear regression is similar to k-nn regression in that the target/response variable is expected to be quantitative, however, one way it varies quite differently is how the training data is used to predict a value for a new observation. Instead of looking at the $k$-nearest neighbours and averaging over their values for a prediction, in linear regression all the training data points are used to create a straight line of "best fit", and then the line is used to "look-up" the predicted value.

For example, let's revisit the smaller version of the Sacramento housing data set and the prediction case where we come across a new house we are interested in purchasing, and it is 2000 square feet! Its advertised list price is $350,000 should we give them what they are asking? Or is that overpriced and we should offer less? 

To answer this question using linear regression, we use the data we have to draw the straight line of "best fit" through our existing data points:

```{r linReg1, message = FALSE, warning = FALSE, echo = FALSE, fig.height = 4, fig.width = 5}
library(tidyverse)
library(scales)
library(caret)
library(gridExtra)

data(Sacramento)
set.seed(2019)
small_sacramento <- sample_n(Sacramento, size = 30)

small_plot <- ggplot(small_sacramento, aes(x = sqft, y = price)) +
  geom_point() +
  xlab("House size (square footage)") +
  ylab("Price (USD)") +
  scale_y_continuous(labels=dollar_format()) +
  geom_smooth(method = "lm", se = FALSE) 
small_plot
```

Then we can use this line to "look up" the predicted price given the value we have for the predictor/explanatory variable (here 2000 square feet). 

```{r linReg2, message = FALSE, warning = FALSE, echo = FALSE, fig.height = 4, fig.width = 5}
small_model <- lm(price ~ sqft, data = small_sacramento)
prediction <- predict(small_model, data.frame(sqft = 2000))

small_plot + 
  geom_vline(xintercept = 2000, linetype = "dotted") +
  geom_point(aes(x = 2000, y = prediction[[1]], color = "red", size = 2.5)) +
  theme(legend.position="none")

print(prediction[[1]])
```

Using linear regression on this small data set to predict the sale price for a 2000 square foot house we get a predicted value of \$287178.80 USD. But wait a minute... How exactly does linear regression choose the line of "best fit"? Many different lines could be drawn through the data points, we show some examples below:

```{r severalLines, echo = FALSE, fig.height = 4, fig.width = 5}

small_plot + 
  geom_abline(intercept = -64542.23, slope = 190, color = "green") +
  geom_abline(intercept = -6900, slope = 175, color = "purple") +
  geom_abline(intercept = -64542.23, slope = 160, color = "red") 
```

Linear regression chooses the straight line of "best fit" by choosing the line that minimzes the vertical distance between itself and each of the observed data points. From the lines shown above, that is the blue line. What exactly do we mean by the vertical distance between the predicted values (which fall along the line of "best fit") and the observed data points? We illustrate these distances in the plot below with a red line:

```{r verticalDistToMin,  echo = FALSE, fig.height = 4, fig.width = 5}
small_sacramento <- small_sacramento %>% 
  mutate(predicted = predict(small_model))
small_plot +
  geom_segment(data = small_sacramento, aes(xend = sqft, yend = predicted), colour = "red") 


```

How do we assess the predictive accuracy of a linear regression model? We use the same error function that we used with k-nn regression: $RMPSE$ (note the use of $RMPSE$ versus $RMSE$ here as discussed in the first section of this chapter).

## Linear regression in R using `caret`

We can perform linear regression in R using the `caret` package in a very similar manner to how we performed k-nn regression, using the `train` function. To do this, instead of setting `method = "knn"` we instead set `method = "lm"`. Another difference is that we do not need to choose $k$ in the context of linear regression. In fact, in the case of a simple linear regression (where we only have one predictor) like the one illustrated in this chapter there are no "things" to fiddle with (these things, like $k$, have a formal name: hyperparameters) and so we do not need to perform cross validation. Below we illustrate how we can use the `caret` package to predict house sale price given house size using a linear regression approach using the full Sacramento real estate data set:

As usual, we should start by putting some test data away in a lock box that we can come back to after we choose our final model, so let's take care of that business now. 

```{r test_train_split_again}
set.seed(2019) # makes the random selection of rows reproducible
training_rows <- Sacramento %>% 
  select(price) %>% 
  unlist() %>% # converts Class from a tibble to a vector
  createDataPartition(p = 0.6, list = FALSE)

X_train <- Sacramento %>% 
  select(sqft) %>% 
  slice(training_rows) %>% 
  data.frame()

Y_train <- Sacramento %>% 
  select(price) %>% 
  slice(training_rows) %>% 
  unlist()

X_test <- Sacramento %>% 
  select(sqft) %>% 
  slice(-training_rows) %>% 
  data.frame()

Y_test <- Sacramento %>% 
  select(price) %>% 
  slice(-training_rows) %>% 
  unlist()
```


Now that we have our training data, we fit our linear regression model:

```{r fitLM, fig.height = 4, fig.width = 5}
lm_model <- train(x = X_train, 
                      y = Y_train, 
                      method = "lm") 
```

And finally, we predict on the test data set to assess how well our model does:

```{r assessFinal}
test_pred <- predict(lm_model, X_test)
lm_modelvalues <- data.frame(obs = Y_test, pred = test_pred)
lm_test_results <- defaultSummary(lm_modelvalues)
lm_test_results
```

Our final model's test error as assessed by $RMSPE$ is 86688.47. Remember that this is in units of the target/response variable, and here that is US Dollars (USD). Does this mean our model is "good" at predicting house sale price based off of the predictor of home size? Again answering this is tricky to answer and requires to use domain knowledge and think about the application you are using the prediction for. 

And what does our final linear regression model look like when we predict across all possible house sizes we might encounter in the Sacremento area? There is a plotting function in the `tidyverse`, `geom_smooth`, that allows us to do this easily by adding a layer on our plot with the linear regression predicted line of "best fit". The default for this add's a plausible range to this line that we are not interested in at this point, so to avoid plotting it, we provide the argument `se = FALSE` in our call to `geom_smooth`.

```{r lm_predict_all, fig.height = 4, fig.width = 5}

train_data <- bind_cols(X_train, data.frame(price = Y_train))

lm_plot_final <- ggplot(train_data, aes(x = sqft, y = price)) +
    geom_point(alpha = 0.4) +
    xlab("House size (square footage)") +
    ylab("Price (USD)") +
    scale_y_continuous(labels = dollar_format())  +
    geom_smooth(method = "lm", se = FALSE) 
lm_plot_final
```

## Comparing linear and k-nn regression

Now that we have a general understanding of both linear and k-nn regression, we can start to compare and contrast these methods as well as the predictions made by them. To start, lets look at the visualization of the linear regression model predictions for the Sacramento real estate data (predicting price from house size) and the "best" k-nn regression model obtained from the same problem:

```{r compareRegression, echo = FALSE, fig.height = 4, fig.width = 10}
k = data.frame(k = 51)
set.seed(1234)
knn_reg_final <- train(x = X_train, y = Y_train, method = "knn", tuneGrid = k)
set.seed(1234)
predictions_all <- data.frame(sqft = seq(from = 500, to = 4250, by = 1))
predictions_all$price <- predict(knn_reg_final, 
                                 data.frame(sqft = seq(from = 500, to = 4250, by = 1)))

plot_final <- ggplot(train_data, aes(x = sqft, y = price)) +
    geom_point(alpha = 0.4) +
    xlab("House size (square footage)") +
    ylab("Price (USD)") +
    scale_y_continuous(labels = dollar_format())  +
    geom_line(data = predictions_all, aes(x = sqft, y = price), color = "blue") +
    ggtitle("k-nn regression") +
    annotate("text", x = 3500, y = 100000, label = paste("RMSPE =", "91620.40"))

lm_plot_final <- lm_plot_final +
  annotate("text", x = 3500, y = 100000, label = paste("RMSPE =", round(lm_test_results[[1]], 2))) +
  ggtitle("linear regression")

grid.arrange(lm_plot_final, plot_final, ncol = 2)
```

What differences do we observe from the visualization above? One obvious difference is the shape of the blue lines. In linear regression we are restricted to a straight line, whereas in k-nn regression our line is much more flexible and can be quite wiggly. There can be an advantage to limiting the model to a straight line, as linear regression does, in that a straight line model is quite interpretable and can be defined by two numbers, the y-intercept and the slope. The slope is particularly meaningful for interpretation, as it tells us what unit increase in the target/response variable we predict given a unit increase in the predictor/explanatory variable. k-nn regression, as simple as it is to implement and understand, has no such interpretability from it's wiggly line. 

There can however also be a disadvantage to using a linear regression model in some cases, particularly when the relationship between the target and the predictor is not linear, but instead someother shape, such as curved or circular. In these cases the prediction model from a linear regression will have high bias, meaning that model/predicted values does not match the actual observed values very well. Such a model would probably have a quite high $RMSE$ when assessing model goodness of fit on the training data and a quite high $RMPSE$ when assessing model prediction quality on a test data set. On such a data set, k-nn regression may fare better. Additionally, there are other types of regression you can learn about in future courses that may do even better at predicting with such data.

How do these two models compare on this data set? On the visualizations above we also printed the $RMPSE$ as calculated from predicting on the test data set that was not used to train/fit the models. The $RMPSE$ for the linear regression model is less than the $RMPSE$ for the k-nn regression model, and thus if were were comparing these in practice we would choose to use the linear regression model to make our predictions because of this (in addition to the fact the the linear regression model is more interpretable). 

Why is the linear regression model doing better at predicting on the test set than the k-nn regression model?

## Additional readings/resources
- Pages 59-71 of [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf) with Applications in R by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani
- [The `caret` Package](https://topepo.github.io/caret/index.html)
