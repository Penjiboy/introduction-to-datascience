--- 
title: "Introduction to Data Science"
author: 
- Tiffany-Anne Timbers
- Melissa Lee
- Samuel Hinshaw
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: rstudio/bookdown-demo
description: "This is an open source textbook for teaching Introductory Data Science."
---

# Introduction to Data Science

Placeholder


## Chapter learning objectives
## Jupyter notebooks
## Loading a tabular dataset
## Assigning value to an object
## Subsetting data frames with `select` & `filter`
### Using `select` to subset multiple columns
### Using `select` to subset a range of columns
### Using `filter` to subset a single column
### Using `filter` to get rows with values above a threshold
## Combining functions using the pipe operator: %>%
### Using %>% to combine `filter` and `select`
### Using %>% with more than two functions
## Creating Visualizations in R
### Using `ggplot` to create a scatter plot
### Using `ggplot` to create a scatter plot
### Formatting ggplot objects
### Coloring points by group
### What's next?

<!--chapter:end:index.Rmd-->


# Reading in data locally and from the web {#reading}

Placeholder


## Overview 
## Chapter learning objectives
## Absolute and relative file paths
##### Loading `avocado_prices.csv` using a relative path:
##### Loading `avocado_prices.csv` using an absolute path:
## Reading tabular data into R
### `read_delim` as a more flexible method to get tabular data into R
### Reading tabular data directly from a URL
### Previewing a data file before reading it into R
## Scraping data off the web using R
### HTML and CSS selectors
### Are you allowed to scrape that website?
### Using `rvest`
## Additional readings/resources

<!--chapter:end:01-reading.Rmd-->


# Cleaning and wrangling data {#wrangling}

Placeholder


## Overview 
## Chapter learning objectives
## Vectors and Data frames
### What is a data frame?
### What is a vector?
### How are vectors different from a list?
### What does this have to do with data frames?
## The `dplyr` functions
## Tidy Data
### What is tidy data?
### Why is tidy data important in R?
### Going from wide to long (or tidy!) using `gather`
### Using separate to deal with multiple delimiters
## Using `purrr`'s `map*` functions to iterate
### A bit more about the `map*` functions
## Additional readings/resources

<!--chapter:end:02-wrangling.Rmd-->


# Effective data visualization {#viz}

Placeholder


## Overview 
## Chapter learning objectives
## `ggplot2` for data visualization in R
## Making effective vizualizations
### Some guiding principles for making effectice visualizations

<!--chapter:end:03-viz.Rmd-->


# Version control with GitHub {#GitHub}

Placeholder


## Overview 
## Videos to learn about version control with GitHub and Git
### Creating a GitHub repository
### Exploring a GitHub repository
### Directly editing files on GitHub
### Logging changes and pushing them to GitHub
## Git command cheatsheet
### Getting a repository from GitHub onto the server for the first time
### Logging changes
### Sending your changes back to GitHub
### Getting changes
## Terminal cheatsheet
### See where you are:
### See what is inside the directory where you are:
### Move to a different directory

<!--chapter:end:04-version_control.Rmd-->


# Classification {#classification}

Placeholder


## Overview 
## Learning objectives 
## Classification
## Wisconsin Breast Cancer Example:
### Data Exploration
#### Variable descriptions
### K-Nearest Neighbour Classifier
## Find the distance between new point and all others in dataset
#### Distance Between Points When There are two explanatory variables/predictors
##### Summary: 
### K-Nearest Neighbours in R 
### More than two explanatory variables/predictors
## Additional readings/resources

<!--chapter:end:05-classification.Rmd-->


# Classification continued {#classification_continued}

Placeholder


## Overview 
## Learning objectives 
## Assessing how good your classifier is
### Assessing your classifier in R
#### Splitting into training and validation sets
#### Creating the k-nn classifier
#### Predict class labels for the validation set
#### Assessing our classifier's accuracy
## Cross-validation for assessing classifier quality
## Choosing the number of neighbours for k-nn classification
## Other ways to increase accuracy
## Test data set
## Scaling your data
## Strengths and limitations of k-nn classification
### Strengths of k-nn classification
### Limitations of k-nn classification
## Additional readings/resources

<!--chapter:end:06-classification_continued.Rmd-->


# Introduction to regression through K-nearest neighbours {#regression1}

Placeholder


## Overview 
## Learning objectives 
## Regression 
## Sacremento real estate example
## K-nearest neighbours regression
## Assessing a knn regression model
## How do different k's affect k-nn regression predictions
## Assessing model goodness with the test set
## Strengths and limitations of k-nn regression
### Strengths of k-nn regression
### Limitations of k-nn regression

<!--chapter:end:07-regression1.Rmd-->


# Regression, continued {#regression2}

Placeholder


## Overview 
## Learning objectives 
## $RMSE$ versus $RMPSE$
## Linear regression
## Linear regression in R using `caret`
## Comparing linear and k-nn regression
## Additional readings/resources

<!--chapter:end:08-regression2.Rmd-->


# Regression, continued some more... {#regression3}

Placeholder


## Overview 
## Learning objectives 
## Multivariate k-nn regression
## Multivariate linear regression
## The other side of regression
## Additional readings/resources

<!--chapter:end:09-regression3.Rmd-->

# Clustering {#clustering}

## Overview 
Introduction to clustering using K-means. Will discuss the K-means algorithm, how we choose K (the number of clusters) and other practical considerations (such as scaling).

## Learning objectives 
By the end of the chapter, students will be able to:

* Describe a case where clustering would be an appropriate tool, and what insight it would bring from the data.
* Explain the kmeans clustering algorithm.
* Interpret the output of a kmeans analysis.
* Perform kmeans clustering in R using `kmeans`.
* Visualize the output of kmeans clustering in R using pair-wise scatter plots.
* Identify when it is necessary to scale variables before clustering and do this using R.
* Use the elbow method to choose the number of clusters for k-means.
* Describe advantages, limitations and assumptions of the kmeans clustering algorithm.

## Clustering
While at first glance, clustering may seem very similar to classification, these two methods have some very important distinction. Most notably, classification is a supervised method (we use past information to predict the future values/labels for our target/response variable), whereas clustering is considered an unsupervised method (there is no target/response variable and we are looking to find sub-groups/clusters of observations based on how similar they are). So, where classification might be used to label future emails as spam or not spam, clustering might be instead be used to group emails into categories based on their similarity, however we would not have labels for these categories in the case of clustering. Another example problem we might try to solve with clustering is grouping Amazon customers into groups based upon their similar purchasing behaviours. Again here, we do not have, nor need, labels for customer groups.

Another way to think about it is, that classification is really about predicting something that you might have a scientific question about and/or hypothesis for, whereas, clustering is very often a hypothesis generating process (you identify things that are similar to each other that might be unexpected, and from those observations, you might generate a question and hypothesis that you might follow-up with classification).

Another major difference between clustering and classification is in how success is determined. With classification we are able to use a test data set to assess prediction performance, in clustering we must use variance metrics to determine how well our defined clusters fit the data. The two metrics used to determine success are between- and within- variation. Ideally we want clusters where the between-variance is large (so that the clusters are well separated) and the within- variation is small (so that the clusters are composed of close/tight-knit observations).

### A toy example

What if we had some customer data, and we wanted to learn more about the types of customers we had so that we could come up with better products and/or promotions to increase our business in a data-driven way. For example, let's consider this data below, where we have assessed customer loyalty and customer satisfaction:

```{r echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
data <- tibble(loyalty = c(7, 5, 8, 7, 3, 1, 8, 4, 2, 7, 6, 7, 6, 5, 9, 7, 9, 5, 2),
               csat = c(1, 1, 2, 2, 2, 3, 4, 4, 4, 6, 6, 7, 7, 7, 8, 8, 9, 9, 3),
               cluster = c("1",
                           "1",
                           "1",
                           "1",
                           "2",
                           "2",
                           "1",
                           "2",
                           "2",
                           "3",
                           "3",
                           "3",
                           "3",
                           "3",
                           "3",
                           "3",
                           "3",
                           "3", 
                           "2"))
```

```{r Example, echo = FALSE, warning = FALSE, fig.height = 4, fig.width = 4.35}
base <- ggplot(data, aes(y = loyalty, x = csat)) +
  geom_point() +
  xlab("Customer satisfaction") +
  ylab("Loyalty") +
  xlim(c(0, 10)) +
  ylim(c(0, 10))

base
```

data modified from: http://www.segmentationstudyguide.com/using-cluster-analysis-for-market-segmentation/

From this data we might ask whether there are sub-groups within our customers? For example do we have customers with high loyalty and high satisfaction? What about low satisfaction and high loyalty? One way to answer such a question is to apply K-means clustering analysis. When we do such an analysis on this data set we identify 3 customer subgroups within our data set:

```{r clusteringExample, echo = FALSE, warning = FALSE, fig.height = 4, fig.width = 5}
ggplot(data, aes(y = loyalty, x = csat, color = cluster)) +
  geom_point() +
  xlab("Customer satisfaction") +
  ylab("Loyalty") +
  xlim(c(0, 10)) +
  ylim(c(0, 10))
```

What are the labels for these groups? We don't really have any, only cluster numbers are output from the clustering algorithm. In a simple case like this, where we can easily visualize the clusters on a scatter plot, we can give labels to these groups after clustering using the positions of the groups on the plot:

- low loyalty and low satisfaction (<font color="#00BA38">green cluster</font>),
- high loyalty and low satisfaction (<font color="#F8766D">pink cluster</font>), 
- and high loyalty and high satisfaction (<font color="#619CFF">blue cluster</font>).

Once we have such data we can use it to inform our future business decisions, and/or ask questions like, why did we not observe customers who had high satisfaction but low loyalty?

## K-means clustering algorithm

How does the K-means clustering algorithm work? Let's use the toy example shown above to illustrate it. First, we start by choosing $K$, the number of clusters. For this example we will choose 3. How do we choose $K$? It's an important question that we will answer later in this chapter. After choosing the number of clusters, we randomly assign each point to a cluster:

```{r clusteringRandomStart, echo = FALSE, warning = FALSE, fig.height = 4, fig.width = 5}
set.seed(1234)
possible <- c("1", "2", "3")
data0 <- data %>% 
  mutate(cluster = sample(possible, 19, replace = TRUE))

rand_start <- ggplot(data0, aes(y = loyalty, x = csat, color = cluster)) +
  geom_point() +
  xlab("Customer satisfaction") +
  ylab("Loyalty") +
  xlim(c(0, 10)) +
  ylim(c(0, 10))

rand_start
```




Next we calculate the location of the cluster centre (called a centroid) for each of the K clusters to be the centre of each cluster. We illustrate that below:

```{r randomCentroid, echo = FALSE, warning = FALSE, fig.height = 4, fig.width = 4.35}
clust1_0 <- data0 %>% 
  filter(cluster == "1")
cent1_0 <- data.frame(csat = mean(clust1_0$csat),
                    loyalty = mean(clust1_0$loyalty))
clust2_0 <- data0 %>% 
  filter(cluster == "2")
cent2_0 <- data.frame(csat = mean(clust2_0$csat),
                    loyalty = mean(clust2_0$loyalty))
clust3_0 <- data0 %>% 
  filter(cluster == "3")
cent3_0 <- data.frame(csat = mean(clust3_0$csat),
                    loyalty = mean(clust3_0$loyalty))
cent_0 <- rbind(cent1_0, cent2_0, cent3_0) %>% 
  mutate(cluster = c("1", "2", "3"))

rand_start +
  geom_point(aes(x = cent1_0$csat, y = cent1_0$loyalty), color = "#F8766D", size = 2.5, shape = 25) +
  geom_point(aes(x = cent2_0$csat, y = cent2_0$loyalty), color = "#00BA38", size = 2.5, shape = 25) +
  geom_point(aes(x = cent3_0$csat, y = cent3_0$loyalty), color = "#619CFF", size = 2.5, shape = 25) 
```


Next, we assign the points to the cluster with the closest centroid based on straight line distance:

```{r iter1, echo = FALSE, warning = FALSE, fig.height = 4, fig.width = 5}

data0 %>%  
  mutate(dist_cent1 = dist(rbind(cent_0[1, 1:2], data0[, 1:2]))[1:19],
         dist_cent2 = dist(rbind(cent_0[2, 1:2], data0[, 1:2]))[1:19],
         dist_cent3 = dist(rbind(cent_0[3, 1:2], data0[, 1:2]))[1:19])

data1 <- data0 %>% 
  mutate(cluster = c("2", "2", "2", "2", "2",
                     "2", "1", "2", "2", "1",
                     "1", "1", "1", "3", "1",
                     "1", "1", "3", "2"))

# data1 <- tibble(loyalty = c(7, 5, 8, 7, 3, 1, 8, 4, 2, 7, 6, 7, 6, 5, 9, 7, 9, 5, 2),
#                csat = c(1, 1, 2, 2, 2, 3, 4, 4, 4, 6, 6, 7, 7, 7, 8, 8, 9, 9, 3),
#                cluster = c("1",
#                            "1",
#                            "1",
#                            "1",
#                            "1",
#                            "1",
#                            "1",
#                            "1",
#                            "2",
#                            "2",
#                            "2",
#                            "3",
#                            "3",
#                            "3",
#                            "3",
#                            "3",
#                            "3",
#                            "3", 
#                            "1"))

it1 <- ggplot(data1, aes(y = loyalty, x = csat, colour = cluster)) +
  geom_point() +
  xlab("Customer satisfaction") +
  ylab("Loyalty") +
  xlim(c(0, 10)) +
  ylim(c(0, 10)) 

it1 +
  geom_point(aes(x = cent1_0$csat, y = cent1_0$loyalty), color = "#F8766D", size = 2.5, shape = 25) +
  geom_point(aes(x = cent2_0$csat, y = cent2_0$loyalty), color = "#00BA38", size = 2.5, shape = 25) +
  geom_point(aes(x = cent3_0$csat, y = cent3_0$loyalty), color = "#619CFF", size = 2.5, shape = 25)
```

Next, we re-adjust the position of the centroids to be the centre of the clusters:

```{r adjustCentroid, echo = FALSE, warning = FALSE, fig.height = 4, fig.width = 5}
clust1_1 <- data1 %>% 
  filter(cluster == "1")
cent1_1 <- data.frame(csat = mean(clust1_1$csat),
                    loyalty = mean(clust1_1$loyalty))
clust2_1 <- data1 %>% 
  filter(cluster == "2")
cent2_1 <- data.frame(csat = mean(clust2_1$csat),
                    loyalty = mean(clust2_1$loyalty))
clust3_1 <- data1 %>% 
  filter(cluster == "3")
cent3_1 <- data.frame(csat = mean(clust3_1$csat),
                    loyalty = mean(clust3_1$loyalty))
cent_1 <- rbind(cent1_1, cent2_1, cent3_1) %>% 
  mutate(cluster = c("1", "2", "3"))

# old_cent1 <- cent1
# clust1 <- data1 %>% 
#   filter(cluster == "1")
# cent1 <- data.frame(csat = mean(clust1$csat),
#                     loyalty = mean(clust1$loyalty))
# old_cent2 <- cent2
# clust2 <- data1 %>% 
#   filter(cluster == "2")
# cent2 <- data.frame(csat = mean(clust2$csat),
#                     loyalty = mean(clust2$loyalty))
# old_cent3 <- cent3
# clust3 <- data1 %>% 
#   filter(cluster == "3")
# cent3 <- data.frame(csat = mean(clust3$csat),
#                     loyalty = mean(clust3$loyalty))

library(grid)
it1 +
  geom_point(aes(x = cent1_1$csat, y = cent1_1$loyalty), color = "#F8766D", size = 2.5) +
  geom_point(aes(x = cent2_1$csat, y = cent2_1$loyalty), color = "#00BA38", size = 2.5) +
  geom_point(aes(x = cent3_1$csat, y = cent3_1$loyalty), color = "#619CFF", size = 2.5) +
  geom_segment(aes(x = cent1_0$csat, y = cent1_0$loyalty, xend = cent1_1$csat, yend = cent1_1$loyalty), color = "black", arrow = arrow(length = unit(0.15, "cm"))) +
  geom_segment(aes(x = cent2_0$csat, y = cent2_0$loyalty, xend = cent2_1$csat, yend = cent2_1$loyalty), color = "black", arrow = arrow(length = unit(0.15, "cm"))) +
  geom_segment(aes(x = cent3_0$csat, y = cent3_0$loyalty, xend = cent3_1$csat, yend = cent3_1$loyalty), color = "black", arrow = arrow(length = unit(0.15, "cm"))) +
  geom_point(aes(x = cent1_0$csat, y = cent1_0$loyalty), color = "#F8766D", size = 2.5, alpha = 0.01) +
  geom_point(aes(x = cent2_0$csat, y = cent2_0$loyalty), color = "#00BA38", size = 2.5, alpha = 0.01) +
  geom_point(aes(x = cent3_0$csat, y = cent3_0$loyalty), color = "#619CFF", size = 2.5, alpha = 0.01) 
```

Then we update cluster assignments of the points to the nearest cluster:

```{r iter2, echo = FALSE, warning = FALSE, fig.height = 4, fig.width = 5}
data1 %>%  
  mutate(dist_cent1 = dist(rbind(cent_1[1, 1:2], data1[, 1:2]))[1:19],
         dist_cent2 = dist(rbind(cent_1[2, 1:2], data1[, 1:2]))[1:19],
         dist_cent3 = dist(rbind(cent_1[3, 1:2], data1[, 1:2]))[1:19])

data2 <- data1 %>% 
  mutate(cluster = c("2", "2", "2", "2", "2",
                     "2", "1", "2", "2", "1",
                     "1", "1", "1", "3", "1",
                     "1", "1", "3", "2"))




data2 <- tibble(loyalty = c(7, 5, 8, 7, 3, 1, 8, 4, 2, 7, 6, 7, 6, 5, 9, 7, 9, 5, 2),
               csat = c(1, 1, 2, 2, 2, 3, 4, 4, 4, 6, 6, 7, 7, 7, 8, 8, 9, 9, 3),
               cluster = c("1",
                           "1",
                           "1",
                           "1",
                           "1",
                           "1",
                           "1",
                           "2",
                           "2",
                           "3",
                           "2",
                           "3",
                           "3",
                           "2",
                           "3",
                           "3",
                           "3",
                           "3", 
                           "1"))

it2 <- ggplot(data2, aes(y = loyalty, x = csat, colour = cluster)) +
  geom_point() +
  xlab("Customer satisfaction") +
  ylab("Loyalty") +
  xlim(c(0, 10)) +
  ylim(c(0, 10)) 

it2 +
  geom_point(aes(x = cent1$csat, y = cent1$loyalty), color = "#F8766D", size = 2.5) +
  geom_point(aes(x = cent2$csat, y = cent2$loyalty), color = "#00BA38", size = 2.5) +
  geom_point(aes(x = cent3$csat, y = cent3$loyalty), color = "#619CFF", size = 2.5) 
```

And again we update the centroid position to be the centre of the new clusters:

```{r adjustCentroid2, echo = FALSE, warning = FALSE, fig.height = 4, fig.width = 5}
old_cent1 <- cent1
clust1 <- data2 %>% 
  filter(cluster == "1")
cent1 <- data.frame(csat = mean(clust1$csat),
                    loyalty = mean(clust1$loyalty))
old_cent2 <- cent2
clust2 <- data2 %>% 
  filter(cluster == "2")
cent2 <- data.frame(csat = mean(clust2$csat),
                    loyalty = mean(clust2$loyalty))
old_cent3 <- cent3
clust3 <- data2 %>% 
  filter(cluster == "3")
cent3 <- data.frame(csat = mean(clust3$csat),
                    loyalty = mean(clust3$loyalty))

it2 +
  geom_point(aes(x = cent1$csat, y = cent1$loyalty), color = "#F8766D", size = 2.5) +
  geom_point(aes(x = cent2$csat, y = cent2$loyalty), color = "#00BA38", size = 2.5) +
  geom_point(aes(x = cent3$csat, y = cent3$loyalty), color = "#619CFF", size = 2.5)  +
  geom_segment(aes(x = old_cent1$csat, y = old_cent1$loyalty, xend = cent1$csat, yend = cent1$loyalty), color = "black") +
  geom_segment(aes(x = old_cent2$csat, y = old_cent2$loyalty, xend = cent2$csat, yend = cent2$loyalty), color = "black") +
  geom_segment(aes(x = old_cent3$csat, y = old_cent3$loyalty, xend = cent3$csat, yend = cent3$loyalty), color = "black") +
  geom_point(aes(x = old_cent1$csat, y = old_cent1$loyalty), color = "#F8766D", size = 2.5, alpha = 0.01) +
  geom_point(aes(x = old_cent2$csat, y = old_cent2$loyalty), color = "#00BA38", size = 2.5, alpha = 0.01) +
  geom_point(aes(x = old_cent3$csat, y = old_cent3$loyalty), color = "#619CFF", size = 2.5, alpha = 0.01) 
```

And then next, we update the points again based on which centroid they are closest too. We do this over and over and over again until we get to a point where the centroids no longer change very much (or don't change at all) between iterations of the algorithm.

Watch the video linked to below for an explanation of the K-means clustering algorithm:
- https://www.coursera.org/lecture/machine-learning-data-analysis/what-is-a-k-means-cluster-analysis-p94tY

*note - when the add pops up to register for this course, you can just click to ignore it (i.e., no need to sign up to watch the entire video)*

## Additional readings:


- Pages 385-390 and 404-405 of [Introduction to Statistical Learning with Applications in R](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf) by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani and the companion video linked to below:

<iframe width="840" height="473" src="https://www.youtube.com/embed/aIybuNt9ps4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<!--chapter:end:10-clustering.Rmd-->

